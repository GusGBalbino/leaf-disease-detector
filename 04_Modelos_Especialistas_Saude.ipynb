{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Modelos Especialistas com Sistema de Threshold Inteligente\n",
        "\n",
        "**Objetivo**: Criar modelos especialistas otimizados que classificam em:\n",
        "- **HEALTHY**: Planta saudÃ¡vel\n",
        "- **UNHEALTHY**: Planta doente (qualquer doenÃ§a)\n",
        "\n",
        "**ðŸŽ¯ Melhorias Implementadas**:\n",
        "- ðŸš¨ **CRÃTICO**: Potato (0% recall Healthy) â†’ Data Augmentation Agressiva + SMOTE\n",
        "- âš ï¸ **IMPORTANTE**: Tomato (56% recall Healthy) â†’ Data Augmentation Moderada + SMOTE  \n",
        "- âœ… **REFINAMENTO**: Pepper (80% recall Healthy) â†’ Data Augmentation Conservadora\n",
        "\n",
        "**ðŸ§  Sistema de Threshold Inteligente**:\n",
        "- **Foco**: Maximizar detecÃ§Ã£o de plantas doentes (recall Unhealthy)\n",
        "- **LÃ³gica**: Thresholds dinÃ¢micos baseados na confianÃ§a da prediÃ§Ã£o\n",
        "- **Tomato**: Base 0.55 â†’ DinÃ¢mico 0.39-0.63 (sensibilidade adaptativa)\n",
        "- **Potato**: Base 0.45 â†’ DinÃ¢mico 0.32-0.52 (mais sensÃ­vel a doenÃ§as)\n",
        "- **Pepper**: Base 0.50 â†’ DinÃ¢mico 0.35-0.58 (equilibrado adaptativo)\n",
        "\n",
        "**EstratÃ©gia**: Data Augmentation Direcionada + SMOTE + Threshold Inteligente para detectar plantas doentes com mÃ¡xima eficÃ¡cia"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-07-10 02:05:02.951982: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1752123902.966438   58193 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1752123902.970780   58193 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1752123902.984268   58193 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1752123902.984294   58193 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1752123902.984296   58193 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1752123902.984297   58193 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "2025-07-10 02:05:02.988194: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=== CARREGANDO DATASETS BINÃRIOS CORRIGIDOS ===\n",
            "ðŸ“‚ Carregando dataset de tomato...\n",
            "   ðŸ¦  Tomato_Bacterial_spot: 2127 â†’ UNHEALTHY\n",
            "   ðŸ¦  Tomato_Early_blight: 1000 â†’ UNHEALTHY\n",
            "   ðŸ¦  Tomato_Late_blight: 1909 â†’ UNHEALTHY\n",
            "   ðŸ¦  Tomato_Leaf_Mold: 952 â†’ UNHEALTHY\n",
            "   ðŸ¦  Tomato_Septoria_leaf_spot: 1771 â†’ UNHEALTHY\n",
            "   ðŸ¦  Tomato_Spider_mites_Two_spotted_spider_mite: 1676 â†’ UNHEALTHY\n",
            "   ðŸ¦  Tomato__Target_Spot: 1404 â†’ UNHEALTHY\n",
            "   ðŸ¦  Tomato__Tomato_YellowLeaf__Curl_Virus: 3208 â†’ UNHEALTHY\n",
            "   ðŸ¦  Tomato__Tomato_mosaic_virus: 373 â†’ UNHEALTHY\n",
            "   âœ… Tomato_healthy: 1591 â†’ HEALTHY\n",
            "   ðŸ“Š Total: 16011 | Healthy: 1591 (9.9%) | Unhealthy: 14420 (90.1%)\n",
            "\n",
            "ðŸ“‚ Carregando dataset de potato...\n",
            "   ðŸ¦  Potato___Early_blight: 1000 â†’ UNHEALTHY\n",
            "   ðŸ¦  Potato___Late_blight: 1000 â†’ UNHEALTHY\n",
            "   âœ… Potato___healthy: 152 â†’ HEALTHY\n",
            "   ðŸ“Š Total: 2152 | Healthy: 152 (7.1%) | Unhealthy: 2000 (92.9%)\n",
            "\n",
            "ðŸ“‚ Carregando dataset de pepper_bell...\n",
            "   ðŸ¦  Pepper__bell___Bacterial_spot: 997 â†’ UNHEALTHY\n",
            "   âœ… Pepper__bell___healthy: 1478 â†’ HEALTHY\n",
            "   ðŸ“Š Total: 2475 | Healthy: 1478 (59.7%) | Unhealthy: 997 (40.3%)\n",
            "\n",
            "âœ… DATASETS BINÃRIOS CARREGADOS: Tomato, Potato, Pepper\n"
          ]
        }
      ],
      "source": [
        "# 1. CARREGAMENTO DE DADOS\n",
        "from utils import *\n",
        "import numpy as np\n",
        "import os\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "import pandas as pd\n",
        "\n",
        "config = carregar_configuracoes()\n",
        "\n",
        "def carregar_dataset(especie):\n",
        "    \"\"\"Carrega dataset agrupando todas as doenÃ§as\"\"\"\n",
        "    print(f\"ðŸ“‚ Carregando dataset de {especie}...\")\n",
        "    \n",
        "    # Construir dataset_info\n",
        "    dataset_info = {}\n",
        "    for esp, info in config['especialistas'].items():\n",
        "        for classe in info['classes']:\n",
        "            dataset_info[classe] = {}\n",
        "    \n",
        "    healthy_images = []\n",
        "    unhealthy_images = []\n",
        "    \n",
        "    # Processar cada classe da espÃ©cie\n",
        "    for classe, info in dataset_info.items():\n",
        "        # Remover underscores \n",
        "        classe_normalizada = classe.lower().replace('_', '')\n",
        "        especie_normalizada = especie.lower().replace('_', '')\n",
        "        \n",
        "        if especie_normalizada in classe_normalizada:\n",
        "            # Usar base_path como diretÃ³rio base das imagens\n",
        "            dir_path = os.path.join(config.get('processed_data_path', config['base_path']), classe)\n",
        "            \n",
        "            if not os.path.exists(dir_path):\n",
        "                print(f\"   âš ï¸ DiretÃ³rio nÃ£o encontrado: {dir_path}\")\n",
        "                continue\n",
        "                \n",
        "            images_in_dir = []\n",
        "            for img_name in os.listdir(dir_path):\n",
        "                if img_name.lower().endswith(('.jpg', '.jpeg', '.png')):\n",
        "                    images_in_dir.append(os.path.join(dir_path, img_name))\n",
        "            \n",
        "            # AGRUPAMENTO BINÃRIO\n",
        "            if 'healthy' in classe.lower():\n",
        "                healthy_images.extend(images_in_dir)\n",
        "                print(f\"   âœ… {classe}: {len(images_in_dir)} â†’ HEALTHY\")\n",
        "            else:\n",
        "                unhealthy_images.extend(images_in_dir)\n",
        "                print(f\"   ðŸ¦  {classe}: {len(images_in_dir)} â†’ UNHEALTHY\")\n",
        "    \n",
        "    # Combinar dados\n",
        "    all_images = healthy_images + unhealthy_images\n",
        "    all_labels = ['healthy'] * len(healthy_images) + ['unhealthy'] * len(unhealthy_images)\n",
        "    \n",
        "    # ProteÃ§Ã£o contra divisÃ£o por zero\n",
        "    if len(all_images) == 0:\n",
        "        print(f\"   âŒ ERRO: Nenhuma imagem encontrada para {especie}!\")\n",
        "        print(f\"   ðŸ” Verifique se as pastas existem e contÃªm imagens.\")\n",
        "        return None\n",
        "    \n",
        "    balance_ratio = len(healthy_images) / len(all_images) * 100\n",
        "    print(f\"   ðŸ“Š Total: {len(all_images)} | Healthy: {len(healthy_images)} ({balance_ratio:.1f}%) | Unhealthy: {len(unhealthy_images)} ({100-balance_ratio:.1f}%)\")\n",
        "    \n",
        "    # Dividindo em treino, validaÃ§Ã£o e teste para todos os datasets\n",
        "    X_temp, X_test, y_temp, y_test = train_test_split(\n",
        "        all_images, all_labels, test_size=0.15, stratify=all_labels, random_state=42\n",
        "    )\n",
        "    \n",
        "    # Dividindo em treino, validaÃ§Ã£o e teste para cada dataset\n",
        "    X_train, X_val, y_train, y_val = train_test_split(\n",
        "        X_temp, y_temp, test_size=0.176, stratify=y_temp, random_state=42\n",
        "    )\n",
        "    \n",
        "    return {\n",
        "        'train': {'X': X_train, 'y': y_train},\n",
        "        'val': {'X': X_val, 'y': y_val},\n",
        "        'test': {'X': X_test, 'y': y_test},\n",
        "        'info': {'balance_ratio': balance_ratio, 'total': len(all_images)}\n",
        "    }\n",
        "\n",
        "# Carregar datasets binÃ¡rios reais\n",
        "print(\"=== CARREGANDO DATASETS BINÃRIOS CORRIGIDOS ===\")\n",
        "dataset_tomato = carregar_dataset('tomato')\n",
        "print()\n",
        "dataset_potato = carregar_dataset('potato')\n",
        "print()\n",
        "dataset_pepper = carregar_dataset('pepper_bell')\n",
        "\n",
        "# Verificar se todos os datasets foram carregados com sucesso\n",
        "datasets_validos = []\n",
        "if dataset_tomato is not None:\n",
        "    datasets_validos.append('Tomato')\n",
        "if dataset_potato is not None:\n",
        "    datasets_validos.append('Potato')    \n",
        "if dataset_pepper is not None:\n",
        "    datasets_validos.append('Pepper')\n",
        "\n",
        "if len(datasets_validos) > 0:\n",
        "    print(f\"\\nâœ… DATASETS BINÃRIOS CARREGADOS: {', '.join(datasets_validos)}\")\n",
        "else:\n",
        "    print(\"\\nâŒ ERRO: Nenhum dataset foi carregado com sucesso!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=== IMPLEMENTANDO ESTRATÃ‰GIAS DE BALANCEAMENTO ===\n",
            "ðŸ”§ Thresholds calibrados para balancear recall Healthy vs Unhealthy:\n",
            "   - Tomato: 0.70 | Potato: 0.60 | Pepper: 0.65\n",
            "âœ… FunÃ§Ãµes de balanceamento carregadas\n"
          ]
        }
      ],
      "source": [
        "# 2. ESTRATÃ‰GIAS DE BALANCEAMENTO OTIMIZADO\n",
        "print(\"=== IMPLEMENTANDO ESTRATÃ‰GIAS DE BALANCEAMENTO ===\")\n",
        "print(\"ðŸ”§ Thresholds calibrados para balancear recall Healthy vs Unhealthy:\")\n",
        "print(\"   - Tomato: 0.70 | Potato: 0.60 | Pepper: 0.65\")\n",
        "\n",
        "def aplicar_data_augmentation_direcionada(dataset, especie):\n",
        "    \"\"\"Aplica Data Augmentation direcionada para classe Healthy\"\"\"\n",
        "    \n",
        "    print(f\"\\nðŸ”„ Aplicando Data Augmentation direcionada para {especie}...\")\n",
        "    \n",
        "    # Separar classes\n",
        "    healthy_images = [img for img, label in zip(dataset['train']['X'], dataset['train']['y']) if label == 'healthy']\n",
        "    unhealthy_images = [img for img, label in zip(dataset['train']['X'], dataset['train']['y']) if label == 'unhealthy']\n",
        "    \n",
        "    print(f\"   Original: Healthy={len(healthy_images)}, Unhealthy={len(unhealthy_images)}\")\n",
        "    \n",
        "    # Definir multiplicadores baseados na severidade do problema\n",
        "    if especie.lower() == 'potato':\n",
        "        multiplicador = 8  # CRÃTICO: 0% recall\n",
        "        print(f\"   ðŸš¨ EstratÃ©gia CRÃTICA: Multiplicador {multiplicador}x\")\n",
        "    elif especie.lower() == 'tomato':\n",
        "        multiplicador = 4  # IMPORTANTE: 56% recall\n",
        "        print(f\"   âš ï¸ EstratÃ©gia IMPORTANTE: Multiplicador {multiplicador}x\")\n",
        "    else:  # pepper\n",
        "        multiplicador = 2  # REFINAMENTO: 80% recall\n",
        "        print(f\"   âœ… EstratÃ©gia REFINAMENTO: Multiplicador {multiplicador}x\")\n",
        "    \n",
        "    # Aplicar augmentaÃ§Ã£o replicando amostras healthy\n",
        "    augmented_healthy = []\n",
        "    augmented_labels = []\n",
        "    \n",
        "    for img_path in healthy_images:\n",
        "        # Adicionar original + cÃ³pias augmentadas\n",
        "        for i in range(multiplicador):\n",
        "            augmented_healthy.append(img_path)\n",
        "            augmented_labels.append('healthy')\n",
        "    \n",
        "    # Combinar todos os dados\n",
        "    all_images = augmented_healthy + unhealthy_images\n",
        "    all_labels = augmented_labels + ['unhealthy'] * len(unhealthy_images)\n",
        "    \n",
        "    print(f\"   Resultado: Healthy={len(augmented_healthy)}, Unhealthy={len(unhealthy_images)}\")\n",
        "    \n",
        "    # Recalcular divisÃµes\n",
        "    X_temp, X_test, y_temp, y_test = train_test_split(\n",
        "        all_images, all_labels, test_size=0.15, stratify=all_labels, random_state=42\n",
        "    )\n",
        "    \n",
        "    X_train, X_val, y_train, y_val = train_test_split(\n",
        "        X_temp, y_temp, test_size=0.176, stratify=y_temp, random_state=42\n",
        "    )\n",
        "    \n",
        "    return {\n",
        "        'train': {'X': X_train, 'y': y_train},\n",
        "        'val': {'X': X_val, 'y': y_val},\n",
        "        'test': {'X': X_test, 'y': y_test},\n",
        "        'info': {\n",
        "            'original_healthy': len(healthy_images),\n",
        "            'augmented_healthy': len(augmented_healthy),\n",
        "            'total_samples': len(all_images)\n",
        "        }\n",
        "    }\n",
        "\n",
        "def aplicar_smote_balanceamento(dataset, especie, target_ratio):\n",
        "    \"\"\"Aplica SMOTE para balanceamento adicional\"\"\"\n",
        "    \n",
        "    print(f\"\\nðŸ”„ Aplicando SMOTE para {especie} (target: {target_ratio*100:.0f}% healthy)...\")\n",
        "    \n",
        "    # Contar classes atuais\n",
        "    healthy_count = sum(1 for label in dataset['train']['y'] if label == 'healthy')\n",
        "    unhealthy_count = len(dataset['train']['y']) - healthy_count\n",
        "    \n",
        "    print(f\"   DistribuiÃ§Ã£o atual: Healthy={healthy_count}, Unhealthy={unhealthy_count}\")\n",
        "    \n",
        "    # Calcular quantas amostras healthy precisamos\n",
        "    total_target = int(unhealthy_count / (1 - target_ratio))\n",
        "    healthy_target = total_target - unhealthy_count\n",
        "    \n",
        "    print(f\"   Target calculado: Healthy={healthy_target}, Unhealthy={unhealthy_count}\")\n",
        "    \n",
        "    # Separar imagens por classe\n",
        "    healthy_images = [img for img, label in zip(dataset['train']['X'], dataset['train']['y']) if label == 'healthy']\n",
        "    unhealthy_images = [img for img, label in zip(dataset['train']['X'], dataset['train']['y']) if label == 'unhealthy']\n",
        "    \n",
        "    # Aplicar SMOTE simulado (replicaÃ§Ã£o inteligente)\n",
        "    balanced_healthy = []\n",
        "    balanced_labels = []\n",
        "    \n",
        "    for i in range(healthy_target):\n",
        "        idx = i % len(healthy_images)\n",
        "        balanced_healthy.append(healthy_images[idx])\n",
        "        balanced_labels.append('healthy')\n",
        "    \n",
        "    # Combinar com amostras unhealthy\n",
        "    all_images = balanced_healthy + unhealthy_images\n",
        "    all_labels = balanced_labels + ['unhealthy'] * len(unhealthy_images)\n",
        "    \n",
        "    print(f\"   Resultado SMOTE: Healthy={len(balanced_healthy)}, Unhealthy={len(unhealthy_images)}\")\n",
        "    \n",
        "    # Recalcular divisÃµes\n",
        "    X_temp, X_test, y_temp, y_test = train_test_split(\n",
        "        all_images, all_labels, test_size=0.15, stratify=all_labels, random_state=42\n",
        "    )\n",
        "    \n",
        "    X_train, X_val, y_train, y_val = train_test_split(\n",
        "        X_temp, y_temp, test_size=0.176, stratify=y_temp, random_state=42\n",
        "    )\n",
        "    \n",
        "    return {\n",
        "        'train': {'X': X_train, 'y': y_train},\n",
        "        'val': {'X': X_val, 'y': y_val},\n",
        "        'test': {'X': X_test, 'y': y_test},\n",
        "        'info': {\n",
        "            'balanced_healthy': len(balanced_healthy),\n",
        "            'total_samples': len(all_images),\n",
        "            'target_ratio': target_ratio\n",
        "        }\n",
        "    }\n",
        "\n",
        "def comparar_distribuicoes(original, balanceado, especie):\n",
        "    \"\"\"Compara distribuiÃ§Ãµes antes e depois\"\"\"\n",
        "    \n",
        "    # Original\n",
        "    orig_healthy = sum(1 for label in original['train']['y'] if label == 'healthy')\n",
        "    orig_unhealthy = len(original['train']['y']) - orig_healthy\n",
        "    orig_ratio = orig_healthy / (orig_healthy + orig_unhealthy) * 100\n",
        "    \n",
        "    # Balanceado\n",
        "    bal_healthy = sum(1 for label in balanceado['train']['y'] if label == 'healthy')\n",
        "    bal_unhealthy = len(balanceado['train']['y']) - bal_healthy\n",
        "    bal_ratio = bal_healthy / (bal_healthy + bal_unhealthy) * 100\n",
        "    \n",
        "    print(f\"\\nðŸ“Š COMPARAÃ‡ÃƒO {especie.upper()}:\")\n",
        "    print(f\"   ORIGINAL:   Healthy={orig_healthy:4d} ({orig_ratio:5.1f}%), Unhealthy={orig_unhealthy:4d}\")\n",
        "    print(f\"   BALANCEADO: Healthy={bal_healthy:4d} ({bal_ratio:5.1f}%), Unhealthy={bal_unhealthy:4d}\")\n",
        "    print(f\"   MELHORIA:   +{bal_healthy - orig_healthy:4d} amostras Healthy ({bal_ratio - orig_ratio:+5.1f}%)\")\n",
        "    \n",
        "    return {\n",
        "        'original': {'healthy': orig_healthy, 'ratio': orig_ratio},\n",
        "        'balanced': {'healthy': bal_healthy, 'ratio': bal_ratio}\n",
        "    }\n",
        "\n",
        "print(\"âœ… FunÃ§Ãµes de balanceamento carregadas\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "=== APLICANDO BALANCEAMENTO OTIMIZADO ===\n",
            "\n",
            "ðŸš¨ POTATO - Aplicando estratÃ©gia CRÃTICA\n",
            "\n",
            "ðŸ”„ Aplicando Data Augmentation direcionada para Potato...\n",
            "   Original: Healthy=106, Unhealthy=1401\n",
            "   ðŸš¨ EstratÃ©gia CRÃTICA: Multiplicador 8x\n",
            "   Resultado: Healthy=848, Unhealthy=1401\n",
            "\n",
            "ðŸ”„ Aplicando SMOTE para Potato (target: 50% healthy)...\n",
            "   DistribuiÃ§Ã£o atual: Healthy=594, Unhealthy=980\n",
            "   Target calculado: Healthy=980, Unhealthy=980\n",
            "   Resultado SMOTE: Healthy=980, Unhealthy=980\n",
            "\n",
            "ðŸ“Š COMPARAÃ‡ÃƒO POTATO:\n",
            "   ORIGINAL:   Healthy= 106 (  7.0%), Unhealthy=1401\n",
            "   BALANCEADO: Healthy= 686 ( 50.0%), Unhealthy= 686\n",
            "   MELHORIA:   + 580 amostras Healthy (+43.0%)\n",
            "\n",
            "âš ï¸ TOMATO - Aplicando estratÃ©gia IMPORTANTE\n",
            "\n",
            "ðŸ”„ Aplicando Data Augmentation direcionada para Tomato...\n",
            "   Original: Healthy=1114, Unhealthy=10099\n",
            "   âš ï¸ EstratÃ©gia IMPORTANTE: Multiplicador 4x\n",
            "   Resultado: Healthy=4456, Unhealthy=10099\n",
            "\n",
            "ðŸ”„ Aplicando SMOTE para Tomato (target: 30% healthy)...\n",
            "   DistribuiÃ§Ã£o atual: Healthy=3120, Unhealthy=7073\n",
            "   Target calculado: Healthy=3031, Unhealthy=7073\n",
            "   Resultado SMOTE: Healthy=3031, Unhealthy=7073\n",
            "\n",
            "ðŸ“Š COMPARAÃ‡ÃƒO TOMATO:\n",
            "   ORIGINAL:   Healthy=1114 (  9.9%), Unhealthy=10099\n",
            "   BALANCEADO: Healthy=2122 ( 30.0%), Unhealthy=4954\n",
            "   MELHORIA:   +1008 amostras Healthy (+20.1%)\n",
            "\n",
            "âœ… PEPPER - Aplicando estratÃ©gia REFINAMENTO\n",
            "\n",
            "ðŸ”„ Aplicando Data Augmentation direcionada para Pepper...\n",
            "   Original: Healthy=1034, Unhealthy=698\n",
            "   âœ… EstratÃ©gia REFINAMENTO: Multiplicador 2x\n",
            "   Resultado: Healthy=2068, Unhealthy=698\n",
            "\n",
            "ðŸ“Š COMPARAÃ‡ÃƒO PEPPER:\n",
            "   ORIGINAL:   Healthy=1034 ( 59.7%), Unhealthy= 698\n",
            "   BALANCEADO: Healthy=1448 ( 74.8%), Unhealthy= 489\n",
            "   MELHORIA:   + 414 amostras Healthy (+15.1%)\n",
            "\n",
            "ðŸŽ¯ RESUMO DAS MELHORIAS APLICADAS:\n",
            "============================================================\n",
            "   Potato:   7.0% â†’  50.0% (+43.0%) ðŸŸ¢ EXCELENTE\n",
            "   Tomato:   9.9% â†’  30.0% (+20.1%) ðŸŸ¡ MELHORADO\n",
            "   Pepper:  59.7% â†’  74.8% (+15.1%) ðŸŸ¢ EXCELENTE\n",
            "============================================================\n",
            "\n",
            "âœ… BALANCEAMENTO CONCLUÃDO!\n",
            "ðŸ“Š Datasets balanceados criados: ['potato', 'tomato', 'pepper']\n"
          ]
        }
      ],
      "source": [
        "# 3. APLICAR BALANCEAMENTO NOS DATASETS\n",
        "print(\"\\n=== APLICANDO BALANCEAMENTO OTIMIZADO ===\")\n",
        "\n",
        "# Criar versÃµes balanceadas dos datasets\n",
        "datasets_balanceados = {}\n",
        "comparacoes = {}\n",
        "\n",
        "# POTATO - EstratÃ©gia CRÃTICA (Data Aug + SMOTE)\n",
        "if dataset_potato is not None:\n",
        "    print(\"\\nðŸš¨ POTATO - Aplicando estratÃ©gia CRÃTICA\")\n",
        "    dataset_potato_aug = aplicar_data_augmentation_direcionada(dataset_potato, 'Potato')\n",
        "    dataset_potato_balanced = aplicar_smote_balanceamento(dataset_potato_aug, 'Potato', target_ratio=0.5)\n",
        "    datasets_balanceados['potato'] = dataset_potato_balanced\n",
        "    comparacoes['potato'] = comparar_distribuicoes(dataset_potato, dataset_potato_balanced, 'Potato')\n",
        "\n",
        "# TOMATO - EstratÃ©gia IMPORTANTE (Data Aug + SMOTE)\n",
        "if dataset_tomato is not None:\n",
        "    print(\"\\nâš ï¸ TOMATO - Aplicando estratÃ©gia IMPORTANTE\")\n",
        "    dataset_tomato_aug = aplicar_data_augmentation_direcionada(dataset_tomato, 'Tomato')\n",
        "    dataset_tomato_balanced = aplicar_smote_balanceamento(dataset_tomato_aug, 'Tomato', target_ratio=0.3)\n",
        "    datasets_balanceados['tomato'] = dataset_tomato_balanced\n",
        "    comparacoes['tomato'] = comparar_distribuicoes(dataset_tomato, dataset_tomato_balanced, 'Tomato')\n",
        "\n",
        "# PEPPER - EstratÃ©gia REFINAMENTO (Apenas Data Aug)\n",
        "if dataset_pepper is not None:\n",
        "    print(\"\\nâœ… PEPPER - Aplicando estratÃ©gia REFINAMENTO\")\n",
        "    dataset_pepper_balanced = aplicar_data_augmentation_direcionada(dataset_pepper, 'Pepper')\n",
        "    datasets_balanceados['pepper'] = dataset_pepper_balanced\n",
        "    comparacoes['pepper'] = comparar_distribuicoes(dataset_pepper, dataset_pepper_balanced, 'Pepper')\n",
        "\n",
        "# Resumo das melhorias\n",
        "print(\"\\nðŸŽ¯ RESUMO DAS MELHORIAS APLICADAS:\")\n",
        "print(\"=\" * 60)\n",
        "for especie, comp in comparacoes.items():\n",
        "    original_ratio = comp['original']['ratio']\n",
        "    balanced_ratio = comp['balanced']['ratio']\n",
        "    melhoria = balanced_ratio - original_ratio\n",
        "    status = \"ðŸŸ¢ EXCELENTE\" if balanced_ratio > 40 else \"ðŸŸ¡ MELHORADO\" if melhoria > 10 else \"ðŸ”´ INSUFICIENTE\"\n",
        "    print(f\"   {especie.capitalize()}: {original_ratio:5.1f}% â†’ {balanced_ratio:5.1f}% ({melhoria:+5.1f}%) {status}\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "print(f\"\\nâœ… BALANCEAMENTO CONCLUÃDO!\")\n",
        "print(f\"ðŸ“Š Datasets balanceados criados: {list(datasets_balanceados.keys())}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=== CRIANDO GERADORES COM DADOS BALANCEADOS ===\n",
            "Found 7076 validated image filenames belonging to 2 classes.\n",
            "Found 1512 validated image filenames belonging to 2 classes.\n",
            "Found 1516 validated image filenames belonging to 2 classes.\n",
            "âœ… Geradores Tomato criados com dados balanceados e augmentaÃ§Ã£o otimizada\n",
            "Found 1372 validated image filenames belonging to 2 classes.\n",
            "Found 294 validated image filenames belonging to 2 classes.\n",
            "Found 294 validated image filenames belonging to 2 classes.\n",
            "âœ… Geradores Potato criados com dados balanceados e augmentaÃ§Ã£o otimizada\n",
            "Found 1937 validated image filenames belonging to 2 classes.\n",
            "Found 414 validated image filenames belonging to 2 classes.\n",
            "Found 415 validated image filenames belonging to 2 classes.\n",
            "âœ… Geradores Pepper criados com dados balanceados e augmentaÃ§Ã£o otimizada\n",
            "\n",
            "ðŸŽ¯ Geradores otimizados criados com augmentaÃ§Ã£o direcionada e dados balanceados!\n",
            "\n",
            "=== CRIANDO MODELOS BINÃRIOS OTIMIZADOS ===\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "I0000 00:00:1752123905.630191   58193 gpu_device.cc:2019] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 4047 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 2060, pci bus id: 0000:01:00.0, compute capability: 7.5\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "âœ… Modelo binÃ¡rio Tomato: 24,136,961 parÃ¢metros\n",
            "âœ… Modelo binÃ¡rio Potato: 24,136,961 parÃ¢metros\n",
            "âœ… Modelo binÃ¡rio Pepper: 24,136,961 parÃ¢metros\n",
            "\n",
            "=== CALCULANDO CLASS WEIGHTS OTIMIZADOS ===\n",
            "   Class weights: Healthy=1.667, Unhealthy=0.714\n",
            "   Class weights: Healthy=1.000, Unhealthy=1.000\n",
            "   Class weights: Healthy=0.669, Unhealthy=1.981\n",
            "âœ… Class weights calculados com base nos dados balanceados\n"
          ]
        }
      ],
      "source": [
        "# 4. ARQUITETURA E TREINAMENTO COM DADOS BALANCEADOS\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.applications import ResNet50\n",
        "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D, Dropout, BatchNormalization\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n",
        "from tensorflow.keras.regularizers import l2\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "def criar_augmentacao_otimizada(especie):\n",
        "    \"\"\"Cria augmentaÃ§Ã£o otimizada baseada na espÃ©cie\"\"\"\n",
        "    \n",
        "    if especie.lower() == 'potato':\n",
        "        # AugmentaÃ§Ã£o agressiva para Potato (problema crÃ­tico)\n",
        "        return ImageDataGenerator(\n",
        "            rescale=1./255,\n",
        "            rotation_range=30,\n",
        "            width_shift_range=0.3,\n",
        "            height_shift_range=0.3,\n",
        "            horizontal_flip=True,\n",
        "            vertical_flip=True,\n",
        "            zoom_range=0.3,\n",
        "            brightness_range=[0.6, 1.4],\n",
        "            shear_range=0.2,\n",
        "            fill_mode='nearest'\n",
        "        )\n",
        "    elif especie.lower() == 'tomato':\n",
        "        # AugmentaÃ§Ã£o moderada para Tomato (problema importante)\n",
        "        return ImageDataGenerator(\n",
        "            rescale=1./255,\n",
        "            rotation_range=25,\n",
        "            width_shift_range=0.2,\n",
        "            height_shift_range=0.2,\n",
        "            horizontal_flip=True,\n",
        "            vertical_flip=True,\n",
        "            zoom_range=0.2,\n",
        "            brightness_range=[0.7, 1.3],\n",
        "            fill_mode='nearest'\n",
        "        )\n",
        "    else:  # pepper\n",
        "        # AugmentaÃ§Ã£o conservadora para Pepper (refinamento)\n",
        "        return ImageDataGenerator(\n",
        "            rescale=1./255,\n",
        "            rotation_range=15,\n",
        "            width_shift_range=0.1,\n",
        "            height_shift_range=0.1,\n",
        "            horizontal_flip=True,\n",
        "            zoom_range=0.1,\n",
        "            brightness_range=[0.8, 1.2],\n",
        "            fill_mode='nearest'\n",
        "        )\n",
        "\n",
        "def criar_classificao_binaria(dataset, config, especie):\n",
        "    \"\"\"Cria geradores otimizados para classificaÃ§Ã£o binÃ¡ria com dados balanceados\"\"\"\n",
        "    \n",
        "    # Data augmentation otimizada por espÃ©cie\n",
        "    train_datagen = criar_augmentacao_otimizada(especie)\n",
        "    \n",
        "    val_test_datagen = ImageDataGenerator(rescale=1./255)\n",
        "    \n",
        "    # DataFrames\n",
        "    train_df = pd.DataFrame({'filename': dataset['train']['X'], 'class': dataset['train']['y']})\n",
        "    val_df = pd.DataFrame({'filename': dataset['val']['X'], 'class': dataset['val']['y']})\n",
        "    test_df = pd.DataFrame({'filename': dataset['test']['X'], 'class': dataset['test']['y']})\n",
        "    \n",
        "    # Geradores binÃ¡rios\n",
        "    train_gen = train_datagen.flow_from_dataframe(\n",
        "        train_df, x_col='filename', y_col='class',\n",
        "        target_size=(config['img_height'], config['img_width']),\n",
        "        batch_size=config['batch_size'],\n",
        "        class_mode='binary', shuffle=True, seed=42\n",
        "    )\n",
        "    \n",
        "    val_gen = val_test_datagen.flow_from_dataframe(\n",
        "        val_df, x_col='filename', y_col='class',\n",
        "        target_size=(config['img_height'], config['img_width']),\n",
        "        batch_size=config['batch_size'],\n",
        "        class_mode='binary', shuffle=False, seed=42\n",
        "    )\n",
        "    \n",
        "    test_gen = val_test_datagen.flow_from_dataframe(\n",
        "        test_df, x_col='filename', y_col='class',\n",
        "        target_size=(config['img_height'], config['img_width']),\n",
        "        batch_size=config['batch_size'],\n",
        "        class_mode='binary', shuffle=False, seed=42\n",
        "    )\n",
        "    \n",
        "    return train_gen, val_gen, test_gen\n",
        "\n",
        "def criar_modelo(especie_nome):\n",
        "    \"\"\"Cria modelo de classificaÃ§Ã£o binÃ¡ria\"\"\"\n",
        "    base_model = ResNet50(\n",
        "        weights='imagenet',\n",
        "        include_top=False,\n",
        "        input_shape=(224, 224, 3)\n",
        "    )\n",
        "    \n",
        "    # Descongelar Ãºltimas camadas\n",
        "    base_model.trainable = True\n",
        "    for layer in base_model.layers[:-15]:\n",
        "        layer.trainable = False\n",
        "    \n",
        "    # Arquitetura otimizada para classificaÃ§Ã£o binÃ¡ria\n",
        "    x = base_model.output\n",
        "    x = GlobalAveragePooling2D()(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = Dense(256, activation='relu', kernel_regularizer=l2(0.001))(x)\n",
        "    x = Dropout(0.6)(x)\n",
        "    x = Dense(64, activation='relu', kernel_regularizer=l2(0.001))(x)\n",
        "    x = Dropout(0.5)(x)\n",
        "    \n",
        "    # SaÃ­da binÃ¡ria com sigmoid\n",
        "    predictions = Dense(1, activation='sigmoid', name=f'output_{especie_nome}')(x)\n",
        "    \n",
        "    modelo = Model(inputs=base_model.input, outputs=predictions)\n",
        "    \n",
        "    print(f\"âœ… Modelo binÃ¡rio {especie_nome}: {modelo.count_params():,} parÃ¢metros\")\n",
        "    return modelo\n",
        "\n",
        "def calcular_class_weights(dataset):\n",
        "    \"\"\"Calcula class weights balanceados\"\"\"\n",
        "    healthy_count = sum(1 for label in dataset['train']['y'] if label == 'healthy')\n",
        "    unhealthy_count = len(dataset['train']['y']) - healthy_count\n",
        "    \n",
        "    total = len(dataset['train']['y'])\n",
        "    weight_healthy = total / (2 * healthy_count)\n",
        "    weight_unhealthy = total / (2 * unhealthy_count)\n",
        "    \n",
        "    class_weights = {0: weight_healthy, 1: weight_unhealthy}  # 0=healthy, 1=unhealthy\n",
        "    \n",
        "    print(f\"   Class weights: Healthy={weight_healthy:.3f}, Unhealthy={weight_unhealthy:.3f}\")\n",
        "    return class_weights\n",
        "\n",
        "# Criar geradores com dados balanceados\n",
        "print(\"=== CRIANDO GERADORES COM DADOS BALANCEADOS ===\")\n",
        "\n",
        "# Usar datasets balanceados se disponÃ­veis, senÃ£o usar originais\n",
        "dataset_tomato_final = datasets_balanceados.get('tomato', dataset_tomato)\n",
        "dataset_potato_final = datasets_balanceados.get('potato', dataset_potato)\n",
        "dataset_pepper_final = datasets_balanceados.get('pepper', dataset_pepper)\n",
        "\n",
        "# Criar geradores otimizados\n",
        "if dataset_tomato_final:\n",
        "    train_gen_tomato, val_gen_tomato, test_gen_tomato = criar_classificao_binaria(dataset_tomato_final, config, 'tomato')\n",
        "    print(\"âœ… Geradores Tomato criados com dados balanceados e augmentaÃ§Ã£o otimizada\")\n",
        "\n",
        "if dataset_potato_final:\n",
        "    train_gen_potato, val_gen_potato, test_gen_potato = criar_classificao_binaria(dataset_potato_final, config, 'potato')\n",
        "    print(\"âœ… Geradores Potato criados com dados balanceados e augmentaÃ§Ã£o otimizada\")\n",
        "\n",
        "if dataset_pepper_final:\n",
        "    train_gen_pepper, val_gen_pepper, test_gen_pepper = criar_classificao_binaria(dataset_pepper_final, config, 'pepper')\n",
        "    print(\"âœ… Geradores Pepper criados com dados balanceados e augmentaÃ§Ã£o otimizada\")\n",
        "\n",
        "print(f\"\\nðŸŽ¯ Geradores otimizados criados com augmentaÃ§Ã£o direcionada e dados balanceados!\")\n",
        "\n",
        "# Criar modelos\n",
        "print(\"\\n=== CRIANDO MODELOS BINÃRIOS OTIMIZADOS ===\")\n",
        "modelo_tomato = criar_modelo('Tomato')\n",
        "modelo_potato = criar_modelo('Potato')\n",
        "modelo_pepper = criar_modelo('Pepper')\n",
        "\n",
        "# Calcular class weights otimizados para dados balanceados\n",
        "print(\"\\n=== CALCULANDO CLASS WEIGHTS OTIMIZADOS ===\")\n",
        "cw_tomato = calcular_class_weights(dataset_tomato_final) if dataset_tomato_final else None\n",
        "cw_potato = calcular_class_weights(dataset_potato_final) if dataset_potato_final else None\n",
        "cw_pepper = calcular_class_weights(dataset_pepper_final) if dataset_pepper_final else None\n",
        "\n",
        "print(\"âœ… Class weights calculados com base nos dados balanceados\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=== TREINAMENTO DOS MODELOS COM DADOS BALANCEADOS ===\n",
            "ðŸŽ¯ EXPECTATIVAS DE MELHORIA:\n",
            "   ðŸš¨ Potato: 0% â†’ 40-60% recall Healthy\n",
            "   âš ï¸ Tomato: 56% â†’ 70-80% recall Healthy\n",
            "   âœ… Pepper: 80% â†’ 85-90% recall Healthy\n",
            "\n",
            "\n",
            "ðŸš€ Treinando Tomato...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/gustavo/.local/lib/python3.10/site-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
            "  self._warn_if_super_not_called()\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/40\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "I0000 00:00:1752123918.223674   58276 service.cc:152] XLA service 0x72300c056c30 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
            "I0000 00:00:1752123918.223714   58276 service.cc:160]   StreamExecutor device (0): NVIDIA GeForce RTX 2060, Compute Capability 7.5\n",
            "2025-07-10 02:05:18.491340: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:269] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
            "I0000 00:00:1752123919.939496   58276 cuda_dnn.cc:529] Loaded cuDNN version 90300\n",
            "2025-07-10 02:05:21.326348: I external/local_xla/xla/service/gpu/autotuning/conv_algorithm_picker.cc:549] Omitted potentially buggy algorithm eng14{k25=0} for conv %cudnn-conv-bias-activation.165 = (f32[32,64,56,56]{3,2,1,0}, u8[0]{0}) custom-call(f32[32,64,56,56]{3,2,1,0} %bitcast.14083, f32[64,64,3,3]{3,2,1,0} %bitcast.14090, f32[64]{0} %bitcast.14092), window={size=3x3 pad=1_1x1_1}, dim_labels=bf01_oi01->bf01, custom_call_target=\"__cudnn$convBiasActivationForward\", metadata={op_type=\"Conv2D\" op_name=\"functional_1/conv2_block1_2_conv_1/convolution\" source_file=\"/home/gustavo/.local/lib/python3.10/site-packages/tensorflow/python/framework/ops.py\" source_line=1200}, backend_config={\"operation_queue_id\":\"0\",\"wait_on_operation_queues\":[],\"cudnn_conv_backend_config\":{\"conv_result_scale\":1,\"activation_mode\":\"kNone\",\"side_input_scale\":0,\"leakyrelu_alpha\":0},\"force_earliest_schedule\":false}\n",
            "2025-07-10 02:05:21.743054: I external/local_xla/xla/service/gpu/autotuning/conv_algorithm_picker.cc:549] Omitted potentially buggy algorithm eng14{k25=0} for conv %cudnn-conv-bias-activation.175 = (f32[32,128,28,28]{3,2,1,0}, u8[0]{0}) custom-call(f32[32,128,28,28]{3,2,1,0} %bitcast.14488, f32[128,128,3,3]{3,2,1,0} %bitcast.14495, f32[128]{0} %bitcast.14497), window={size=3x3 pad=1_1x1_1}, dim_labels=bf01_oi01->bf01, custom_call_target=\"__cudnn$convBiasActivationForward\", metadata={op_type=\"Conv2D\" op_name=\"functional_1/conv3_block1_2_conv_1/convolution\" source_file=\"/home/gustavo/.local/lib/python3.10/site-packages/tensorflow/python/framework/ops.py\" source_line=1200}, backend_config={\"operation_queue_id\":\"0\",\"wait_on_operation_queues\":[],\"cudnn_conv_backend_config\":{\"conv_result_scale\":1,\"activation_mode\":\"kNone\",\"side_input_scale\":0,\"leakyrelu_alpha\":0},\"force_earliest_schedule\":false}\n",
            "2025-07-10 02:05:22.201017: I external/local_xla/xla/service/gpu/autotuning/conv_algorithm_picker.cc:549] Omitted potentially buggy algorithm eng14{k25=0} for conv %cudnn-conv-bias-activation.188 = (f32[32,256,14,14]{3,2,1,0}, u8[0]{0}) custom-call(f32[32,256,14,14]{3,2,1,0} %bitcast.15016, f32[256,256,3,3]{3,2,1,0} %bitcast.15023, f32[256]{0} %bitcast.15025), window={size=3x3 pad=1_1x1_1}, dim_labels=bf01_oi01->bf01, custom_call_target=\"__cudnn$convBiasActivationForward\", metadata={op_type=\"Conv2D\" op_name=\"functional_1/conv4_block1_2_conv_1/convolution\" source_file=\"/home/gustavo/.local/lib/python3.10/site-packages/tensorflow/python/framework/ops.py\" source_line=1200}, backend_config={\"operation_queue_id\":\"0\",\"wait_on_operation_queues\":[],\"cudnn_conv_backend_config\":{\"conv_result_scale\":1,\"activation_mode\":\"kNone\",\"side_input_scale\":0,\"leakyrelu_alpha\":0},\"force_earliest_schedule\":false}\n",
            "2025-07-10 02:05:22.643359: I external/local_xla/xla/service/gpu/autotuning/conv_algorithm_picker.cc:549] Omitted potentially buggy algorithm eng14{k25=0} for conv %cudnn-conv-bias-activation.207 = (f32[32,512,7,7]{3,2,1,0}, u8[0]{0}) custom-call(f32[32,512,7,7]{3,2,1,0} %bitcast.15790, f32[512,512,3,3]{3,2,1,0} %bitcast.15797, f32[512]{0} %bitcast.15799), window={size=3x3 pad=1_1x1_1}, dim_labels=bf01_oi01->bf01, custom_call_target=\"__cudnn$convBiasActivationForward\", metadata={op_type=\"Conv2D\" op_name=\"functional_1/conv5_block1_2_conv_1/convolution\" source_file=\"/home/gustavo/.local/lib/python3.10/site-packages/tensorflow/python/framework/ops.py\" source_line=1200}, backend_config={\"operation_queue_id\":\"0\",\"wait_on_operation_queues\":[],\"cudnn_conv_backend_config\":{\"conv_result_scale\":1,\"activation_mode\":\"kNone\",\"side_input_scale\":0,\"leakyrelu_alpha\":0},\"force_earliest_schedule\":false}\n",
            "I0000 00:00:1752123926.553818   58276 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m170/222\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”â”â”â”\u001b[0m \u001b[1m13s\u001b[0m 254ms/step - accuracy: 0.6676 - loss: 1.2270"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-07-10 02:06:11.777126: I external/local_xla/xla/service/gpu/autotuning/conv_algorithm_picker.cc:549] Omitted potentially buggy algorithm eng14{k25=0} for conv %cudnn-conv-bias-activation.165 = (f32[4,64,56,56]{3,2,1,0}, u8[0]{0}) custom-call(f32[4,64,56,56]{3,2,1,0} %bitcast.14083, f32[64,64,3,3]{3,2,1,0} %bitcast.14090, f32[64]{0} %bitcast.14092), window={size=3x3 pad=1_1x1_1}, dim_labels=bf01_oi01->bf01, custom_call_target=\"__cudnn$convBiasActivationForward\", metadata={op_type=\"Conv2D\" op_name=\"functional_1/conv2_block1_2_conv_1/convolution\" source_file=\"/home/gustavo/.local/lib/python3.10/site-packages/tensorflow/python/framework/ops.py\" source_line=1200}, backend_config={\"operation_queue_id\":\"0\",\"wait_on_operation_queues\":[],\"cudnn_conv_backend_config\":{\"conv_result_scale\":1,\"activation_mode\":\"kNone\",\"side_input_scale\":0,\"leakyrelu_alpha\":0},\"force_earliest_schedule\":false}\n",
            "2025-07-10 02:06:11.988250: I external/local_xla/xla/service/gpu/autotuning/conv_algorithm_picker.cc:549] Omitted potentially buggy algorithm eng14{k25=0} for conv %cudnn-conv-bias-activation.175 = (f32[4,128,28,28]{3,2,1,0}, u8[0]{0}) custom-call(f32[4,128,28,28]{3,2,1,0} %bitcast.14488, f32[128,128,3,3]{3,2,1,0} %bitcast.14495, f32[128]{0} %bitcast.14497), window={size=3x3 pad=1_1x1_1}, dim_labels=bf01_oi01->bf01, custom_call_target=\"__cudnn$convBiasActivationForward\", metadata={op_type=\"Conv2D\" op_name=\"functional_1/conv3_block1_2_conv_1/convolution\" source_file=\"/home/gustavo/.local/lib/python3.10/site-packages/tensorflow/python/framework/ops.py\" source_line=1200}, backend_config={\"operation_queue_id\":\"0\",\"wait_on_operation_queues\":[],\"cudnn_conv_backend_config\":{\"conv_result_scale\":1,\"activation_mode\":\"kNone\",\"side_input_scale\":0,\"leakyrelu_alpha\":0},\"force_earliest_schedule\":false}\n",
            "2025-07-10 02:06:12.193810: I external/local_xla/xla/service/gpu/autotuning/conv_algorithm_picker.cc:549] Omitted potentially buggy algorithm eng14{k25=0} for conv %cudnn-conv-bias-activation.188 = (f32[4,256,14,14]{3,2,1,0}, u8[0]{0}) custom-call(f32[4,256,14,14]{3,2,1,0} %bitcast.15016, f32[256,256,3,3]{3,2,1,0} %bitcast.15023, f32[256]{0} %bitcast.15025), window={size=3x3 pad=1_1x1_1}, dim_labels=bf01_oi01->bf01, custom_call_target=\"__cudnn$convBiasActivationForward\", metadata={op_type=\"Conv2D\" op_name=\"functional_1/conv4_block1_2_conv_1/convolution\" source_file=\"/home/gustavo/.local/lib/python3.10/site-packages/tensorflow/python/framework/ops.py\" source_line=1200}, backend_config={\"operation_queue_id\":\"0\",\"wait_on_operation_queues\":[],\"cudnn_conv_backend_config\":{\"conv_result_scale\":1,\"activation_mode\":\"kNone\",\"side_input_scale\":0,\"leakyrelu_alpha\":0},\"force_earliest_schedule\":false}\n",
            "2025-07-10 02:06:12.429575: I external/local_xla/xla/service/gpu/autotuning/conv_algorithm_picker.cc:549] Omitted potentially buggy algorithm eng14{k25=0} for conv %cudnn-conv-bias-activation.207 = (f32[4,512,7,7]{3,2,1,0}, u8[0]{0}) custom-call(f32[4,512,7,7]{3,2,1,0} %bitcast.15790, f32[512,512,3,3]{3,2,1,0} %bitcast.15797, f32[512]{0} %bitcast.15799), window={size=3x3 pad=1_1x1_1}, dim_labels=bf01_oi01->bf01, custom_call_target=\"__cudnn$convBiasActivationForward\", metadata={op_type=\"Conv2D\" op_name=\"functional_1/conv5_block1_2_conv_1/convolution\" source_file=\"/home/gustavo/.local/lib/python3.10/site-packages/tensorflow/python/framework/ops.py\" source_line=1200}, backend_config={\"operation_queue_id\":\"0\",\"wait_on_operation_queues\":[],\"cudnn_conv_backend_config\":{\"conv_result_scale\":1,\"activation_mode\":\"kNone\",\"side_input_scale\":0,\"leakyrelu_alpha\":0},\"force_earliest_schedule\":false}\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m222/222\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 290ms/step - accuracy: 0.6868 - loss: 1.1926"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-07-10 02:06:37.906207: I external/local_xla/xla/service/gpu/autotuning/conv_algorithm_picker.cc:549] Omitted potentially buggy algorithm eng14{k25=0} for conv %cudnn-conv-bias-activation.162 = (f32[8,64,56,56]{3,2,1,0}, u8[0]{0}) custom-call(f32[8,64,56,56]{3,2,1,0} %bitcast.4834, f32[64,64,3,3]{3,2,1,0} %bitcast.4841, f32[64]{0} %bitcast.4843), window={size=3x3 pad=1_1x1_1}, dim_labels=bf01_oi01->bf01, custom_call_target=\"__cudnn$convBiasActivationForward\", metadata={op_type=\"Conv2D\" op_name=\"functional_1/conv2_block1_2_conv_1/convolution\" source_file=\"/home/gustavo/.local/lib/python3.10/site-packages/tensorflow/python/framework/ops.py\" source_line=1200}, backend_config={\"operation_queue_id\":\"0\",\"wait_on_operation_queues\":[],\"cudnn_conv_backend_config\":{\"conv_result_scale\":1,\"activation_mode\":\"kNone\",\"side_input_scale\":0,\"leakyrelu_alpha\":0},\"force_earliest_schedule\":false}\n",
            "2025-07-10 02:06:38.131334: I external/local_xla/xla/service/gpu/autotuning/conv_algorithm_picker.cc:549] Omitted potentially buggy algorithm eng14{k25=0} for conv %cudnn-conv-bias-activation.172 = (f32[8,128,28,28]{3,2,1,0}, u8[0]{0}) custom-call(f32[8,128,28,28]{3,2,1,0} %bitcast.5239, f32[128,128,3,3]{3,2,1,0} %bitcast.5246, f32[128]{0} %bitcast.5248), window={size=3x3 pad=1_1x1_1}, dim_labels=bf01_oi01->bf01, custom_call_target=\"__cudnn$convBiasActivationForward\", metadata={op_type=\"Conv2D\" op_name=\"functional_1/conv3_block1_2_conv_1/convolution\" source_file=\"/home/gustavo/.local/lib/python3.10/site-packages/tensorflow/python/framework/ops.py\" source_line=1200}, backend_config={\"operation_queue_id\":\"0\",\"wait_on_operation_queues\":[],\"cudnn_conv_backend_config\":{\"conv_result_scale\":1,\"activation_mode\":\"kNone\",\"side_input_scale\":0,\"leakyrelu_alpha\":0},\"force_earliest_schedule\":false}\n",
            "2025-07-10 02:06:38.396923: I external/local_xla/xla/service/gpu/autotuning/conv_algorithm_picker.cc:549] Omitted potentially buggy algorithm eng14{k25=0} for conv %cudnn-conv-bias-activation.185 = (f32[8,256,14,14]{3,2,1,0}, u8[0]{0}) custom-call(f32[8,256,14,14]{3,2,1,0} %bitcast.5767, f32[256,256,3,3]{3,2,1,0} %bitcast.5774, f32[256]{0} %bitcast.5776), window={size=3x3 pad=1_1x1_1}, dim_labels=bf01_oi01->bf01, custom_call_target=\"__cudnn$convBiasActivationForward\", metadata={op_type=\"Conv2D\" op_name=\"functional_1/conv4_block1_2_conv_1/convolution\" source_file=\"/home/gustavo/.local/lib/python3.10/site-packages/tensorflow/python/framework/ops.py\" source_line=1200}, backend_config={\"operation_queue_id\":\"0\",\"wait_on_operation_queues\":[],\"cudnn_conv_backend_config\":{\"conv_result_scale\":1,\"activation_mode\":\"kNone\",\"side_input_scale\":0,\"leakyrelu_alpha\":0},\"force_earliest_schedule\":false}\n",
            "2025-07-10 02:06:38.681529: I external/local_xla/xla/service/gpu/autotuning/conv_algorithm_picker.cc:549] Omitted potentially buggy algorithm eng14{k25=0} for conv %cudnn-conv-bias-activation.204 = (f32[8,512,7,7]{3,2,1,0}, u8[0]{0}) custom-call(f32[8,512,7,7]{3,2,1,0} %bitcast.6541, f32[512,512,3,3]{3,2,1,0} %bitcast.6548, f32[512]{0} %bitcast.6550), window={size=3x3 pad=1_1x1_1}, dim_labels=bf01_oi01->bf01, custom_call_target=\"__cudnn$convBiasActivationForward\", metadata={op_type=\"Conv2D\" op_name=\"functional_1/conv5_block1_2_conv_1/convolution\" source_file=\"/home/gustavo/.local/lib/python3.10/site-packages/tensorflow/python/framework/ops.py\" source_line=1200}, backend_config={\"operation_queue_id\":\"0\",\"wait_on_operation_queues\":[],\"cudnn_conv_backend_config\":{\"conv_result_scale\":1,\"activation_mode\":\"kNone\",\"side_input_scale\":0,\"leakyrelu_alpha\":0},\"force_earliest_schedule\":false}\n",
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m222/222\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m89s\u001b[0m 339ms/step - accuracy: 0.6871 - loss: 1.1921 - val_accuracy: 0.6362 - val_loss: 1.1919 - learning_rate: 1.0000e-04\n",
            "Epoch 2/40\n",
            "\u001b[1m222/222\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 257ms/step - accuracy: 0.8242 - loss: 0.9276"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m222/222\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m63s\u001b[0m 285ms/step - accuracy: 0.8242 - loss: 0.9275 - val_accuracy: 0.9087 - val_loss: 0.7782 - learning_rate: 1.0000e-04\n",
            "Epoch 3/40\n",
            "\u001b[1m222/222\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 264ms/step - accuracy: 0.8599 - loss: 0.8521 - val_accuracy: 0.8353 - val_loss: 0.8814 - learning_rate: 1.0000e-04\n",
            "Epoch 4/40\n",
            "\u001b[1m222/222\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 262ms/step - accuracy: 0.8688 - loss: 0.8293 - val_accuracy: 0.8889 - val_loss: 0.9157 - learning_rate: 1.0000e-04\n",
            "Epoch 5/40\n",
            "\u001b[1m222/222\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 252ms/step - accuracy: 0.8784 - loss: 0.7954"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m222/222\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m60s\u001b[0m 271ms/step - accuracy: 0.8784 - loss: 0.7953 - val_accuracy: 0.9325 - val_loss: 0.6775 - learning_rate: 1.0000e-04\n",
            "Epoch 6/40\n",
            "\u001b[1m222/222\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 266ms/step - accuracy: 0.8829 - loss: 0.7584"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m222/222\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m63s\u001b[0m 283ms/step - accuracy: 0.8829 - loss: 0.7584 - val_accuracy: 0.9345 - val_loss: 0.7236 - learning_rate: 1.0000e-04\n",
            "Epoch 7/40\n",
            "\u001b[1m222/222\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m56s\u001b[0m 253ms/step - accuracy: 0.8885 - loss: 0.7413 - val_accuracy: 0.8955 - val_loss: 0.7156 - learning_rate: 1.0000e-04\n",
            "Epoch 8/40\n",
            "\u001b[1m222/222\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 265ms/step - accuracy: 0.8950 - loss: 0.7230 - val_accuracy: 0.6640 - val_loss: 1.2426 - learning_rate: 1.0000e-04\n",
            "Epoch 9/40\n",
            "\u001b[1m222/222\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m60s\u001b[0m 272ms/step - accuracy: 0.8938 - loss: 0.6965 - val_accuracy: 0.6792 - val_loss: 1.1086 - learning_rate: 1.0000e-04\n",
            "Epoch 10/40\n",
            "\u001b[1m222/222\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 240ms/step - accuracy: 0.9043 - loss: 0.6723"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m222/222\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m57s\u001b[0m 256ms/step - accuracy: 0.9042 - loss: 0.6723 - val_accuracy: 0.9438 - val_loss: 0.5564 - learning_rate: 1.0000e-04\n",
            "Epoch 11/40\n",
            "\u001b[1m222/222\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 266ms/step - accuracy: 0.9059 - loss: 0.6525 - val_accuracy: 0.8380 - val_loss: 0.7422 - learning_rate: 1.0000e-04\n",
            "Epoch 12/40\n",
            "\u001b[1m222/222\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m61s\u001b[0m 273ms/step - accuracy: 0.9064 - loss: 0.6511 - val_accuracy: 0.9101 - val_loss: 0.6813 - learning_rate: 1.0000e-04\n",
            "Epoch 13/40\n",
            "\u001b[1m222/222\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m55s\u001b[0m 249ms/step - accuracy: 0.9046 - loss: 0.6310 - val_accuracy: 0.8922 - val_loss: 0.6076 - learning_rate: 1.0000e-04\n",
            "Epoch 14/40\n",
            "\u001b[1m222/222\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 267ms/step - accuracy: 0.9139 - loss: 0.5924 - val_accuracy: 0.8419 - val_loss: 0.6871 - learning_rate: 1.0000e-04\n",
            "Epoch 15/40\n",
            "\u001b[1m222/222\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m61s\u001b[0m 275ms/step - accuracy: 0.9084 - loss: 0.5842 - val_accuracy: 0.5873 - val_loss: 1.4234 - learning_rate: 1.0000e-04\n",
            "Epoch 16/40\n",
            "\u001b[1m222/222\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 262ms/step - accuracy: 0.9119 - loss: 0.5617 - val_accuracy: 0.9061 - val_loss: 0.5572 - learning_rate: 1.0000e-04\n",
            "Epoch 17/40\n",
            "\u001b[1m222/222\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m61s\u001b[0m 273ms/step - accuracy: 0.9122 - loss: 0.5395 - val_accuracy: 0.7235 - val_loss: 0.9746 - learning_rate: 3.0000e-05\n",
            "Epoch 18/40\n",
            "\u001b[1m222/222\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 262ms/step - accuracy: 0.9195 - loss: 0.5182 - val_accuracy: 0.8757 - val_loss: 0.6041 - learning_rate: 3.0000e-05\n",
            "Epoch 19/40\n",
            "\u001b[1m222/222\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m56s\u001b[0m 253ms/step - accuracy: 0.9149 - loss: 0.5198 - val_accuracy: 0.9213 - val_loss: 0.5125 - learning_rate: 3.0000e-05\n",
            "Epoch 20/40\n",
            "\u001b[1m222/222\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m61s\u001b[0m 277ms/step - accuracy: 0.9289 - loss: 0.5145 - val_accuracy: 0.8757 - val_loss: 0.5828 - learning_rate: 3.0000e-05\n",
            "Epoch 21/40\n",
            "\u001b[1m222/222\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m62s\u001b[0m 279ms/step - accuracy: 0.9302 - loss: 0.4997 - val_accuracy: 0.7460 - val_loss: 0.9119 - learning_rate: 3.0000e-05\n",
            "Epoch 22/40\n",
            "\u001b[1m222/222\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 262ms/step - accuracy: 0.9318 - loss: 0.4804"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m222/222\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m62s\u001b[0m 279ms/step - accuracy: 0.9317 - loss: 0.4805 - val_accuracy: 0.9570 - val_loss: 0.4221 - learning_rate: 3.0000e-05\n",
            "Epoch 23/40\n",
            "\u001b[1m222/222\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 262ms/step - accuracy: 0.9268 - loss: 0.4771 - val_accuracy: 0.4788 - val_loss: 1.8924 - learning_rate: 3.0000e-05\n",
            "Epoch 24/40\n",
            "\u001b[1m222/222\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m60s\u001b[0m 269ms/step - accuracy: 0.9334 - loss: 0.4691 - val_accuracy: 0.6971 - val_loss: 1.0903 - learning_rate: 3.0000e-05\n",
            "Epoch 25/40\n",
            "\u001b[1m222/222\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m60s\u001b[0m 269ms/step - accuracy: 0.9264 - loss: 0.4691 - val_accuracy: 0.7553 - val_loss: 0.8627 - learning_rate: 3.0000e-05\n",
            "Epoch 26/40\n",
            "\u001b[1m222/222\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m57s\u001b[0m 256ms/step - accuracy: 0.9284 - loss: 0.4688 - val_accuracy: 0.9352 - val_loss: 0.4588 - learning_rate: 3.0000e-05\n",
            "Epoch 27/40\n",
            "\u001b[1m222/222\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 264ms/step - accuracy: 0.9351 - loss: 0.4472 - val_accuracy: 0.8175 - val_loss: 0.6962 - learning_rate: 3.0000e-05\n",
            "Epoch 28/40\n",
            "\u001b[1m222/222\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m61s\u001b[0m 273ms/step - accuracy: 0.9125 - loss: 0.5035 - val_accuracy: 0.7540 - val_loss: 0.8801 - learning_rate: 3.0000e-05\n",
            "Epoch 29/40\n",
            "\u001b[1m222/222\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m57s\u001b[0m 257ms/step - accuracy: 0.9342 - loss: 0.4466 - val_accuracy: 0.7989 - val_loss: 0.7526 - learning_rate: 9.0000e-06\n",
            "Epoch 30/40\n",
            "\u001b[1m222/222\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 267ms/step - accuracy: 0.9367 - loss: 0.4351 - val_accuracy: 0.7460 - val_loss: 0.9288 - learning_rate: 9.0000e-06\n",
            "Epoch 31/40\n",
            "\u001b[1m222/222\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 267ms/step - accuracy: 0.9416 - loss: 0.4293 - val_accuracy: 0.8519 - val_loss: 0.6370 - learning_rate: 9.0000e-06\n",
            "Epoch 32/40\n",
            "\u001b[1m222/222\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m57s\u001b[0m 256ms/step - accuracy: 0.9383 - loss: 0.4410 - val_accuracy: 0.7679 - val_loss: 0.8614 - learning_rate: 9.0000e-06\n",
            "Epoch 33/40\n",
            "\u001b[1m222/222\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 262ms/step - accuracy: 0.9410 - loss: 0.4213 - val_accuracy: 0.8889 - val_loss: 0.5392 - learning_rate: 9.0000e-06\n",
            "Epoch 34/40\n",
            "\u001b[1m222/222\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m62s\u001b[0m 277ms/step - accuracy: 0.9342 - loss: 0.4292 - val_accuracy: 0.8532 - val_loss: 0.6266 - learning_rate: 9.0000e-06\n",
            "Epoch 35/40\n",
            "\u001b[1m222/222\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 261ms/step - accuracy: 0.9419 - loss: 0.4136 - val_accuracy: 0.8393 - val_loss: 0.6681 - learning_rate: 2.7000e-06\n",
            "Epoch 36/40\n",
            "\u001b[1m222/222\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 266ms/step - accuracy: 0.9482 - loss: 0.4004 - val_accuracy: 0.8062 - val_loss: 0.7435 - learning_rate: 2.7000e-06\n",
            "Epoch 37/40\n",
            "\u001b[1m222/222\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m61s\u001b[0m 273ms/step - accuracy: 0.9450 - loss: 0.4051 - val_accuracy: 0.7857 - val_loss: 0.8045 - learning_rate: 2.7000e-06\n",
            "âœ… Tomato concluÃ­do! Melhor accuracy: 0.9570\n",
            "\n",
            "ðŸš€ Treinando Potato...\n",
            "Epoch 1/40\n",
            "\u001b[1m26/43\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”â”â”â”â”â”â”\u001b[0m \u001b[1m4s\u001b[0m 275ms/step - accuracy: 0.5155 - loss: 1.5517"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-07-10 02:42:37.215739: I external/local_xla/xla/service/gpu/autotuning/conv_algorithm_picker.cc:549] Omitted potentially buggy algorithm eng14{k25=0} for conv %cudnn-conv-bias-activation.165 = (f32[28,64,56,56]{3,2,1,0}, u8[0]{0}) custom-call(f32[28,64,56,56]{3,2,1,0} %bitcast.14083, f32[64,64,3,3]{3,2,1,0} %bitcast.14090, f32[64]{0} %bitcast.14092), window={size=3x3 pad=1_1x1_1}, dim_labels=bf01_oi01->bf01, custom_call_target=\"__cudnn$convBiasActivationForward\", metadata={op_type=\"Conv2D\" op_name=\"functional_1_1/conv2_block1_2_conv_1/convolution\" source_file=\"/home/gustavo/.local/lib/python3.10/site-packages/tensorflow/python/framework/ops.py\" source_line=1200}, backend_config={\"operation_queue_id\":\"0\",\"wait_on_operation_queues\":[],\"cudnn_conv_backend_config\":{\"conv_result_scale\":1,\"activation_mode\":\"kNone\",\"side_input_scale\":0,\"leakyrelu_alpha\":0},\"force_earliest_schedule\":false}\n",
            "2025-07-10 02:42:37.614883: I external/local_xla/xla/service/gpu/autotuning/conv_algorithm_picker.cc:549] Omitted potentially buggy algorithm eng14{k25=0} for conv %cudnn-conv-bias-activation.175 = (f32[28,128,28,28]{3,2,1,0}, u8[0]{0}) custom-call(f32[28,128,28,28]{3,2,1,0} %bitcast.14488, f32[128,128,3,3]{3,2,1,0} %bitcast.14495, f32[128]{0} %bitcast.14497), window={size=3x3 pad=1_1x1_1}, dim_labels=bf01_oi01->bf01, custom_call_target=\"__cudnn$convBiasActivationForward\", metadata={op_type=\"Conv2D\" op_name=\"functional_1_1/conv3_block1_2_conv_1/convolution\" source_file=\"/home/gustavo/.local/lib/python3.10/site-packages/tensorflow/python/framework/ops.py\" source_line=1200}, backend_config={\"operation_queue_id\":\"0\",\"wait_on_operation_queues\":[],\"cudnn_conv_backend_config\":{\"conv_result_scale\":1,\"activation_mode\":\"kNone\",\"side_input_scale\":0,\"leakyrelu_alpha\":0},\"force_earliest_schedule\":false}\n",
            "2025-07-10 02:42:38.075703: I external/local_xla/xla/service/gpu/autotuning/conv_algorithm_picker.cc:549] Omitted potentially buggy algorithm eng14{k25=0} for conv %cudnn-conv-bias-activation.188 = (f32[28,256,14,14]{3,2,1,0}, u8[0]{0}) custom-call(f32[28,256,14,14]{3,2,1,0} %bitcast.15016, f32[256,256,3,3]{3,2,1,0} %bitcast.15023, f32[256]{0} %bitcast.15025), window={size=3x3 pad=1_1x1_1}, dim_labels=bf01_oi01->bf01, custom_call_target=\"__cudnn$convBiasActivationForward\", metadata={op_type=\"Conv2D\" op_name=\"functional_1_1/conv4_block1_2_conv_1/convolution\" source_file=\"/home/gustavo/.local/lib/python3.10/site-packages/tensorflow/python/framework/ops.py\" source_line=1200}, backend_config={\"operation_queue_id\":\"0\",\"wait_on_operation_queues\":[],\"cudnn_conv_backend_config\":{\"conv_result_scale\":1,\"activation_mode\":\"kNone\",\"side_input_scale\":0,\"leakyrelu_alpha\":0},\"force_earliest_schedule\":false}\n",
            "2025-07-10 02:42:38.508696: I external/local_xla/xla/service/gpu/autotuning/conv_algorithm_picker.cc:549] Omitted potentially buggy algorithm eng14{k25=0} for conv %cudnn-conv-bias-activation.207 = (f32[28,512,7,7]{3,2,1,0}, u8[0]{0}) custom-call(f32[28,512,7,7]{3,2,1,0} %bitcast.15790, f32[512,512,3,3]{3,2,1,0} %bitcast.15797, f32[512]{0} %bitcast.15799), window={size=3x3 pad=1_1x1_1}, dim_labels=bf01_oi01->bf01, custom_call_target=\"__cudnn$convBiasActivationForward\", metadata={op_type=\"Conv2D\" op_name=\"functional_1_1/conv5_block1_2_conv_1/convolution\" source_file=\"/home/gustavo/.local/lib/python3.10/site-packages/tensorflow/python/framework/ops.py\" source_line=1200}, backend_config={\"operation_queue_id\":\"0\",\"wait_on_operation_queues\":[],\"cudnn_conv_backend_config\":{\"conv_result_scale\":1,\"activation_mode\":\"kNone\",\"side_input_scale\":0,\"leakyrelu_alpha\":0},\"force_earliest_schedule\":false}\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m43/43\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 552ms/step - accuracy: 0.5243 - loss: 1.5255"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-07-10 02:42:54.685852: I external/local_xla/xla/service/gpu/autotuning/conv_algorithm_picker.cc:549] Omitted potentially buggy algorithm eng14{k25=0} for conv %cudnn-conv-bias-activation.162 = (f32[6,64,56,56]{3,2,1,0}, u8[0]{0}) custom-call(f32[6,64,56,56]{3,2,1,0} %bitcast.4834, f32[64,64,3,3]{3,2,1,0} %bitcast.4841, f32[64]{0} %bitcast.4843), window={size=3x3 pad=1_1x1_1}, dim_labels=bf01_oi01->bf01, custom_call_target=\"__cudnn$convBiasActivationForward\", metadata={op_type=\"Conv2D\" op_name=\"functional_1_1/conv2_block1_2_conv_1/convolution\" source_file=\"/home/gustavo/.local/lib/python3.10/site-packages/tensorflow/python/framework/ops.py\" source_line=1200}, backend_config={\"operation_queue_id\":\"0\",\"wait_on_operation_queues\":[],\"cudnn_conv_backend_config\":{\"conv_result_scale\":1,\"activation_mode\":\"kNone\",\"side_input_scale\":0,\"leakyrelu_alpha\":0},\"force_earliest_schedule\":false}\n",
            "2025-07-10 02:42:54.905454: I external/local_xla/xla/service/gpu/autotuning/conv_algorithm_picker.cc:549] Omitted potentially buggy algorithm eng14{k25=0} for conv %cudnn-conv-bias-activation.172 = (f32[6,128,28,28]{3,2,1,0}, u8[0]{0}) custom-call(f32[6,128,28,28]{3,2,1,0} %bitcast.5239, f32[128,128,3,3]{3,2,1,0} %bitcast.5246, f32[128]{0} %bitcast.5248), window={size=3x3 pad=1_1x1_1}, dim_labels=bf01_oi01->bf01, custom_call_target=\"__cudnn$convBiasActivationForward\", metadata={op_type=\"Conv2D\" op_name=\"functional_1_1/conv3_block1_2_conv_1/convolution\" source_file=\"/home/gustavo/.local/lib/python3.10/site-packages/tensorflow/python/framework/ops.py\" source_line=1200}, backend_config={\"operation_queue_id\":\"0\",\"wait_on_operation_queues\":[],\"cudnn_conv_backend_config\":{\"conv_result_scale\":1,\"activation_mode\":\"kNone\",\"side_input_scale\":0,\"leakyrelu_alpha\":0},\"force_earliest_schedule\":false}\n",
            "2025-07-10 02:42:55.199565: I external/local_xla/xla/service/gpu/autotuning/conv_algorithm_picker.cc:549] Omitted potentially buggy algorithm eng14{k25=0} for conv %cudnn-conv-bias-activation.185 = (f32[6,256,14,14]{3,2,1,0}, u8[0]{0}) custom-call(f32[6,256,14,14]{3,2,1,0} %bitcast.5767, f32[256,256,3,3]{3,2,1,0} %bitcast.5774, f32[256]{0} %bitcast.5776), window={size=3x3 pad=1_1x1_1}, dim_labels=bf01_oi01->bf01, custom_call_target=\"__cudnn$convBiasActivationForward\", metadata={op_type=\"Conv2D\" op_name=\"functional_1_1/conv4_block1_2_conv_1/convolution\" source_file=\"/home/gustavo/.local/lib/python3.10/site-packages/tensorflow/python/framework/ops.py\" source_line=1200}, backend_config={\"operation_queue_id\":\"0\",\"wait_on_operation_queues\":[],\"cudnn_conv_backend_config\":{\"conv_result_scale\":1,\"activation_mode\":\"kNone\",\"side_input_scale\":0,\"leakyrelu_alpha\":0},\"force_earliest_schedule\":false}\n",
            "2025-07-10 02:42:55.500347: I external/local_xla/xla/service/gpu/autotuning/conv_algorithm_picker.cc:549] Omitted potentially buggy algorithm eng14{k25=0} for conv %cudnn-conv-bias-activation.204 = (f32[6,512,7,7]{3,2,1,0}, u8[0]{0}) custom-call(f32[6,512,7,7]{3,2,1,0} %bitcast.6541, f32[512,512,3,3]{3,2,1,0} %bitcast.6548, f32[512]{0} %bitcast.6550), window={size=3x3 pad=1_1x1_1}, dim_labels=bf01_oi01->bf01, custom_call_target=\"__cudnn$convBiasActivationForward\", metadata={op_type=\"Conv2D\" op_name=\"functional_1_1/conv5_block1_2_conv_1/convolution\" source_file=\"/home/gustavo/.local/lib/python3.10/site-packages/tensorflow/python/framework/ops.py\" source_line=1200}, backend_config={\"operation_queue_id\":\"0\",\"wait_on_operation_queues\":[],\"cudnn_conv_backend_config\":{\"conv_result_scale\":1,\"activation_mode\":\"kNone\",\"side_input_scale\":0,\"leakyrelu_alpha\":0},\"force_earliest_schedule\":false}\n",
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m43/43\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 727ms/step - accuracy: 0.5249 - loss: 1.5242 - val_accuracy: 0.5000 - val_loss: 1.2473 - learning_rate: 1.0000e-04\n",
            "Epoch 2/40\n",
            "\u001b[1m43/43\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 241ms/step - accuracy: 0.6047 - loss: 1.3653 - val_accuracy: 0.5000 - val_loss: 1.2473 - learning_rate: 1.0000e-04\n",
            "Epoch 3/40\n",
            "\u001b[1m43/43\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 318ms/step - accuracy: 0.6232 - loss: 1.3635 - val_accuracy: 0.5000 - val_loss: 1.2427 - learning_rate: 1.0000e-04\n",
            "Epoch 4/40\n",
            "\u001b[1m43/43\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 239ms/step - accuracy: 0.6454 - loss: 1.2627"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m43/43\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 274ms/step - accuracy: 0.6455 - loss: 1.2630 - val_accuracy: 0.5884 - val_loss: 1.2271 - learning_rate: 1.0000e-04\n",
            "Epoch 5/40\n",
            "\u001b[1m43/43\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 231ms/step - accuracy: 0.6466 - loss: 1.2282"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m43/43\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 275ms/step - accuracy: 0.6467 - loss: 1.2280 - val_accuracy: 0.6224 - val_loss: 1.2040 - learning_rate: 1.0000e-04\n",
            "Epoch 6/40\n",
            "\u001b[1m43/43\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 309ms/step - accuracy: 0.7068 - loss: 1.1718 - val_accuracy: 0.6054 - val_loss: 1.1867 - learning_rate: 1.0000e-04\n",
            "Epoch 7/40\n",
            "\u001b[1m43/43\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 232ms/step - accuracy: 0.7010 - loss: 1.1197"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m43/43\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 271ms/step - accuracy: 0.7012 - loss: 1.1199 - val_accuracy: 0.7619 - val_loss: 1.1459 - learning_rate: 1.0000e-04\n",
            "Epoch 8/40\n",
            "\u001b[1m43/43\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 237ms/step - accuracy: 0.7078 - loss: 1.1592"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m43/43\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 277ms/step - accuracy: 0.7082 - loss: 1.1584 - val_accuracy: 0.7755 - val_loss: 1.1108 - learning_rate: 1.0000e-04\n",
            "Epoch 9/40\n",
            "\u001b[1m43/43\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 315ms/step - accuracy: 0.7259 - loss: 1.1210 - val_accuracy: 0.7721 - val_loss: 1.0793 - learning_rate: 1.0000e-04\n",
            "Epoch 10/40\n",
            "\u001b[1m43/43\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 254ms/step - accuracy: 0.6831 - loss: 1.1535 - val_accuracy: 0.5646 - val_loss: 1.3078 - learning_rate: 1.0000e-04\n",
            "Epoch 11/40\n",
            "\u001b[1m43/43\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 253ms/step - accuracy: 0.7224 - loss: 1.0851 - val_accuracy: 0.6769 - val_loss: 1.1119 - learning_rate: 1.0000e-04\n",
            "Epoch 12/40\n",
            "\u001b[1m43/43\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 312ms/step - accuracy: 0.7256 - loss: 1.1156 - val_accuracy: 0.5986 - val_loss: 1.2499 - learning_rate: 1.0000e-04\n",
            "Epoch 13/40\n",
            "\u001b[1m43/43\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 243ms/step - accuracy: 0.7210 - loss: 1.1196 - val_accuracy: 0.6054 - val_loss: 1.2354 - learning_rate: 1.0000e-04\n",
            "Epoch 14/40\n",
            "\u001b[1m43/43\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 240ms/step - accuracy: 0.7171 - loss: 1.0817 - val_accuracy: 0.7653 - val_loss: 1.0025 - learning_rate: 1.0000e-04\n",
            "Epoch 15/40\n",
            "\u001b[1m43/43\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 316ms/step - accuracy: 0.7453 - loss: 1.0663 - val_accuracy: 0.6224 - val_loss: 1.2861 - learning_rate: 1.0000e-04\n",
            "Epoch 16/40\n",
            "\u001b[1m43/43\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 249ms/step - accuracy: 0.7331 - loss: 1.0692 - val_accuracy: 0.5102 - val_loss: 2.0115 - learning_rate: 1.0000e-04\n",
            "Epoch 17/40\n",
            "\u001b[1m43/43\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 249ms/step - accuracy: 0.7617 - loss: 1.0282 - val_accuracy: 0.6020 - val_loss: 1.4013 - learning_rate: 1.0000e-04\n",
            "Epoch 18/40\n",
            "\u001b[1m43/43\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 328ms/step - accuracy: 0.7159 - loss: 1.0948 - val_accuracy: 0.6939 - val_loss: 1.1789 - learning_rate: 1.0000e-04\n",
            "Epoch 19/40\n",
            "\u001b[1m43/43\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 246ms/step - accuracy: 0.7391 - loss: 1.0474 - val_accuracy: 0.6020 - val_loss: 1.5080 - learning_rate: 1.0000e-04\n",
            "Epoch 20/40\n",
            "\u001b[1m43/43\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 262ms/step - accuracy: 0.7509 - loss: 1.0435 - val_accuracy: 0.6259 - val_loss: 1.3612 - learning_rate: 1.0000e-04\n",
            "Epoch 21/40\n",
            "\u001b[1m43/43\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 316ms/step - accuracy: 0.7417 - loss: 1.0347 - val_accuracy: 0.6667 - val_loss: 1.1717 - learning_rate: 3.0000e-05\n",
            "Epoch 22/40\n",
            "\u001b[1m43/43\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 244ms/step - accuracy: 0.7683 - loss: 1.0310 - val_accuracy: 0.7075 - val_loss: 1.0934 - learning_rate: 3.0000e-05\n",
            "Epoch 23/40\n",
            "\u001b[1m43/43\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 253ms/step - accuracy: 0.7607 - loss: 1.0206 - val_accuracy: 0.6259 - val_loss: 1.3206 - learning_rate: 3.0000e-05\n",
            "âœ… Potato concluÃ­do! Melhor accuracy: 0.7755\n",
            "\n",
            "ðŸš€ Treinando Pepper...\n",
            "Epoch 1/40\n",
            "\u001b[1m35/61\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”â”â”â”â”â”â”â”\u001b[0m \u001b[1m6s\u001b[0m 233ms/step - accuracy: 0.4866 - loss: 1.5510"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-07-10 02:47:43.474361: I external/local_xla/xla/service/gpu/autotuning/conv_algorithm_picker.cc:549] Omitted potentially buggy algorithm eng14{k25=0} for conv %cudnn-conv-bias-activation.165 = (f32[17,64,56,56]{3,2,1,0}, u8[0]{0}) custom-call(f32[17,64,56,56]{3,2,1,0} %bitcast.14083, f32[64,64,3,3]{3,2,1,0} %bitcast.14090, f32[64]{0} %bitcast.14092), window={size=3x3 pad=1_1x1_1}, dim_labels=bf01_oi01->bf01, custom_call_target=\"__cudnn$convBiasActivationForward\", metadata={op_type=\"Conv2D\" op_name=\"functional_2_1/conv2_block1_2_conv_1/convolution\" source_file=\"/home/gustavo/.local/lib/python3.10/site-packages/tensorflow/python/framework/ops.py\" source_line=1200}, backend_config={\"operation_queue_id\":\"0\",\"wait_on_operation_queues\":[],\"cudnn_conv_backend_config\":{\"conv_result_scale\":1,\"activation_mode\":\"kNone\",\"side_input_scale\":0,\"leakyrelu_alpha\":0},\"force_earliest_schedule\":false}\n",
            "2025-07-10 02:47:43.854767: I external/local_xla/xla/service/gpu/autotuning/conv_algorithm_picker.cc:549] Omitted potentially buggy algorithm eng14{k25=0} for conv %cudnn-conv-bias-activation.175 = (f32[17,128,28,28]{3,2,1,0}, u8[0]{0}) custom-call(f32[17,128,28,28]{3,2,1,0} %bitcast.14488, f32[128,128,3,3]{3,2,1,0} %bitcast.14495, f32[128]{0} %bitcast.14497), window={size=3x3 pad=1_1x1_1}, dim_labels=bf01_oi01->bf01, custom_call_target=\"__cudnn$convBiasActivationForward\", metadata={op_type=\"Conv2D\" op_name=\"functional_2_1/conv3_block1_2_conv_1/convolution\" source_file=\"/home/gustavo/.local/lib/python3.10/site-packages/tensorflow/python/framework/ops.py\" source_line=1200}, backend_config={\"operation_queue_id\":\"0\",\"wait_on_operation_queues\":[],\"cudnn_conv_backend_config\":{\"conv_result_scale\":1,\"activation_mode\":\"kNone\",\"side_input_scale\":0,\"leakyrelu_alpha\":0},\"force_earliest_schedule\":false}\n",
            "2025-07-10 02:47:44.252845: I external/local_xla/xla/service/gpu/autotuning/conv_algorithm_picker.cc:549] Omitted potentially buggy algorithm eng14{k25=0} for conv %cudnn-conv-bias-activation.188 = (f32[17,256,14,14]{3,2,1,0}, u8[0]{0}) custom-call(f32[17,256,14,14]{3,2,1,0} %bitcast.15016, f32[256,256,3,3]{3,2,1,0} %bitcast.15023, f32[256]{0} %bitcast.15025), window={size=3x3 pad=1_1x1_1}, dim_labels=bf01_oi01->bf01, custom_call_target=\"__cudnn$convBiasActivationForward\", metadata={op_type=\"Conv2D\" op_name=\"functional_2_1/conv4_block1_2_conv_1/convolution\" source_file=\"/home/gustavo/.local/lib/python3.10/site-packages/tensorflow/python/framework/ops.py\" source_line=1200}, backend_config={\"operation_queue_id\":\"0\",\"wait_on_operation_queues\":[],\"cudnn_conv_backend_config\":{\"conv_result_scale\":1,\"activation_mode\":\"kNone\",\"side_input_scale\":0,\"leakyrelu_alpha\":0},\"force_earliest_schedule\":false}\n",
            "2025-07-10 02:47:44.660120: I external/local_xla/xla/service/gpu/autotuning/conv_algorithm_picker.cc:549] Omitted potentially buggy algorithm eng14{k25=0} for conv %cudnn-conv-bias-activation.207 = (f32[17,512,7,7]{3,2,1,0}, u8[0]{0}) custom-call(f32[17,512,7,7]{3,2,1,0} %bitcast.15790, f32[512,512,3,3]{3,2,1,0} %bitcast.15797, f32[512]{0} %bitcast.15799), window={size=3x3 pad=1_1x1_1}, dim_labels=bf01_oi01->bf01, custom_call_target=\"__cudnn$convBiasActivationForward\", metadata={op_type=\"Conv2D\" op_name=\"functional_2_1/conv5_block1_2_conv_1/convolution\" source_file=\"/home/gustavo/.local/lib/python3.10/site-packages/tensorflow/python/framework/ops.py\" source_line=1200}, backend_config={\"operation_queue_id\":\"0\",\"wait_on_operation_queues\":[],\"cudnn_conv_backend_config\":{\"conv_result_scale\":1,\"activation_mode\":\"kNone\",\"side_input_scale\":0,\"leakyrelu_alpha\":0},\"force_earliest_schedule\":false}\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m61/61\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 342ms/step - accuracy: 0.5222 - loss: 1.4957"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-07-10 02:48:01.143691: I external/local_xla/xla/service/gpu/autotuning/conv_algorithm_picker.cc:549] Omitted potentially buggy algorithm eng14{k25=0} for conv %cudnn-conv-bias-activation.162 = (f32[30,64,56,56]{3,2,1,0}, u8[0]{0}) custom-call(f32[30,64,56,56]{3,2,1,0} %bitcast.4834, f32[64,64,3,3]{3,2,1,0} %bitcast.4841, f32[64]{0} %bitcast.4843), window={size=3x3 pad=1_1x1_1}, dim_labels=bf01_oi01->bf01, custom_call_target=\"__cudnn$convBiasActivationForward\", metadata={op_type=\"Conv2D\" op_name=\"functional_2_1/conv2_block1_2_conv_1/convolution\" source_file=\"/home/gustavo/.local/lib/python3.10/site-packages/tensorflow/python/framework/ops.py\" source_line=1200}, backend_config={\"operation_queue_id\":\"0\",\"wait_on_operation_queues\":[],\"cudnn_conv_backend_config\":{\"conv_result_scale\":1,\"activation_mode\":\"kNone\",\"side_input_scale\":0,\"leakyrelu_alpha\":0},\"force_earliest_schedule\":false}\n",
            "2025-07-10 02:48:01.581835: I external/local_xla/xla/service/gpu/autotuning/conv_algorithm_picker.cc:549] Omitted potentially buggy algorithm eng14{k25=0} for conv %cudnn-conv-bias-activation.172 = (f32[30,128,28,28]{3,2,1,0}, u8[0]{0}) custom-call(f32[30,128,28,28]{3,2,1,0} %bitcast.5239, f32[128,128,3,3]{3,2,1,0} %bitcast.5246, f32[128]{0} %bitcast.5248), window={size=3x3 pad=1_1x1_1}, dim_labels=bf01_oi01->bf01, custom_call_target=\"__cudnn$convBiasActivationForward\", metadata={op_type=\"Conv2D\" op_name=\"functional_2_1/conv3_block1_2_conv_1/convolution\" source_file=\"/home/gustavo/.local/lib/python3.10/site-packages/tensorflow/python/framework/ops.py\" source_line=1200}, backend_config={\"operation_queue_id\":\"0\",\"wait_on_operation_queues\":[],\"cudnn_conv_backend_config\":{\"conv_result_scale\":1,\"activation_mode\":\"kNone\",\"side_input_scale\":0,\"leakyrelu_alpha\":0},\"force_earliest_schedule\":false}\n",
            "2025-07-10 02:48:02.049560: I external/local_xla/xla/service/gpu/autotuning/conv_algorithm_picker.cc:549] Omitted potentially buggy algorithm eng14{k25=0} for conv %cudnn-conv-bias-activation.185 = (f32[30,256,14,14]{3,2,1,0}, u8[0]{0}) custom-call(f32[30,256,14,14]{3,2,1,0} %bitcast.5767, f32[256,256,3,3]{3,2,1,0} %bitcast.5774, f32[256]{0} %bitcast.5776), window={size=3x3 pad=1_1x1_1}, dim_labels=bf01_oi01->bf01, custom_call_target=\"__cudnn$convBiasActivationForward\", metadata={op_type=\"Conv2D\" op_name=\"functional_2_1/conv4_block1_2_conv_1/convolution\" source_file=\"/home/gustavo/.local/lib/python3.10/site-packages/tensorflow/python/framework/ops.py\" source_line=1200}, backend_config={\"operation_queue_id\":\"0\",\"wait_on_operation_queues\":[],\"cudnn_conv_backend_config\":{\"conv_result_scale\":1,\"activation_mode\":\"kNone\",\"side_input_scale\":0,\"leakyrelu_alpha\":0},\"force_earliest_schedule\":false}\n",
            "2025-07-10 02:48:02.530948: I external/local_xla/xla/service/gpu/autotuning/conv_algorithm_picker.cc:549] Omitted potentially buggy algorithm eng14{k25=0} for conv %cudnn-conv-bias-activation.204 = (f32[30,512,7,7]{3,2,1,0}, u8[0]{0}) custom-call(f32[30,512,7,7]{3,2,1,0} %bitcast.6541, f32[512,512,3,3]{3,2,1,0} %bitcast.6548, f32[512]{0} %bitcast.6550), window={size=3x3 pad=1_1x1_1}, dim_labels=bf01_oi01->bf01, custom_call_target=\"__cudnn$convBiasActivationForward\", metadata={op_type=\"Conv2D\" op_name=\"functional_2_1/conv5_block1_2_conv_1/convolution\" source_file=\"/home/gustavo/.local/lib/python3.10/site-packages/tensorflow/python/framework/ops.py\" source_line=1200}, backend_config={\"operation_queue_id\":\"0\",\"wait_on_operation_queues\":[],\"cudnn_conv_backend_config\":{\"conv_result_scale\":1,\"activation_mode\":\"kNone\",\"side_input_scale\":0,\"leakyrelu_alpha\":0},\"force_earliest_schedule\":false}\n",
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m61/61\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 527ms/step - accuracy: 0.5232 - loss: 1.4942 - val_accuracy: 0.2512 - val_loss: 1.3355 - learning_rate: 1.0000e-04\n",
            "Epoch 2/40\n",
            "\u001b[1m61/61\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 224ms/step - accuracy: 0.6583 - loss: 1.2827"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m61/61\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 250ms/step - accuracy: 0.6585 - loss: 1.2825 - val_accuracy: 0.6039 - val_loss: 1.2285 - learning_rate: 1.0000e-04\n",
            "Epoch 3/40\n",
            "\u001b[1m61/61\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 272ms/step - accuracy: 0.6930 - loss: 1.2194"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m61/61\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 305ms/step - accuracy: 0.6929 - loss: 1.2195 - val_accuracy: 0.8261 - val_loss: 1.1409 - learning_rate: 1.0000e-04\n",
            "Epoch 4/40\n",
            "\u001b[1m61/61\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 217ms/step - accuracy: 0.7202 - loss: 1.1898"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m61/61\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 248ms/step - accuracy: 0.7203 - loss: 1.1897 - val_accuracy: 0.8333 - val_loss: 1.0965 - learning_rate: 1.0000e-04\n",
            "Epoch 5/40\n",
            "\u001b[1m61/61\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 268ms/step - accuracy: 0.7267 - loss: 1.1562"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m61/61\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 295ms/step - accuracy: 0.7266 - loss: 1.1560 - val_accuracy: 0.8671 - val_loss: 0.9996 - learning_rate: 1.0000e-04\n",
            "Epoch 6/40\n",
            "\u001b[1m61/61\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 232ms/step - accuracy: 0.7588 - loss: 1.0848 - val_accuracy: 0.8382 - val_loss: 0.9398 - learning_rate: 1.0000e-04\n",
            "Epoch 7/40\n",
            "\u001b[1m61/61\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 234ms/step - accuracy: 0.7556 - loss: 1.0639 - val_accuracy: 0.8647 - val_loss: 0.9911 - learning_rate: 1.0000e-04\n",
            "Epoch 8/40\n",
            "\u001b[1m61/61\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 291ms/step - accuracy: 0.7753 - loss: 1.0062 - val_accuracy: 0.8333 - val_loss: 0.9347 - learning_rate: 1.0000e-04\n",
            "Epoch 9/40\n",
            "\u001b[1m61/61\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 218ms/step - accuracy: 0.7797 - loss: 1.0324"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m61/61\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 244ms/step - accuracy: 0.7795 - loss: 1.0327 - val_accuracy: 0.8696 - val_loss: 0.8973 - learning_rate: 1.0000e-04\n",
            "Epoch 10/40\n",
            "\u001b[1m61/61\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 285ms/step - accuracy: 0.7848 - loss: 1.0092 - val_accuracy: 0.8623 - val_loss: 0.8443 - learning_rate: 1.0000e-04\n",
            "Epoch 11/40\n",
            "\u001b[1m61/61\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 239ms/step - accuracy: 0.8013 - loss: 0.9939 - val_accuracy: 0.8502 - val_loss: 0.8376 - learning_rate: 1.0000e-04\n",
            "Epoch 12/40\n",
            "\u001b[1m61/61\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 281ms/step - accuracy: 0.8059 - loss: 0.9938 - val_accuracy: 0.5918 - val_loss: 1.3258 - learning_rate: 1.0000e-04\n",
            "Epoch 13/40\n",
            "\u001b[1m61/61\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 234ms/step - accuracy: 0.7907 - loss: 1.0046 - val_accuracy: 0.4638 - val_loss: 1.5335 - learning_rate: 1.0000e-04\n",
            "Epoch 14/40\n",
            "\u001b[1m61/61\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 277ms/step - accuracy: 0.7883 - loss: 0.9931 - val_accuracy: 0.7488 - val_loss: 1.0647 - learning_rate: 1.0000e-04\n",
            "Epoch 15/40\n",
            "\u001b[1m61/61\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 240ms/step - accuracy: 0.7809 - loss: 1.0403 - val_accuracy: 0.6594 - val_loss: 1.3362 - learning_rate: 1.0000e-04\n",
            "Epoch 16/40\n",
            "\u001b[1m61/61\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 287ms/step - accuracy: 0.7977 - loss: 0.9647 - val_accuracy: 0.5459 - val_loss: 1.4462 - learning_rate: 1.0000e-04\n",
            "Epoch 17/40\n",
            "\u001b[1m61/61\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 237ms/step - accuracy: 0.8018 - loss: 0.9603 - val_accuracy: 0.8237 - val_loss: 0.9921 - learning_rate: 1.0000e-04\n",
            "Epoch 18/40\n",
            "\u001b[1m61/61\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 219ms/step - accuracy: 0.8121 - loss: 0.9553"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m61/61\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 247ms/step - accuracy: 0.8121 - loss: 0.9551 - val_accuracy: 0.8913 - val_loss: 0.7792 - learning_rate: 3.0000e-05\n",
            "Epoch 19/40\n",
            "\u001b[1m61/61\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 269ms/step - accuracy: 0.8227 - loss: 0.9135"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m61/61\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 298ms/step - accuracy: 0.8226 - loss: 0.9137 - val_accuracy: 0.9034 - val_loss: 0.7723 - learning_rate: 3.0000e-05\n",
            "Epoch 20/40\n",
            "\u001b[1m61/61\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 235ms/step - accuracy: 0.8321 - loss: 0.8966 - val_accuracy: 0.9034 - val_loss: 0.7630 - learning_rate: 3.0000e-05\n",
            "Epoch 21/40\n",
            "\u001b[1m61/61\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 285ms/step - accuracy: 0.8272 - loss: 0.9203 - val_accuracy: 0.8937 - val_loss: 0.7927 - learning_rate: 3.0000e-05\n",
            "Epoch 22/40\n",
            "\u001b[1m61/61\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 241ms/step - accuracy: 0.8230 - loss: 0.8737 - val_accuracy: 0.8889 - val_loss: 0.8321 - learning_rate: 3.0000e-05\n",
            "Epoch 23/40\n",
            "\u001b[1m61/61\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 284ms/step - accuracy: 0.8260 - loss: 0.9066 - val_accuracy: 0.8647 - val_loss: 0.8091 - learning_rate: 3.0000e-05\n",
            "Epoch 24/40\n",
            "\u001b[1m61/61\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 226ms/step - accuracy: 0.8534 - loss: 0.8456 - val_accuracy: 0.8961 - val_loss: 0.7655 - learning_rate: 3.0000e-05\n",
            "Epoch 25/40\n",
            "\u001b[1m61/61\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 286ms/step - accuracy: 0.8333 - loss: 0.8939 - val_accuracy: 0.7995 - val_loss: 0.9284 - learning_rate: 3.0000e-05\n",
            "Epoch 26/40\n",
            "\u001b[1m61/61\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 234ms/step - accuracy: 0.8290 - loss: 0.8774 - val_accuracy: 0.9034 - val_loss: 0.7478 - learning_rate: 3.0000e-05\n",
            "Epoch 27/40\n",
            "\u001b[1m61/61\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 230ms/step - accuracy: 0.8241 - loss: 0.8976 - val_accuracy: 0.8986 - val_loss: 0.7087 - learning_rate: 3.0000e-05\n",
            "Epoch 28/40\n",
            "\u001b[1m61/61\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 232ms/step - accuracy: 0.8501 - loss: 0.8592 - val_accuracy: 0.8696 - val_loss: 0.8375 - learning_rate: 3.0000e-05\n",
            "Epoch 29/40\n",
            "\u001b[1m61/61\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 218ms/step - accuracy: 0.8274 - loss: 0.9250"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m61/61\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 245ms/step - accuracy: 0.8275 - loss: 0.9244 - val_accuracy: 0.9058 - val_loss: 0.7155 - learning_rate: 3.0000e-05\n",
            "Epoch 30/40\n",
            "\u001b[1m61/61\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 270ms/step - accuracy: 0.8347 - loss: 0.9030"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m61/61\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 296ms/step - accuracy: 0.8347 - loss: 0.9029 - val_accuracy: 0.9203 - val_loss: 0.7094 - learning_rate: 3.0000e-05\n",
            "Epoch 31/40\n",
            "\u001b[1m61/61\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 235ms/step - accuracy: 0.8413 - loss: 0.8632 - val_accuracy: 0.5918 - val_loss: 1.2267 - learning_rate: 3.0000e-05\n",
            "Epoch 32/40\n",
            "\u001b[1m61/61\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 288ms/step - accuracy: 0.8447 - loss: 0.8411 - val_accuracy: 0.7923 - val_loss: 0.9356 - learning_rate: 3.0000e-05\n",
            "Epoch 33/40\n",
            "\u001b[1m61/61\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 234ms/step - accuracy: 0.8434 - loss: 0.8771 - val_accuracy: 0.9082 - val_loss: 0.7231 - learning_rate: 3.0000e-05\n",
            "Epoch 34/40\n",
            "\u001b[1m61/61\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 296ms/step - accuracy: 0.8580 - loss: 0.8586 - val_accuracy: 0.9179 - val_loss: 0.6961 - learning_rate: 9.0000e-06\n",
            "Epoch 35/40\n",
            "\u001b[1m61/61\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 239ms/step - accuracy: 0.8428 - loss: 0.8623 - val_accuracy: 0.9179 - val_loss: 0.7009 - learning_rate: 9.0000e-06\n",
            "Epoch 36/40\n",
            "\u001b[1m61/61\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 279ms/step - accuracy: 0.8651 - loss: 0.8406 - val_accuracy: 0.9179 - val_loss: 0.6940 - learning_rate: 9.0000e-06\n",
            "Epoch 37/40\n",
            "\u001b[1m61/61\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 247ms/step - accuracy: 0.8643 - loss: 0.8325 - val_accuracy: 0.9203 - val_loss: 0.6902 - learning_rate: 9.0000e-06\n",
            "Epoch 38/40\n",
            "\u001b[1m61/61\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 296ms/step - accuracy: 0.8472 - loss: 0.8281 - val_accuracy: 0.9179 - val_loss: 0.6922 - learning_rate: 9.0000e-06\n",
            "Epoch 39/40\n",
            "\u001b[1m61/61\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 249ms/step - accuracy: 0.8793 - loss: 0.8132 - val_accuracy: 0.8961 - val_loss: 0.7343 - learning_rate: 9.0000e-06\n",
            "Epoch 40/40\n",
            "\u001b[1m61/61\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 238ms/step - accuracy: 0.8718 - loss: 0.8152 - val_accuracy: 0.8841 - val_loss: 0.7406 - learning_rate: 9.0000e-06\n",
            "âœ… Pepper concluÃ­do! Melhor accuracy: 0.9203\n",
            "\n",
            "ðŸŽ¯ TODOS OS MODELOS TREINADOS COM DADOS BALANCEADOS!\n"
          ]
        }
      ],
      "source": [
        "# 5. TREINAMENTO OTIMIZADO COM DADOS BALANCEADOS E CLASS WEIGHTS\n",
        "def treinar_modelo_binario(modelo, especie, train_gen, val_gen, class_weights):\n",
        "    \"\"\"Treina modelo de classificaÃ§Ã£o binÃ¡ria com class weights\"\"\"\n",
        "    print(f\"\\nðŸš€ Treinando {especie}...\")\n",
        "    \n",
        "    # CompilaÃ§Ã£o\n",
        "    modelo.compile(\n",
        "        optimizer=Adam(learning_rate=0.0001),\n",
        "        loss='binary_crossentropy',\n",
        "        metrics=['accuracy']\n",
        "    )\n",
        "    \n",
        "    # Callback\n",
        "    callbacks = [\n",
        "        EarlyStopping(\n",
        "            monitor='val_accuracy',\n",
        "            patience=15,\n",
        "            restore_best_weights=True,\n",
        "            min_delta=0.001\n",
        "        ),\n",
        "        ReduceLROnPlateau(\n",
        "            monitor='val_loss',\n",
        "            factor=0.3,\n",
        "            patience=6,\n",
        "            min_lr=1e-8\n",
        "        ),\n",
        "        ModelCheckpoint(\n",
        "            filepath=f'modelos_salvos/especialistas/modelo_binario_{especie.lower()}.h5',\n",
        "            monitor='val_accuracy',\n",
        "            save_best_only=True\n",
        "        )\n",
        "    ]\n",
        "    \n",
        "    # Treinamento\n",
        "    history = modelo.fit(\n",
        "        train_gen,\n",
        "        epochs=40,\n",
        "        validation_data=val_gen,\n",
        "        class_weight=class_weights,\n",
        "        callbacks=callbacks,\n",
        "        verbose=1\n",
        "    )\n",
        "    \n",
        "    final_accuracy = max(history.history['val_accuracy'])\n",
        "    print(f\"âœ… {especie} concluÃ­do! Melhor accuracy: {final_accuracy:.4f}\")\n",
        "    \n",
        "    return history\n",
        "\n",
        "# Treinar todos os modelos com dados balanceados\n",
        "os.makedirs('modelos_salvos', exist_ok=True)\n",
        "os.makedirs('modelos_salvos/especialistas', exist_ok=True)\n",
        "\n",
        "print(\"=== TREINAMENTO DOS MODELOS COM DADOS BALANCEADOS ===\")\n",
        "print(\"ðŸŽ¯ EXPECTATIVAS DE MELHORIA:\")\n",
        "print(\"   ðŸš¨ Potato: 0% â†’ 40-60% recall Healthy\")\n",
        "print(\"   âš ï¸ Tomato: 56% â†’ 70-80% recall Healthy\") \n",
        "print(\"   âœ… Pepper: 80% â†’ 85-90% recall Healthy\")\n",
        "print()\n",
        "\n",
        "# Treinar modelos com verificaÃ§Ã£o de dados balanceados\n",
        "histories = {}\n",
        "\n",
        "if cw_tomato is not None:\n",
        "    histories['tomato'] = treinar_modelo_binario(modelo_tomato, 'Tomato', train_gen_tomato, val_gen_tomato, cw_tomato)\n",
        "\n",
        "if cw_potato is not None:\n",
        "    histories['potato'] = treinar_modelo_binario(modelo_potato, 'Potato', train_gen_potato, val_gen_potato, cw_potato)\n",
        "\n",
        "if cw_pepper is not None:\n",
        "    histories['pepper'] = treinar_modelo_binario(modelo_pepper, 'Pepper', train_gen_pepper, val_gen_pepper, cw_pepper)\n",
        "\n",
        "print(\"\\nðŸŽ¯ TODOS OS MODELOS TREINADOS COM DADOS BALANCEADOS!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=== AVALIAÃ‡ÃƒO FINAL COM THRESHOLD INTELIGENTE ===\n",
            "ðŸ§  Sistema inteligente priorizando detecÃ§Ã£o de plantas doentes:\n",
            "   - Thresholds dinÃ¢micos baseados na confianÃ§a da prediÃ§Ã£o\n",
            "   - Tomato: Base 0.55 (ajuste 0.39-0.63)\n",
            "   - Potato: Base 0.45 (ajuste 0.32-0.52)\n",
            "   - Pepper: Base 0.50 (ajuste 0.35-0.58)\n",
            "\n",
            "ðŸ“Š Avaliando Tomato com threshold inteligente (foco em Unhealthy)...\n",
            "   ðŸ§  Aplicando threshold inteligente para Tomato...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-07-10 02:58:39.221484: I external/local_xla/xla/service/gpu/autotuning/conv_algorithm_picker.cc:549] Omitted potentially buggy algorithm eng14{k25=0} for conv %cudnn-conv-bias-activation.162 = (f32[12,64,56,56]{3,2,1,0}, u8[0]{0}) custom-call(f32[12,64,56,56]{3,2,1,0} %bitcast.4595, f32[64,64,3,3]{3,2,1,0} %bitcast.4602, f32[64]{0} %bitcast.4604), window={size=3x3 pad=1_1x1_1}, dim_labels=bf01_oi01->bf01, custom_call_target=\"__cudnn$convBiasActivationForward\", metadata={op_type=\"Conv2D\" op_name=\"functional_1/conv2_block1_2_conv_1/convolution\" source_file=\"/home/gustavo/.local/lib/python3.10/site-packages/tensorflow/python/framework/ops.py\" source_line=1200}, backend_config={\"operation_queue_id\":\"0\",\"wait_on_operation_queues\":[],\"cudnn_conv_backend_config\":{\"conv_result_scale\":1,\"activation_mode\":\"kNone\",\"side_input_scale\":0,\"leakyrelu_alpha\":0},\"force_earliest_schedule\":false}\n",
            "2025-07-10 02:58:39.565145: I external/local_xla/xla/service/gpu/autotuning/conv_algorithm_picker.cc:549] Omitted potentially buggy algorithm eng14{k25=0} for conv %cudnn-conv-bias-activation.172 = (f32[12,128,28,28]{3,2,1,0}, u8[0]{0}) custom-call(f32[12,128,28,28]{3,2,1,0} %bitcast.5000, f32[128,128,3,3]{3,2,1,0} %bitcast.5007, f32[128]{0} %bitcast.5009), window={size=3x3 pad=1_1x1_1}, dim_labels=bf01_oi01->bf01, custom_call_target=\"__cudnn$convBiasActivationForward\", metadata={op_type=\"Conv2D\" op_name=\"functional_1/conv3_block1_2_conv_1/convolution\" source_file=\"/home/gustavo/.local/lib/python3.10/site-packages/tensorflow/python/framework/ops.py\" source_line=1200}, backend_config={\"operation_queue_id\":\"0\",\"wait_on_operation_queues\":[],\"cudnn_conv_backend_config\":{\"conv_result_scale\":1,\"activation_mode\":\"kNone\",\"side_input_scale\":0,\"leakyrelu_alpha\":0},\"force_earliest_schedule\":false}\n",
            "2025-07-10 02:58:39.917272: I external/local_xla/xla/service/gpu/autotuning/conv_algorithm_picker.cc:549] Omitted potentially buggy algorithm eng14{k25=0} for conv %cudnn-conv-bias-activation.185 = (f32[12,256,14,14]{3,2,1,0}, u8[0]{0}) custom-call(f32[12,256,14,14]{3,2,1,0} %bitcast.5528, f32[256,256,3,3]{3,2,1,0} %bitcast.5535, f32[256]{0} %bitcast.5537), window={size=3x3 pad=1_1x1_1}, dim_labels=bf01_oi01->bf01, custom_call_target=\"__cudnn$convBiasActivationForward\", metadata={op_type=\"Conv2D\" op_name=\"functional_1/conv4_block1_2_conv_1/convolution\" source_file=\"/home/gustavo/.local/lib/python3.10/site-packages/tensorflow/python/framework/ops.py\" source_line=1200}, backend_config={\"operation_queue_id\":\"0\",\"wait_on_operation_queues\":[],\"cudnn_conv_backend_config\":{\"conv_result_scale\":1,\"activation_mode\":\"kNone\",\"side_input_scale\":0,\"leakyrelu_alpha\":0},\"force_earliest_schedule\":false}\n",
            "2025-07-10 02:58:40.272005: I external/local_xla/xla/service/gpu/autotuning/conv_algorithm_picker.cc:549] Omitted potentially buggy algorithm eng14{k25=0} for conv %cudnn-conv-bias-activation.204 = (f32[12,512,7,7]{3,2,1,0}, u8[0]{0}) custom-call(f32[12,512,7,7]{3,2,1,0} %bitcast.6302, f32[512,512,3,3]{3,2,1,0} %bitcast.6309, f32[512]{0} %bitcast.6311), window={size=3x3 pad=1_1x1_1}, dim_labels=bf01_oi01->bf01, custom_call_target=\"__cudnn$convBiasActivationForward\", metadata={op_type=\"Conv2D\" op_name=\"functional_1/conv5_block1_2_conv_1/convolution\" source_file=\"/home/gustavo/.local/lib/python3.10/site-packages/tensorflow/python/framework/ops.py\" source_line=1200}, backend_config={\"operation_queue_id\":\"0\",\"wait_on_operation_queues\":[],\"cudnn_conv_backend_config\":{\"conv_result_scale\":1,\"activation_mode\":\"kNone\",\"side_input_scale\":0,\"leakyrelu_alpha\":0},\"force_earliest_schedule\":false}\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "   ðŸŽ¯ Threshold inteligente aplicado:\n",
            "      Faixa de thresholds: 0.385 - 0.632\n",
            "      Recall Healthy: 0.954\n",
            "      Recall Unhealthy: 0.951 â­\n",
            "\n",
            "   ðŸ“Š THRESHOLD PADRÃƒO (0.5):\n",
            "      Accuracy: 0.9551\n",
            "      Recall Healthy: 0.9473\n",
            "      Recall Unhealthy: 0.9585\n",
            "\n",
            "   ðŸ§  THRESHOLD INTELIGENTE:\n",
            "      Accuracy: 0.9518\n",
            "      AUC-ROC: 0.9927\n",
            "      Recall Healthy: 0.9538\n",
            "      Recall Unhealthy: 0.9510 â­\n",
            "      Matriz: [[434,  21], [ 52, 1009]]\n",
            "\n",
            "   Classification Report (Threshold Inteligente):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "     Healthy       0.89      0.95      0.92       455\n",
            "   Unhealthy       0.98      0.95      0.97      1061\n",
            "\n",
            "    accuracy                           0.95      1516\n",
            "   macro avg       0.94      0.95      0.94      1516\n",
            "weighted avg       0.95      0.95      0.95      1516\n",
            "\n",
            "\n",
            "   ðŸŽ¯ MELHORIA NO RECALL UNHEALTHY: -0.008\n",
            "\n",
            "ðŸ“Š Avaliando Potato com threshold inteligente (foco em Unhealthy)...\n",
            "   ðŸ§  Aplicando threshold inteligente para Potato...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/gustavo/.local/lib/python3.10/site-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
            "  self._warn_if_super_not_called()\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "   ðŸŽ¯ Threshold inteligente aplicado:\n",
            "      Faixa de thresholds: 0.383 - 0.517\n",
            "      Recall Healthy: 0.762\n",
            "      Recall Unhealthy: 0.721 â­\n",
            "\n",
            "   ðŸ“Š THRESHOLD PADRÃƒO (0.5):\n",
            "      Accuracy: 0.7313\n",
            "      Recall Healthy: 0.9048\n",
            "      Recall Unhealthy: 0.5578\n",
            "\n",
            "   ðŸ§  THRESHOLD INTELIGENTE:\n",
            "      Accuracy: 0.7415\n",
            "      AUC-ROC: 0.8327\n",
            "      Recall Healthy: 0.7619\n",
            "      Recall Unhealthy: 0.7211 â­\n",
            "      Matriz: [[112,  35], [ 41, 106]]\n",
            "\n",
            "   Classification Report (Threshold Inteligente):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "     Healthy       0.73      0.76      0.75       147\n",
            "   Unhealthy       0.75      0.72      0.74       147\n",
            "\n",
            "    accuracy                           0.74       294\n",
            "   macro avg       0.74      0.74      0.74       294\n",
            "weighted avg       0.74      0.74      0.74       294\n",
            "\n",
            "\n",
            "   ðŸŽ¯ MELHORIA NO RECALL UNHEALTHY: +0.163\n",
            "\n",
            "ðŸ“Š Avaliando Pepper com threshold inteligente (foco em Unhealthy)...\n",
            "   ðŸ§  Aplicando threshold inteligente para Pepper...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/gustavo/.local/lib/python3.10/site-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
            "  self._warn_if_super_not_called()\n",
            "2025-07-10 02:58:57.436907: I external/local_xla/xla/service/gpu/autotuning/conv_algorithm_picker.cc:549] Omitted potentially buggy algorithm eng14{k25=0} for conv %cudnn-conv-bias-activation.162 = (f32[31,64,56,56]{3,2,1,0}, u8[0]{0}) custom-call(f32[31,64,56,56]{3,2,1,0} %bitcast.4595, f32[64,64,3,3]{3,2,1,0} %bitcast.4602, f32[64]{0} %bitcast.4604), window={size=3x3 pad=1_1x1_1}, dim_labels=bf01_oi01->bf01, custom_call_target=\"__cudnn$convBiasActivationForward\", metadata={op_type=\"Conv2D\" op_name=\"functional_2_1/conv2_block1_2_conv_1/convolution\" source_file=\"/home/gustavo/.local/lib/python3.10/site-packages/tensorflow/python/framework/ops.py\" source_line=1200}, backend_config={\"operation_queue_id\":\"0\",\"wait_on_operation_queues\":[],\"cudnn_conv_backend_config\":{\"conv_result_scale\":1,\"activation_mode\":\"kNone\",\"side_input_scale\":0,\"leakyrelu_alpha\":0},\"force_earliest_schedule\":false}\n",
            "2025-07-10 02:58:57.864272: I external/local_xla/xla/service/gpu/autotuning/conv_algorithm_picker.cc:549] Omitted potentially buggy algorithm eng14{k25=0} for conv %cudnn-conv-bias-activation.172 = (f32[31,128,28,28]{3,2,1,0}, u8[0]{0}) custom-call(f32[31,128,28,28]{3,2,1,0} %bitcast.5000, f32[128,128,3,3]{3,2,1,0} %bitcast.5007, f32[128]{0} %bitcast.5009), window={size=3x3 pad=1_1x1_1}, dim_labels=bf01_oi01->bf01, custom_call_target=\"__cudnn$convBiasActivationForward\", metadata={op_type=\"Conv2D\" op_name=\"functional_2_1/conv3_block1_2_conv_1/convolution\" source_file=\"/home/gustavo/.local/lib/python3.10/site-packages/tensorflow/python/framework/ops.py\" source_line=1200}, backend_config={\"operation_queue_id\":\"0\",\"wait_on_operation_queues\":[],\"cudnn_conv_backend_config\":{\"conv_result_scale\":1,\"activation_mode\":\"kNone\",\"side_input_scale\":0,\"leakyrelu_alpha\":0},\"force_earliest_schedule\":false}\n",
            "2025-07-10 02:58:58.371171: I external/local_xla/xla/service/gpu/autotuning/conv_algorithm_picker.cc:549] Omitted potentially buggy algorithm eng14{k25=0} for conv %cudnn-conv-bias-activation.185 = (f32[31,256,14,14]{3,2,1,0}, u8[0]{0}) custom-call(f32[31,256,14,14]{3,2,1,0} %bitcast.5528, f32[256,256,3,3]{3,2,1,0} %bitcast.5535, f32[256]{0} %bitcast.5537), window={size=3x3 pad=1_1x1_1}, dim_labels=bf01_oi01->bf01, custom_call_target=\"__cudnn$convBiasActivationForward\", metadata={op_type=\"Conv2D\" op_name=\"functional_2_1/conv4_block1_2_conv_1/convolution\" source_file=\"/home/gustavo/.local/lib/python3.10/site-packages/tensorflow/python/framework/ops.py\" source_line=1200}, backend_config={\"operation_queue_id\":\"0\",\"wait_on_operation_queues\":[],\"cudnn_conv_backend_config\":{\"conv_result_scale\":1,\"activation_mode\":\"kNone\",\"side_input_scale\":0,\"leakyrelu_alpha\":0},\"force_earliest_schedule\":false}\n",
            "2025-07-10 02:58:58.833287: I external/local_xla/xla/service/gpu/autotuning/conv_algorithm_picker.cc:549] Omitted potentially buggy algorithm eng14{k25=0} for conv %cudnn-conv-bias-activation.204 = (f32[31,512,7,7]{3,2,1,0}, u8[0]{0}) custom-call(f32[31,512,7,7]{3,2,1,0} %bitcast.6302, f32[512,512,3,3]{3,2,1,0} %bitcast.6309, f32[512]{0} %bitcast.6311), window={size=3x3 pad=1_1x1_1}, dim_labels=bf01_oi01->bf01, custom_call_target=\"__cudnn$convBiasActivationForward\", metadata={op_type=\"Conv2D\" op_name=\"functional_2_1/conv5_block1_2_conv_1/convolution\" source_file=\"/home/gustavo/.local/lib/python3.10/site-packages/tensorflow/python/framework/ops.py\" source_line=1200}, backend_config={\"operation_queue_id\":\"0\",\"wait_on_operation_queues\":[],\"cudnn_conv_backend_config\":{\"conv_result_scale\":1,\"activation_mode\":\"kNone\",\"side_input_scale\":0,\"leakyrelu_alpha\":0},\"force_earliest_schedule\":false}\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "   ðŸŽ¯ Threshold inteligente aplicado:\n",
            "      Faixa de thresholds: 0.350 - 0.575\n",
            "      Recall Healthy: 0.952\n",
            "      Recall Unhealthy: 0.762 â­\n",
            "\n",
            "   ðŸ“Š THRESHOLD PADRÃƒO (0.5):\n",
            "      Accuracy: 0.9036\n",
            "      Recall Healthy: 0.9516\n",
            "      Recall Unhealthy: 0.7619\n",
            "\n",
            "   ðŸ§  THRESHOLD INTELIGENTE:\n",
            "      Accuracy: 0.9036\n",
            "      AUC-ROC: 0.9582\n",
            "      Recall Healthy: 0.9516\n",
            "      Recall Unhealthy: 0.7619 â­\n",
            "      Matriz: [[295,  15], [ 25,  80]]\n",
            "\n",
            "   Classification Report (Threshold Inteligente):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "     Healthy       0.92      0.95      0.94       310\n",
            "   Unhealthy       0.84      0.76      0.80       105\n",
            "\n",
            "    accuracy                           0.90       415\n",
            "   macro avg       0.88      0.86      0.87       415\n",
            "weighted avg       0.90      0.90      0.90       415\n",
            "\n",
            "\n",
            "   ðŸŽ¯ MELHORIA NO RECALL UNHEALTHY: +0.000\n",
            "\n",
            "=== COMPARAÃ‡ÃƒO FINAL: RECALL UNHEALTHY (THRESHOLD INTELIGENTE) ===\n",
            "ðŸŽ¯ FOCO: Detectar plantas doentes (evitar falsos negativos)\n",
            "======================================================================\n",
            "   Tomato:\n",
            "      ðŸ¦  Recall UNHEALTHY: 95.9% â†’ 95.1% (-0.8%) (Meta: 80-90%) ðŸŸ¢ EXCELENTE\n",
            "      âœ… Recall HEALTHY: 94.7% â†’ 95.4% (monitoramento)\n",
            "      ðŸ§  Thresholds aplicados: 0.385 - 0.632\n",
            "   Potato:\n",
            "      ðŸ¦  Recall UNHEALTHY: 55.8% â†’ 72.1% (+16.3%) (Meta: 75-85%) ðŸ”´ INSUFICIENTE\n",
            "      âœ… Recall HEALTHY: 90.5% â†’ 76.2% (monitoramento)\n",
            "      ðŸ§  Thresholds aplicados: 0.383 - 0.517\n",
            "   Pepper:\n",
            "      ðŸ¦  Recall UNHEALTHY: 76.2% â†’ 76.2% (+0.0%) (Meta: 70-80%) ðŸŸ¡ BOM\n",
            "      âœ… Recall HEALTHY: 95.2% â†’ 95.2% (monitoramento)\n",
            "      ðŸ§  Thresholds aplicados: 0.350 - 0.575\n",
            "======================================================================\n",
            "\n",
            "ðŸŽ¯ RESUMO DAS MELHORIAS NO RECALL UNHEALTHY:\n",
            "   ðŸ“‰ Tomato: -0.8 pontos percentuais\n",
            "   ðŸ“ˆ Potato: +16.3 pontos percentuais\n",
            "   âž¡ï¸ Pepper: +0.0 pontos percentuais\n",
            "\n",
            "ðŸ† MELHORIA MÃ‰DIA: +5.2 pontos percentuais no recall Unhealthy\n"
          ]
        }
      ],
      "source": [
        "# 6. AVALIAÃ‡ÃƒO OTIMIZADA E COMPARAÃ‡ÃƒO DE RESULTADOS\n",
        "from sklearn.metrics import (\n",
        "    classification_report, \n",
        "    accuracy_score, \n",
        "    confusion_matrix, \n",
        "    roc_auc_score, \n",
        "    recall_score, \n",
        "    precision_score, \n",
        "    f1_score\n",
        "    )\n",
        "\n",
        "def threshold_inteligente(probabilidade, especie, confianca_base=0.5):\n",
        "    \"\"\"\n",
        "    Threshold inteligente baseado na confianÃ§a da prediÃ§Ã£o\n",
        "    Prioriza detecÃ§Ã£o de plantas doentes (recall Unhealthy)\n",
        "    \"\"\"\n",
        "    \n",
        "    # ConfiguraÃ§Ãµes base por espÃ©cie (mais baixos = mais sensÃ­vel a unhealthy)\n",
        "    thresholds_base = {\n",
        "        'tomato': 0.55,    # Moderadamente sensÃ­vel\n",
        "        'potato': 0.45,    # Mais sensÃ­vel (problema histÃ³rico)\n",
        "        'pepper': 0.50     # Equilibrado\n",
        "    }\n",
        "    \n",
        "    threshold_base = thresholds_base.get(especie.lower(), 0.5)\n",
        "    \n",
        "    # Ajuste dinÃ¢mico baseado na confianÃ§a\n",
        "    if probabilidade >= 0.8:\n",
        "        # Alta confianÃ§a - usar threshold mais baixo para capturar unhealthy\n",
        "        threshold_ajustado = threshold_base * 0.7\n",
        "    elif probabilidade >= 0.6:\n",
        "        # ConfianÃ§a mÃ©dia-alta - leve reduÃ§Ã£o\n",
        "        threshold_ajustado = threshold_base * 0.85\n",
        "    elif probabilidade >= 0.4:\n",
        "        # ConfianÃ§a mÃ©dia - threshold base\n",
        "        threshold_ajustado = threshold_base\n",
        "    else:\n",
        "        # Baixa confianÃ§a - ser mais conservador\n",
        "        threshold_ajustado = threshold_base * 1.15\n",
        "    \n",
        "    # Garantir limites vÃ¡lidos\n",
        "    threshold_ajustado = max(0.2, min(0.8, threshold_ajustado))\n",
        "    \n",
        "    return threshold_ajustado\n",
        "\n",
        "def otimizar_threshold_inteligente(modelo, test_gen, dataset_test, especie):\n",
        "    \"\"\"Aplica threshold inteligente priorizando detecÃ§Ã£o de plantas doentes\"\"\"\n",
        "    \n",
        "    print(f\"   ðŸ§  Aplicando threshold inteligente para {especie}...\")\n",
        "    \n",
        "    # Obter todas as prediÃ§Ãµes\n",
        "    test_gen.reset()\n",
        "    predictions_prob = modelo.predict(test_gen, verbose=0).flatten()\n",
        "    true_classes = [1 if label == 'unhealthy' else 0 for label in dataset_test['y']]\n",
        "    \n",
        "    # Aplicar threshold inteligente para cada prediÃ§Ã£o\n",
        "    predictions_class_inteligente = []\n",
        "    thresholds_usados = []\n",
        "    \n",
        "    for prob in predictions_prob:\n",
        "        threshold = threshold_inteligente(prob, especie)\n",
        "        prediction = 1 if prob > threshold else 0\n",
        "        predictions_class_inteligente.append(prediction)\n",
        "        thresholds_usados.append(threshold)\n",
        "    \n",
        "    predictions_class_inteligente = np.array(predictions_class_inteligente)\n",
        "    \n",
        "    # Calcular mÃ©tricas\n",
        "    cm_inteligente = confusion_matrix(true_classes, predictions_class_inteligente)\n",
        "    \n",
        "    if cm_inteligente.shape == (2, 2):\n",
        "        tn, fp, fn, tp = cm_inteligente.ravel()\n",
        "        healthy_recall = tn / (tn + fp) if (tn + fp) > 0 else 0\n",
        "        unhealthy_recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
        "        \n",
        "        print(f\"   ðŸŽ¯ Threshold inteligente aplicado:\")\n",
        "        print(f\"      Faixa de thresholds: {min(thresholds_usados):.3f} - {max(thresholds_usados):.3f}\")\n",
        "        print(f\"      Recall Healthy: {healthy_recall:.3f}\")\n",
        "        print(f\"      Recall Unhealthy: {unhealthy_recall:.3f} â­\")\n",
        "        \n",
        "        return {\n",
        "            'thresholds_usados': thresholds_usados,\n",
        "            'predictions': predictions_class_inteligente,\n",
        "            'recall_healthy': healthy_recall,\n",
        "            'recall_unhealthy': unhealthy_recall\n",
        "        }\n",
        "    \n",
        "    return None\n",
        "\n",
        "def avaliar_modelo_otimizado(modelo, especie, test_gen, dataset_test):\n",
        "    \"\"\"AvaliaÃ§Ã£o completa com threshold inteligente priorizando detecÃ§Ã£o de doenÃ§as\"\"\"\n",
        "    print(f\"\\nðŸ“Š Avaliando {especie} com threshold inteligente (foco em Unhealthy)...\")\n",
        "    \n",
        "    test_gen.reset()\n",
        "    \n",
        "    # Aplicar threshold inteligente\n",
        "    resultado_inteligente = otimizar_threshold_inteligente(modelo, test_gen, dataset_test, especie)\n",
        "    \n",
        "    # PrediÃ§Ãµes com threshold padrÃ£o para comparaÃ§Ã£o\n",
        "    test_gen.reset()\n",
        "    predictions_prob = modelo.predict(test_gen, verbose=0).flatten()\n",
        "    predictions_class_default = (predictions_prob > 0.5).astype(int).flatten()\n",
        "    \n",
        "    # Classes verdadeiras\n",
        "    true_classes = [1 if label == 'unhealthy' else 0 for label in dataset_test['y']]\n",
        "    \n",
        "    # MÃ©tricas com threshold padrÃ£o (0.5)\n",
        "    print(f\"\\n   ðŸ“Š THRESHOLD PADRÃƒO (0.5):\")\n",
        "    cm_default = confusion_matrix(true_classes, predictions_class_default)\n",
        "    accuracy_default = accuracy_score(true_classes, predictions_class_default)\n",
        "    \n",
        "    if cm_default.shape == (2, 2):\n",
        "        tn, fp, fn, tp = cm_default.ravel()\n",
        "        healthy_recall_default = tn / (tn + fp) if (tn + fp) > 0 else 0\n",
        "        unhealthy_recall_default = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
        "        print(f\"      Accuracy: {accuracy_default:.4f}\")\n",
        "        print(f\"      Recall Healthy: {healthy_recall_default:.4f}\")\n",
        "        print(f\"      Recall Unhealthy: {unhealthy_recall_default:.4f}\")\n",
        "    \n",
        "    # MÃ©tricas com threshold inteligente\n",
        "    if resultado_inteligente:\n",
        "        predictions_class_inteligente = resultado_inteligente['predictions']\n",
        "        cm_inteligente = confusion_matrix(true_classes, predictions_class_inteligente)\n",
        "        accuracy_inteligente = accuracy_score(true_classes, predictions_class_inteligente)\n",
        "        auc_score = roc_auc_score(true_classes, predictions_prob)\n",
        "        \n",
        "        print(f\"\\n   ðŸ§  THRESHOLD INTELIGENTE:\")\n",
        "        if cm_inteligente.shape == (2, 2):\n",
        "            tn, fp, fn, tp = cm_inteligente.ravel()\n",
        "            healthy_recall_inteligente = tn / (tn + fp) if (tn + fp) > 0 else 0\n",
        "            unhealthy_recall_inteligente = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
        "            \n",
        "            print(f\"      Accuracy: {accuracy_inteligente:.4f}\")\n",
        "            print(f\"      AUC-ROC: {auc_score:.4f}\")\n",
        "            print(f\"      Recall Healthy: {healthy_recall_inteligente:.4f}\")\n",
        "            print(f\"      Recall Unhealthy: {unhealthy_recall_inteligente:.4f} â­\")\n",
        "            print(f\"      Matriz: [[{tn:3d}, {fp:3d}], [{fn:3d}, {tp:3d}]]\")\n",
        "            \n",
        "            # RelatÃ³rio detalhado\n",
        "            print(\"\\n   Classification Report (Threshold Inteligente):\")\n",
        "            print(classification_report(true_classes, predictions_class_inteligente, target_names=['Healthy', 'Unhealthy'], zero_division=0))\n",
        "            \n",
        "            # AnÃ¡lise da melhoria\n",
        "            melhoria_unhealthy = unhealthy_recall_inteligente - unhealthy_recall_default\n",
        "            print(f\"\\n   ðŸŽ¯ MELHORIA NO RECALL UNHEALTHY: {melhoria_unhealthy:+.3f}\")\n",
        "            \n",
        "            return {\n",
        "                'threshold_inteligente': resultado_inteligente,\n",
        "                'accuracy_default': accuracy_default,\n",
        "                'accuracy_otimo': accuracy_inteligente,\n",
        "                'healthy_recall_default': healthy_recall_default,\n",
        "                'healthy_recall_otimo': healthy_recall_inteligente,\n",
        "                'unhealthy_recall_default': unhealthy_recall_default,\n",
        "                'unhealthy_recall_otimo': unhealthy_recall_inteligente,\n",
        "                'auc_roc': auc_score,\n",
        "                'confusion_matrix': cm_inteligente,\n",
        "                'melhoria_unhealthy': melhoria_unhealthy\n",
        "            }\n",
        "    \n",
        "    return None\n",
        "\n",
        "# Avaliar todos os modelos com threshold inteligente\n",
        "print(\"=== AVALIAÃ‡ÃƒO FINAL COM THRESHOLD INTELIGENTE ===\")\n",
        "print(\"ðŸ§  Sistema inteligente priorizando detecÃ§Ã£o de plantas doentes:\")\n",
        "print(\"   - Thresholds dinÃ¢micos baseados na confianÃ§a da prediÃ§Ã£o\")\n",
        "print(\"   - Tomato: Base 0.55 (ajuste 0.39-0.63)\")\n",
        "print(\"   - Potato: Base 0.45 (ajuste 0.32-0.52)\")  \n",
        "print(\"   - Pepper: Base 0.50 (ajuste 0.35-0.58)\")\n",
        "\n",
        "resultados_finais = {}\n",
        "\n",
        "# Usar datasets finais para teste (balanceados tÃªm mesmo test set que originais)\n",
        "dataset_tomato_test = dataset_tomato_final['test'] if dataset_tomato_final else None\n",
        "dataset_potato_test = dataset_potato_final['test'] if dataset_potato_final else None  \n",
        "dataset_pepper_test = dataset_pepper_final['test'] if dataset_pepper_final else None\n",
        "\n",
        "if dataset_tomato_test and 'tomato' in histories:\n",
        "    resultados_finais['tomato'] = avaliar_modelo_otimizado(modelo_tomato, 'Tomato', test_gen_tomato, dataset_tomato_test)\n",
        "\n",
        "if dataset_potato_test and 'potato' in histories:\n",
        "    resultados_finais['potato'] = avaliar_modelo_otimizado(modelo_potato, 'Potato', test_gen_potato, dataset_potato_test)\n",
        "\n",
        "if dataset_pepper_test and 'pepper' in histories:\n",
        "    resultados_finais['pepper'] = avaliar_modelo_otimizado(modelo_pepper, 'Pepper', test_gen_pepper, dataset_pepper_test)\n",
        "\n",
        "# ComparaÃ§Ã£o final focando na melhoria do recall Unhealthy\n",
        "print(f\"\\n=== COMPARAÃ‡ÃƒO FINAL: RECALL UNHEALTHY (THRESHOLD INTELIGENTE) ===\")\n",
        "print(\"ðŸŽ¯ FOCO: Detectar plantas doentes (evitar falsos negativos)\")\n",
        "print(\"=\" * 70)\n",
        "for especie, resultado in resultados_finais.items():\n",
        "    if resultado:\n",
        "        # MÃ©tricas Unhealthy (principal objetivo)\n",
        "        unhealthy_antes = resultado['unhealthy_recall_default'] * 100\n",
        "        unhealthy_depois = resultado['unhealthy_recall_otimo'] * 100\n",
        "        melhoria_unhealthy = resultado.get('melhoria_unhealthy', 0) * 100\n",
        "        \n",
        "        # MÃ©tricas Healthy (monitoramento)\n",
        "        healthy_antes = resultado['healthy_recall_default'] * 100\n",
        "        healthy_depois = resultado['healthy_recall_otimo'] * 100\n",
        "        \n",
        "        # Metas para recall Unhealthy (detectar plantas doentes)\n",
        "        if especie == 'potato':\n",
        "            meta_unhealthy = \"75-85%\"\n",
        "            status = \"ðŸŸ¢ EXCELENTE\" if unhealthy_depois >= 85 else \"ðŸŸ¡ BOM\" if unhealthy_depois >= 75 else \"ðŸ”´ INSUFICIENTE\"\n",
        "        elif especie == 'tomato':\n",
        "            meta_unhealthy = \"80-90%\"\n",
        "            status = \"ðŸŸ¢ EXCELENTE\" if unhealthy_depois >= 90 else \"ðŸŸ¡ BOM\" if unhealthy_depois >= 80 else \"ðŸ”´ INSUFICIENTE\"\n",
        "        else:  # pepper\n",
        "            meta_unhealthy = \"70-80%\"\n",
        "            status = \"ðŸŸ¢ EXCELENTE\" if unhealthy_depois >= 80 else \"ðŸŸ¡ BOM\" if unhealthy_depois >= 70 else \"ðŸ”´ INSUFICIENTE\"\n",
        "        \n",
        "        print(f\"   {especie.capitalize()}:\")\n",
        "        print(f\"      ðŸ¦  Recall UNHEALTHY: {unhealthy_antes:.1f}% â†’ {unhealthy_depois:.1f}% ({melhoria_unhealthy:+.1f}%) (Meta: {meta_unhealthy}) {status}\")\n",
        "        print(f\"      âœ… Recall HEALTHY: {healthy_antes:.1f}% â†’ {healthy_depois:.1f}% (monitoramento)\")\n",
        "        \n",
        "        # Mostrar faixa de thresholds usados\n",
        "        if 'threshold_inteligente' in resultado:\n",
        "            thresholds = resultado['threshold_inteligente']['thresholds_usados']\n",
        "            print(f\"      ðŸ§  Thresholds aplicados: {min(thresholds):.3f} - {max(thresholds):.3f}\")\n",
        "\n",
        "print(\"=\" * 70)\n",
        "\n",
        "# Resumo das melhorias\n",
        "print(f\"\\nðŸŽ¯ RESUMO DAS MELHORIAS NO RECALL UNHEALTHY:\")\n",
        "total_melhoria = 0\n",
        "especies_melhoradas = 0\n",
        "\n",
        "for especie, resultado in resultados_finais.items():\n",
        "    if resultado and 'melhoria_unhealthy' in resultado:\n",
        "        melhoria = resultado['melhoria_unhealthy'] * 100\n",
        "        total_melhoria += melhoria\n",
        "        especies_melhoradas += 1\n",
        "        \n",
        "        status_emoji = \"ðŸ“ˆ\" if melhoria > 0 else \"ðŸ“‰\" if melhoria < 0 else \"âž¡ï¸\"\n",
        "        print(f\"   {status_emoji} {especie.capitalize()}: {melhoria:+.1f} pontos percentuais\")\n",
        "\n",
        "if especies_melhoradas > 0:\n",
        "    melhoria_media = total_melhoria / especies_melhoradas\n",
        "    print(f\"\\nðŸ† MELHORIA MÃ‰DIA: {melhoria_media:+.1f} pontos percentuais no recall Unhealthy\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "=== SALVANDO MODELOS OTIMIZADOS COM BALANCEAMENTO ===\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "âœ… Modelo Tomato salvo com melhorias de balanceamento\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "âœ… Modelo Potato salvo com melhorias de balanceamento\n",
            "âœ… Modelo Pepper salvo com melhorias de balanceamento\n",
            "\n",
            "ðŸŽ¯ RESUMO DAS MELHORIAS IMPLEMENTADAS:\n",
            "======================================================================\n",
            "âœ… Data Augmentation Direcionada aplicada\n",
            "âœ… SMOTE para balanceamento numÃ©rico aplicado\n",
            "âœ… Sistema de Threshold Inteligente implementado\n",
            "âœ… Foco na detecÃ§Ã£o de plantas doentes (recall Unhealthy)\n",
            "âœ… Thresholds dinÃ¢micos baseados na confianÃ§a da prediÃ§Ã£o\n",
            "âœ… Modelos treinados com dados balanceados\n",
            "âœ… Arquivos salvos com sistema inteligente\n",
            "======================================================================\n",
            "\n",
            "ðŸŽ¯ ARQUIVOS SALVOS:\n",
            "   - Modelos: modelos_salvos/especialistas/\n",
            "   - Sistema: modelos_salvos/especialistas/sistema_threshold_inteligente.pkl\n",
            "   - PrÃ³ximo passo: Implementar na API e testar!\n",
            "\n",
            "ðŸ§  SISTEMA DE THRESHOLD INTELIGENTE:\n",
            "   - Tomato: Base 0.55 â†’ DinÃ¢mico 0.39-0.63\n",
            "   - Potato: Base 0.45 â†’ DinÃ¢mico 0.32-0.52\n",
            "   - Pepper: Base 0.50 â†’ DinÃ¢mico 0.35-0.58\n",
            "   - LÃ³gica: ConfianÃ§a alta = threshold baixo (mais sensÃ­vel a unhealthy)\n",
            "   - Objetivo: Maximizar detecÃ§Ã£o de plantas doentes\n"
          ]
        }
      ],
      "source": [
        "# 7. SALVAR MODELOS FINAIS COM MELHORIAS\n",
        "print(\"\\n=== SALVANDO MODELOS OTIMIZADOS COM BALANCEAMENTO ===\")\n",
        "\n",
        "# Salvar modelos treinados com dados balanceados\n",
        "import pickle\n",
        "\n",
        "# Salvar modelos\n",
        "if 'tomato' in histories:\n",
        "    modelo_tomato.save('modelos_salvos/especialistas/especialista_tomato_balanceado_final.h5')\n",
        "    print(\"âœ… Modelo Tomato salvo com melhorias de balanceamento\")\n",
        "\n",
        "if 'potato' in histories:\n",
        "    modelo_potato.save('modelos_salvos/especialistas/especialista_potato_balanceado_final.h5')\n",
        "    print(\"âœ… Modelo Potato salvo com melhorias de balanceamento\")\n",
        "\n",
        "if 'pepper' in histories:\n",
        "    modelo_pepper.save('modelos_salvos/especialistas/especialista_pepper_balanceado_final.h5')\n",
        "    print(\"âœ… Modelo Pepper salvo com melhorias de balanceamento\")\n",
        "\n",
        "# Salvar informaÃ§Ãµes do sistema de threshold inteligente\n",
        "sistema_threshold_inteligente = {}\n",
        "for especie, resultado in resultados_finais.items():\n",
        "    if resultado and 'threshold_inteligente' in resultado:\n",
        "        threshold_info = resultado['threshold_inteligente']\n",
        "        sistema_threshold_inteligente[especie] = {\n",
        "            'sistema': 'threshold_inteligente',\n",
        "            'thresholds_range': f\"{min(threshold_info['thresholds_usados']):.3f}-{max(threshold_info['thresholds_usados']):.3f}\",\n",
        "            'healthy_recall': resultado['healthy_recall_otimo'],\n",
        "            'unhealthy_recall': resultado['unhealthy_recall_otimo'],\n",
        "            'melhoria_unhealthy': resultado.get('melhoria_unhealthy', 0),\n",
        "            'accuracy': resultado['accuracy_otimo'],\n",
        "            'thresholds_detalhados': threshold_info['thresholds_usados']\n",
        "        }\n",
        "\n",
        "with open('modelos_salvos/especialistas/sistema_threshold_inteligente.pkl', 'wb') as f:\n",
        "    pickle.dump(sistema_threshold_inteligente, f)\n",
        "\n",
        "print(\"\\nðŸŽ¯ RESUMO DAS MELHORIAS IMPLEMENTADAS:\")\n",
        "print(\"=\" * 70)\n",
        "print(\"âœ… Data Augmentation Direcionada aplicada\")\n",
        "print(\"âœ… SMOTE para balanceamento numÃ©rico aplicado\")\n",
        "print(\"âœ… Sistema de Threshold Inteligente implementado\")\n",
        "print(\"âœ… Foco na detecÃ§Ã£o de plantas doentes (recall Unhealthy)\")\n",
        "print(\"âœ… Thresholds dinÃ¢micos baseados na confianÃ§a da prediÃ§Ã£o\")\n",
        "print(\"âœ… Modelos treinados com dados balanceados\")\n",
        "print(\"âœ… Arquivos salvos com sistema inteligente\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "print(f\"\\nðŸŽ¯ ARQUIVOS SALVOS:\")\n",
        "print(f\"   - Modelos: modelos_salvos/especialistas/\")\n",
        "print(f\"   - Sistema: modelos_salvos/especialistas/sistema_threshold_inteligente.pkl\")\n",
        "print(f\"   - PrÃ³ximo passo: Implementar na API e testar!\")\n",
        "\n",
        "print(f\"\\nðŸ§  SISTEMA DE THRESHOLD INTELIGENTE:\")\n",
        "print(f\"   - Tomato: Base 0.55 â†’ DinÃ¢mico 0.39-0.63\")\n",
        "print(f\"   - Potato: Base 0.45 â†’ DinÃ¢mico 0.32-0.52\") \n",
        "print(f\"   - Pepper: Base 0.50 â†’ DinÃ¢mico 0.35-0.58\")\n",
        "print(f\"   - LÃ³gica: ConfianÃ§a alta = threshold baixo (mais sensÃ­vel a unhealthy)\")\n",
        "print(f\"   - Objetivo: Maximizar detecÃ§Ã£o de plantas doentes\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
