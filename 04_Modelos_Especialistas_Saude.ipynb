{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Modelos Especialistas com Sistema de Threshold Inteligente\n",
        "\n",
        "**Objetivo**: Criar modelos especialistas otimizados que classificam em:\n",
        "- **HEALTHY**: Planta saud√°vel\n",
        "- **UNHEALTHY**: Planta doente (qualquer doen√ßa)\n",
        "\n",
        "**üéØ Melhorias Implementadas**:\n",
        "- üö® **CR√çTICO**: Potato (0% recall Healthy) ‚Üí Data Augmentation Agressiva + SMOTE\n",
        "- ‚ö†Ô∏è **IMPORTANTE**: Tomato (56% recall Healthy) ‚Üí Data Augmentation Moderada + SMOTE  \n",
        "- ‚úÖ **REFINAMENTO**: Pepper (80% recall Healthy) ‚Üí Data Augmentation Conservadora\n",
        "\n",
        "**üß† Sistema de Threshold Inteligente**:\n",
        "- **Foco**: Maximizar detec√ß√£o de plantas doentes (recall Unhealthy)\n",
        "- **L√≥gica**: Thresholds din√¢micos baseados na confian√ßa da predi√ß√£o\n",
        "- **Tomato**: Base 0.55 ‚Üí Din√¢mico 0.39-0.63 (sensibilidade adaptativa)\n",
        "- **Potato**: Base 0.45 ‚Üí Din√¢mico 0.32-0.52 (mais sens√≠vel a doen√ßas)\n",
        "- **Pepper**: Base 0.50 ‚Üí Din√¢mico 0.35-0.58 (equilibrado adaptativo)\n",
        "\n",
        "**Estrat√©gia**: Data Augmentation Direcionada + SMOTE + Threshold Inteligente para detectar plantas doentes com m√°xima efic√°cia\n",
        "\n",
        "## üî¨ Otimiza√ß√£o de Thresholds Cient√≠ficos\n",
        "\n",
        "**An√°lise Realizada**: Substitui√ß√£o do sistema de threshold din√¢mico por thresholds fixos otimizados cientificamente.\n",
        "\n",
        "**Metodologia**:\n",
        "- **Range de Teste**: 0.1 a 0.9 com step 0.05\n",
        "- **M√©tricas**: Acur√°cia, Precis√£o, Recall, F1-Score\n",
        "- **Crit√©rio**: Maximiza√ß√£o do F1-Score para cada esp√©cie\n",
        "- **Amostra**: 300 imagens representativas (20 por categoria)\n",
        "\n",
        "**Thresholds √ìtimos Encontrados**:\n",
        "- **Tomato**: 0.75 (F1=100% - Modelo sens√≠vel, threshold alto)\n",
        "- **Potato**: 0.65 (F1=95.2% - Equilibrado)\n",
        "- **Pepper**: 0.15 (F1=95.2% - Modelo conservador, threshold baixo)\n",
        "\n",
        "**Resultados**: Melhoria de 73.3% para 93.8% na acur√°cia de sa√∫de (+20.5 pontos)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-07-11 19:03:31.029999: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1752271411.168978    3780 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1752271411.204992    3780 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1752271411.522422    3780 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1752271411.522452    3780 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1752271411.522454    3780 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1752271411.522456    3780 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "2025-07-11 19:03:31.559648: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=== CARREGANDO DATASETS BIN√ÅRIOS CORRIGIDOS ===\n",
            "üìÇ Carregando dataset de tomato...\n",
            "   ü¶† Tomato_Bacterial_spot: 2127 ‚Üí UNHEALTHY\n",
            "   ü¶† Tomato_Early_blight: 1000 ‚Üí UNHEALTHY\n",
            "   ü¶† Tomato_Late_blight: 1909 ‚Üí UNHEALTHY\n",
            "   ü¶† Tomato_Leaf_Mold: 952 ‚Üí UNHEALTHY\n",
            "   ü¶† Tomato_Septoria_leaf_spot: 1771 ‚Üí UNHEALTHY\n",
            "   ü¶† Tomato_Spider_mites_Two_spotted_spider_mite: 1676 ‚Üí UNHEALTHY\n",
            "   ü¶† Tomato__Target_Spot: 1404 ‚Üí UNHEALTHY\n",
            "   ü¶† Tomato__Tomato_YellowLeaf__Curl_Virus: 3208 ‚Üí UNHEALTHY\n",
            "   ü¶† Tomato__Tomato_mosaic_virus: 373 ‚Üí UNHEALTHY\n",
            "   ‚úÖ Tomato_healthy: 1591 ‚Üí HEALTHY\n",
            "   üìä Total: 16011 | Healthy: 1591 (9.9%) | Unhealthy: 14420 (90.1%)\n",
            "\n",
            "üìÇ Carregando dataset de potato...\n",
            "   ü¶† Potato___Early_blight: 1000 ‚Üí UNHEALTHY\n",
            "   ü¶† Potato___Late_blight: 1000 ‚Üí UNHEALTHY\n",
            "   ‚úÖ Potato___healthy: 152 ‚Üí HEALTHY\n",
            "   üìä Total: 2152 | Healthy: 152 (7.1%) | Unhealthy: 2000 (92.9%)\n",
            "\n",
            "üìÇ Carregando dataset de pepper_bell...\n",
            "   ü¶† Pepper__bell___Bacterial_spot: 997 ‚Üí UNHEALTHY\n",
            "   ‚úÖ Pepper__bell___healthy: 1478 ‚Üí HEALTHY\n",
            "   üìä Total: 2475 | Healthy: 1478 (59.7%) | Unhealthy: 997 (40.3%)\n",
            "\n",
            "‚úÖ DATASETS BIN√ÅRIOS CARREGADOS: Tomato, Potato, Pepper\n"
          ]
        }
      ],
      "source": [
        "# 1. CARREGAMENTO DE DADOS\n",
        "from utils import *\n",
        "import numpy as np\n",
        "import os\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "import pandas as pd\n",
        "\n",
        "config = carregar_configuracoes()\n",
        "\n",
        "def carregar_dataset(especie):\n",
        "    \"\"\"Carrega dataset agrupando todas as doen√ßas\"\"\"\n",
        "    print(f\"üìÇ Carregando dataset de {especie}...\")\n",
        "    \n",
        "    # Construir dataset_info\n",
        "    dataset_info = {}\n",
        "    for esp, info in config['especialistas'].items():\n",
        "        for classe in info['classes']:\n",
        "            dataset_info[classe] = {}\n",
        "    \n",
        "    healthy_images = []\n",
        "    unhealthy_images = []\n",
        "    \n",
        "    # Processar cada classe da esp√©cie\n",
        "    for classe, info in dataset_info.items():\n",
        "        # Remover underscores \n",
        "        classe_normalizada = classe.lower().replace('_', '')\n",
        "        especie_normalizada = especie.lower().replace('_', '')\n",
        "        \n",
        "        if especie_normalizada in classe_normalizada:\n",
        "            dir_path = os.path.join(config.get('processed_data_path', config['base_path']), classe)\n",
        "            \n",
        "            if not os.path.exists(dir_path):\n",
        "                print(f\"   ‚ö†Ô∏è Diret√≥rio n√£o encontrado: {dir_path}\")\n",
        "                continue\n",
        "                \n",
        "            images_in_dir = []\n",
        "            for img_name in os.listdir(dir_path):\n",
        "                if img_name.lower().endswith(('.jpg', '.jpeg', '.png')):\n",
        "                    images_in_dir.append(os.path.join(dir_path, img_name))\n",
        "            \n",
        "            # AGRUPAMENTO BIN√ÅRIO\n",
        "            if 'healthy' in classe.lower():\n",
        "                healthy_images.extend(images_in_dir)\n",
        "                print(f\"   ‚úÖ {classe}: {len(images_in_dir)} ‚Üí HEALTHY\")\n",
        "            else:\n",
        "                unhealthy_images.extend(images_in_dir)\n",
        "                print(f\"   ü¶† {classe}: {len(images_in_dir)} ‚Üí UNHEALTHY\")\n",
        "    \n",
        "    # Combinar dados\n",
        "    all_images = healthy_images + unhealthy_images\n",
        "    all_labels = ['healthy'] * len(healthy_images) + ['unhealthy'] * len(unhealthy_images)\n",
        "    \n",
        "    # Prote√ß√£o contra divis√£o por zero\n",
        "    if len(all_images) == 0:\n",
        "        print(f\"   ‚ùå ERRO: Nenhuma imagem encontrada para {especie}!\")\n",
        "        print(f\"   üîç Verifique se as pastas existem e cont√™m imagens.\")\n",
        "        return None\n",
        "    \n",
        "    balance_ratio = len(healthy_images) / len(all_images) * 100\n",
        "    print(f\"   üìä Total: {len(all_images)} | Healthy: {len(healthy_images)} ({balance_ratio:.1f}%) | Unhealthy: {len(unhealthy_images)} ({100-balance_ratio:.1f}%)\")\n",
        "    \n",
        "    # Dividindo em treino, valida√ß√£o e teste para todos os datasets\n",
        "    X_temp, X_test, y_temp, y_test = train_test_split(\n",
        "        all_images, all_labels, test_size=0.15, stratify=all_labels, random_state=42\n",
        "    )\n",
        "    \n",
        "    # Dividindo em treino, valida√ß√£o e teste para cada dataset\n",
        "    X_train, X_val, y_train, y_val = train_test_split(\n",
        "        X_temp, y_temp, test_size=0.176, stratify=y_temp, random_state=42\n",
        "    )\n",
        "    \n",
        "    return {\n",
        "        'train': {'X': X_train, 'y': y_train},\n",
        "        'val': {'X': X_val, 'y': y_val},\n",
        "        'test': {'X': X_test, 'y': y_test},\n",
        "        'info': {'balance_ratio': balance_ratio, 'total': len(all_images)}\n",
        "    }\n",
        "\n",
        "# Carregar datasets bin√°rios reais\n",
        "print(\"=== CARREGANDO DATASETS BIN√ÅRIOS CORRIGIDOS ===\")\n",
        "dataset_tomato = carregar_dataset('tomato')\n",
        "print()\n",
        "dataset_potato = carregar_dataset('potato')\n",
        "print()\n",
        "dataset_pepper = carregar_dataset('pepper_bell')\n",
        "\n",
        "# Verificar se todos os datasets foram carregados com sucesso\n",
        "datasets_validos = []\n",
        "if dataset_tomato is not None:\n",
        "    datasets_validos.append('Tomato')\n",
        "if dataset_potato is not None:\n",
        "    datasets_validos.append('Potato')    \n",
        "if dataset_pepper is not None:\n",
        "    datasets_validos.append('Pepper')\n",
        "\n",
        "if len(datasets_validos) > 0:\n",
        "    print(f\"\\n‚úÖ DATASETS BIN√ÅRIOS CARREGADOS: {', '.join(datasets_validos)}\")\n",
        "else:\n",
        "    print(\"\\n‚ùå ERRO: Nenhum dataset foi carregado com sucesso!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=== IMPLEMENTANDO ESTRAT√âGIAS DE BALANCEAMENTO ===\n",
            "üîß Thresholds calibrados para balancear recall Healthy vs Unhealthy:\n",
            "   - Tomato: 0.70 | Potato: 0.60 | Pepper: 0.65\n",
            "‚úÖ Fun√ß√µes de balanceamento carregadas\n"
          ]
        }
      ],
      "source": [
        "# 2. ESTRAT√âGIAS DE BALANCEAMENTO OTIMIZADO\n",
        "print(\"=== IMPLEMENTANDO ESTRAT√âGIAS DE BALANCEAMENTO ===\")\n",
        "print(\"üîß Thresholds calibrados para balancear recall Healthy vs Unhealthy:\")\n",
        "print(\"   - Tomato: 0.70 | Potato: 0.60 | Pepper: 0.65\")\n",
        "\n",
        "def aplicar_data_augmentation_direcionada(dataset, especie):\n",
        "    \"\"\"Aplica Data Augmentation direcionada para classe Healthy\"\"\"\n",
        "    \n",
        "    print(f\"\\nüîÑ Aplicando Data Augmentation direcionada para {especie}...\")\n",
        "    \n",
        "    # Separar classes\n",
        "    healthy_images = [img for img, label in zip(dataset['train']['X'], dataset['train']['y']) if label == 'healthy']\n",
        "    unhealthy_images = [img for img, label in zip(dataset['train']['X'], dataset['train']['y']) if label == 'unhealthy']\n",
        "    \n",
        "    print(f\"   Original: Healthy={len(healthy_images)}, Unhealthy={len(unhealthy_images)}\")\n",
        "    \n",
        "    # Definir multiplicadores baseados na severidade do problema\n",
        "    if especie.lower() == 'potato':\n",
        "        multiplicador = 8  # CR√çTICO: 0% recall\n",
        "        print(f\"   üö® Estrat√©gia CR√çTICA: Multiplicador {multiplicador}x\")\n",
        "    elif especie.lower() == 'tomato':\n",
        "        multiplicador = 4  # IMPORTANTE: 56% recall\n",
        "        print(f\"   ‚ö†Ô∏è Estrat√©gia IMPORTANTE: Multiplicador {multiplicador}x\")\n",
        "    else:  # pepper\n",
        "        multiplicador = 2  # REFINAMENTO: 80% recall\n",
        "        print(f\"   ‚úÖ Estrat√©gia REFINAMENTO: Multiplicador {multiplicador}x\")\n",
        "    \n",
        "    # Aplicar augmenta√ß√£o replicando amostras healthy\n",
        "    augmented_healthy = []\n",
        "    augmented_labels = []\n",
        "    \n",
        "    for img_path in healthy_images:\n",
        "        for i in range(multiplicador):\n",
        "            augmented_healthy.append(img_path)\n",
        "            augmented_labels.append('healthy')\n",
        "    \n",
        "    # Combinar todos os dados\n",
        "    all_images = augmented_healthy + unhealthy_images\n",
        "    all_labels = augmented_labels + ['unhealthy'] * len(unhealthy_images)\n",
        "    \n",
        "    print(f\"   Resultado: Healthy={len(augmented_healthy)}, Unhealthy={len(unhealthy_images)}\")\n",
        "    \n",
        "    # Recalcular divis√µes\n",
        "    X_temp, X_test, y_temp, y_test = train_test_split(\n",
        "        all_images, all_labels, test_size=0.15, stratify=all_labels, random_state=42\n",
        "    )\n",
        "    \n",
        "    X_train, X_val, y_train, y_val = train_test_split(\n",
        "        X_temp, y_temp, test_size=0.176, stratify=y_temp, random_state=42\n",
        "    )\n",
        "    \n",
        "    return {\n",
        "        'train': {'X': X_train, 'y': y_train},\n",
        "        'val': {'X': X_val, 'y': y_val},\n",
        "        'test': {'X': X_test, 'y': y_test},\n",
        "        'info': {\n",
        "            'original_healthy': len(healthy_images),\n",
        "            'augmented_healthy': len(augmented_healthy),\n",
        "            'total_samples': len(all_images)\n",
        "        }\n",
        "    }\n",
        "\n",
        "def aplicar_smote_balanceamento(dataset, especie, target_ratio):\n",
        "    \"\"\"Aplica SMOTE para balanceamento adicional\"\"\"\n",
        "    \n",
        "    print(f\"\\nüîÑ Aplicando SMOTE para {especie} (target: {target_ratio*100:.0f}% healthy)...\")\n",
        "    \n",
        "    # Contar classes atuais\n",
        "    healthy_count = sum(1 for label in dataset['train']['y'] if label == 'healthy')\n",
        "    unhealthy_count = len(dataset['train']['y']) - healthy_count\n",
        "    \n",
        "    print(f\"   Distribui√ß√£o atual: Healthy={healthy_count}, Unhealthy={unhealthy_count}\")\n",
        "    \n",
        "    # Calcular quantas amostras healthy precisamos\n",
        "    total_target = int(unhealthy_count / (1 - target_ratio))\n",
        "    healthy_target = total_target - unhealthy_count\n",
        "    \n",
        "    print(f\"   Target calculado: Healthy={healthy_target}, Unhealthy={unhealthy_count}\")\n",
        "    \n",
        "    # Separar imagens por classe\n",
        "    healthy_images = [img for img, label in zip(dataset['train']['X'], dataset['train']['y']) if label == 'healthy']\n",
        "    unhealthy_images = [img for img, label in zip(dataset['train']['X'], dataset['train']['y']) if label == 'unhealthy']\n",
        "    \n",
        "    # Aplicar SMOTE simulado (replica√ß√£o inteligente)\n",
        "    balanced_healthy = []\n",
        "    balanced_labels = []\n",
        "    \n",
        "    for i in range(healthy_target):\n",
        "        idx = i % len(healthy_images)\n",
        "        balanced_healthy.append(healthy_images[idx])\n",
        "        balanced_labels.append('healthy')\n",
        "    \n",
        "    # Combinar com amostras unhealthy\n",
        "    all_images = balanced_healthy + unhealthy_images\n",
        "    all_labels = balanced_labels + ['unhealthy'] * len(unhealthy_images)\n",
        "    \n",
        "    print(f\"   Resultado SMOTE: Healthy={len(balanced_healthy)}, Unhealthy={len(unhealthy_images)}\")\n",
        "    \n",
        "    # Recalcular divis√µes\n",
        "    X_temp, X_test, y_temp, y_test = train_test_split(\n",
        "        all_images, all_labels, test_size=0.15, stratify=all_labels, random_state=42\n",
        "    )\n",
        "    \n",
        "    X_train, X_val, y_train, y_val = train_test_split(\n",
        "        X_temp, y_temp, test_size=0.176, stratify=y_temp, random_state=42\n",
        "    )\n",
        "    \n",
        "    return {\n",
        "        'train': {'X': X_train, 'y': y_train},\n",
        "        'val': {'X': X_val, 'y': y_val},\n",
        "        'test': {'X': X_test, 'y': y_test},\n",
        "        'info': {\n",
        "            'balanced_healthy': len(balanced_healthy),\n",
        "            'total_samples': len(all_images),\n",
        "            'target_ratio': target_ratio\n",
        "        }\n",
        "    }\n",
        "\n",
        "def comparar_distribuicoes(original, balanceado, especie):\n",
        "    \"\"\"Compara distribui√ß√µes antes e depois\"\"\"\n",
        "    \n",
        "    # Original\n",
        "    orig_healthy = sum(1 for label in original['train']['y'] if label == 'healthy')\n",
        "    orig_unhealthy = len(original['train']['y']) - orig_healthy\n",
        "    orig_ratio = orig_healthy / (orig_healthy + orig_unhealthy) * 100\n",
        "    \n",
        "    # Balanceado\n",
        "    bal_healthy = sum(1 for label in balanceado['train']['y'] if label == 'healthy')\n",
        "    bal_unhealthy = len(balanceado['train']['y']) - bal_healthy\n",
        "    bal_ratio = bal_healthy / (bal_healthy + bal_unhealthy) * 100\n",
        "    \n",
        "    print(f\"\\nüìä COMPARA√á√ÉO {especie.upper()}:\")\n",
        "    print(f\"   ORIGINAL:   Healthy={orig_healthy:4d} ({orig_ratio:5.1f}%), Unhealthy={orig_unhealthy:4d}\")\n",
        "    print(f\"   BALANCEADO: Healthy={bal_healthy:4d} ({bal_ratio:5.1f}%), Unhealthy={bal_unhealthy:4d}\")\n",
        "    print(f\"   MELHORIA:   +{bal_healthy - orig_healthy:4d} amostras Healthy ({bal_ratio - orig_ratio:+5.1f}%)\")\n",
        "    \n",
        "    return {\n",
        "        'original': {'healthy': orig_healthy, 'ratio': orig_ratio},\n",
        "        'balanced': {'healthy': bal_healthy, 'ratio': bal_ratio}\n",
        "    }\n",
        "\n",
        "print(\"‚úÖ Fun√ß√µes de balanceamento carregadas\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "=== APLICANDO BALANCEAMENTO OTIMIZADO ===\n",
            "\n",
            "üö® POTATO - Aplicando estrat√©gia CR√çTICA\n",
            "\n",
            "üîÑ Aplicando Data Augmentation direcionada para Potato...\n",
            "   Original: Healthy=106, Unhealthy=1401\n",
            "   üö® Estrat√©gia CR√çTICA: Multiplicador 8x\n",
            "   Resultado: Healthy=848, Unhealthy=1401\n",
            "\n",
            "üîÑ Aplicando SMOTE para Potato (target: 50% healthy)...\n",
            "   Distribui√ß√£o atual: Healthy=594, Unhealthy=980\n",
            "   Target calculado: Healthy=980, Unhealthy=980\n",
            "   Resultado SMOTE: Healthy=980, Unhealthy=980\n",
            "\n",
            "üìä COMPARA√á√ÉO POTATO:\n",
            "   ORIGINAL:   Healthy= 106 (  7.0%), Unhealthy=1401\n",
            "   BALANCEADO: Healthy= 686 ( 50.0%), Unhealthy= 686\n",
            "   MELHORIA:   + 580 amostras Healthy (+43.0%)\n",
            "\n",
            "‚ö†Ô∏è TOMATO - Aplicando estrat√©gia IMPORTANTE\n",
            "\n",
            "üîÑ Aplicando Data Augmentation direcionada para Tomato...\n",
            "   Original: Healthy=1114, Unhealthy=10099\n",
            "   ‚ö†Ô∏è Estrat√©gia IMPORTANTE: Multiplicador 4x\n",
            "   Resultado: Healthy=4456, Unhealthy=10099\n",
            "\n",
            "üîÑ Aplicando SMOTE para Tomato (target: 30% healthy)...\n",
            "   Distribui√ß√£o atual: Healthy=3120, Unhealthy=7073\n",
            "   Target calculado: Healthy=3031, Unhealthy=7073\n",
            "   Resultado SMOTE: Healthy=3031, Unhealthy=7073\n",
            "\n",
            "üìä COMPARA√á√ÉO TOMATO:\n",
            "   ORIGINAL:   Healthy=1114 (  9.9%), Unhealthy=10099\n",
            "   BALANCEADO: Healthy=2122 ( 30.0%), Unhealthy=4954\n",
            "   MELHORIA:   +1008 amostras Healthy (+20.1%)\n",
            "\n",
            "‚úÖ PEPPER - Aplicando estrat√©gia REFINAMENTO\n",
            "\n",
            "üîÑ Aplicando Data Augmentation direcionada para Pepper...\n",
            "   Original: Healthy=1034, Unhealthy=698\n",
            "   ‚úÖ Estrat√©gia REFINAMENTO: Multiplicador 2x\n",
            "   Resultado: Healthy=2068, Unhealthy=698\n",
            "\n",
            "üìä COMPARA√á√ÉO PEPPER:\n",
            "   ORIGINAL:   Healthy=1034 ( 59.7%), Unhealthy= 698\n",
            "   BALANCEADO: Healthy=1448 ( 74.8%), Unhealthy= 489\n",
            "   MELHORIA:   + 414 amostras Healthy (+15.1%)\n",
            "\n",
            "üéØ RESUMO DAS MELHORIAS APLICADAS:\n",
            "============================================================\n",
            "   Potato:   7.0% ‚Üí  50.0% (+43.0%) üü¢ EXCELENTE\n",
            "   Tomato:   9.9% ‚Üí  30.0% (+20.1%) üü° MELHORADO\n",
            "   Pepper:  59.7% ‚Üí  74.8% (+15.1%) üü¢ EXCELENTE\n",
            "============================================================\n",
            "\n",
            "‚úÖ BALANCEAMENTO CONCLU√çDO!\n",
            "üìä Datasets balanceados criados: ['potato', 'tomato', 'pepper']\n"
          ]
        }
      ],
      "source": [
        "# 3. APLICAR BALANCEAMENTO NOS DATASETS\n",
        "print(\"\\n=== APLICANDO BALANCEAMENTO OTIMIZADO ===\")\n",
        "\n",
        "# Criar vers√µes balanceadas dos datasets\n",
        "datasets_balanceados = {}\n",
        "comparacoes = {}\n",
        "\n",
        "# POTATO - Estrat√©gia CR√çTICA (Data Aug + SMOTE)\n",
        "if dataset_potato is not None:\n",
        "    print(\"\\nüö® POTATO - Aplicando estrat√©gia CR√çTICA\")\n",
        "    dataset_potato_aug = aplicar_data_augmentation_direcionada(dataset_potato, 'Potato')\n",
        "    dataset_potato_balanced = aplicar_smote_balanceamento(dataset_potato_aug, 'Potato', target_ratio=0.5)\n",
        "    datasets_balanceados['potato'] = dataset_potato_balanced\n",
        "    comparacoes['potato'] = comparar_distribuicoes(dataset_potato, dataset_potato_balanced, 'Potato')\n",
        "\n",
        "# TOMATO - Estrat√©gia IMPORTANTE (Data Aug + SMOTE)\n",
        "if dataset_tomato is not None:\n",
        "    print(\"\\n‚ö†Ô∏è TOMATO - Aplicando estrat√©gia IMPORTANTE\")\n",
        "    dataset_tomato_aug = aplicar_data_augmentation_direcionada(dataset_tomato, 'Tomato')\n",
        "    dataset_tomato_balanced = aplicar_smote_balanceamento(dataset_tomato_aug, 'Tomato', target_ratio=0.3)\n",
        "    datasets_balanceados['tomato'] = dataset_tomato_balanced\n",
        "    comparacoes['tomato'] = comparar_distribuicoes(dataset_tomato, dataset_tomato_balanced, 'Tomato')\n",
        "\n",
        "# PEPPER - Estrat√©gia REFINAMENTO (Apenas Data Aug)\n",
        "if dataset_pepper is not None:\n",
        "    print(\"\\n‚úÖ PEPPER - Aplicando estrat√©gia REFINAMENTO\")\n",
        "    dataset_pepper_balanced = aplicar_data_augmentation_direcionada(dataset_pepper, 'Pepper')\n",
        "    datasets_balanceados['pepper'] = dataset_pepper_balanced\n",
        "    comparacoes['pepper'] = comparar_distribuicoes(dataset_pepper, dataset_pepper_balanced, 'Pepper')\n",
        "\n",
        "# Resumo das melhorias\n",
        "print(\"\\nüéØ RESUMO DAS MELHORIAS APLICADAS:\")\n",
        "print(\"=\" * 60)\n",
        "for especie, comp in comparacoes.items():\n",
        "    original_ratio = comp['original']['ratio']\n",
        "    balanced_ratio = comp['balanced']['ratio']\n",
        "    melhoria = balanced_ratio - original_ratio\n",
        "    status = \"üü¢ EXCELENTE\" if balanced_ratio > 40 else \"üü° MELHORADO\" if melhoria > 10 else \"üî¥ INSUFICIENTE\"\n",
        "    print(f\"   {especie.capitalize()}: {original_ratio:5.1f}% ‚Üí {balanced_ratio:5.1f}% ({melhoria:+5.1f}%) {status}\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "print(f\"\\n‚úÖ BALANCEAMENTO CONCLU√çDO!\")\n",
        "print(f\"üìä Datasets balanceados criados: {list(datasets_balanceados.keys())}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=== CRIANDO GERADORES COM DADOS BALANCEADOS ===\n",
            "Found 7076 validated image filenames belonging to 2 classes.\n",
            "Found 1512 validated image filenames belonging to 2 classes.\n",
            "Found 1516 validated image filenames belonging to 2 classes.\n",
            "‚úÖ Geradores Tomato criados com dados balanceados e augmenta√ß√£o otimizada\n",
            "Found 1372 validated image filenames belonging to 2 classes.\n",
            "Found 294 validated image filenames belonging to 2 classes.\n",
            "Found 294 validated image filenames belonging to 2 classes.\n",
            "‚úÖ Geradores Potato criados com dados balanceados e augmenta√ß√£o otimizada\n",
            "Found 1937 validated image filenames belonging to 2 classes.\n",
            "Found 414 validated image filenames belonging to 2 classes.\n",
            "Found 415 validated image filenames belonging to 2 classes.\n",
            "‚úÖ Geradores Pepper criados com dados balanceados e augmenta√ß√£o otimizada\n",
            "\n",
            "üéØ Geradores otimizados criados com augmenta√ß√£o direcionada e dados balanceados!\n",
            "\n",
            "=== CRIANDO MODELOS BIN√ÅRIOS OTIMIZADOS ===\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "I0000 00:00:1752271420.766047    3780 gpu_device.cc:2019] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 3685 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 2060, pci bus id: 0000:01:00.0, compute capability: 7.5\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ Modelo bin√°rio Tomato: 24,136,961 par√¢metros\n",
            "‚úÖ Modelo bin√°rio Potato: 24,136,961 par√¢metros\n",
            "‚úÖ Modelo bin√°rio Pepper: 24,136,961 par√¢metros\n",
            "\n",
            "=== CALCULANDO CLASS WEIGHTS OTIMIZADOS ===\n",
            "   Class weights: Healthy=1.667, Unhealthy=0.714\n",
            "   Class weights: Healthy=1.000, Unhealthy=1.000\n",
            "   Class weights: Healthy=0.669, Unhealthy=1.981\n",
            "‚úÖ Class weights calculados com base nos dados balanceados\n"
          ]
        }
      ],
      "source": [
        "# 4. ARQUITETURA E TREINAMENTO COM DADOS BALANCEADOS\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.applications import ResNet50\n",
        "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D, Dropout, BatchNormalization\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n",
        "from tensorflow.keras.regularizers import l2\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "def criar_augmentacao_otimizada(especie):\n",
        "    \"\"\"Cria augmenta√ß√£o otimizada baseada na esp√©cie\"\"\"\n",
        "    \n",
        "    if especie.lower() == 'potato':\n",
        "        # Augmenta√ß√£o agressiva para Potato (problema cr√≠tico)\n",
        "        return ImageDataGenerator(\n",
        "            rescale=1./255,\n",
        "            rotation_range=30,\n",
        "            width_shift_range=0.3,\n",
        "            height_shift_range=0.3,\n",
        "            horizontal_flip=True,\n",
        "            vertical_flip=True,\n",
        "            zoom_range=0.3,\n",
        "            brightness_range=[0.6, 1.4],\n",
        "            shear_range=0.2,\n",
        "            fill_mode='nearest'\n",
        "        )\n",
        "    elif especie.lower() == 'tomato':\n",
        "        # Augmenta√ß√£o moderada para Tomato (problema importante)\n",
        "        return ImageDataGenerator(\n",
        "            rescale=1./255,\n",
        "            rotation_range=25,\n",
        "            width_shift_range=0.2,\n",
        "            height_shift_range=0.2,\n",
        "            horizontal_flip=True,\n",
        "            vertical_flip=True,\n",
        "            zoom_range=0.2,\n",
        "            brightness_range=[0.7, 1.3],\n",
        "            fill_mode='nearest'\n",
        "        )\n",
        "    else:  # Pepper Bell\n",
        "        # Augmenta√ß√£o conservadora para Pepper (refinamento)\n",
        "        return ImageDataGenerator(\n",
        "            rescale=1./255,\n",
        "            rotation_range=15,\n",
        "            width_shift_range=0.1,\n",
        "            height_shift_range=0.1,\n",
        "            horizontal_flip=True,\n",
        "            zoom_range=0.1,\n",
        "            brightness_range=[0.8, 1.2],\n",
        "            fill_mode='nearest'\n",
        "        )\n",
        "\n",
        "def criar_classificao_binaria(dataset, config, especie):\n",
        "    \"\"\"Cria geradores otimizados para classifica√ß√£o bin√°ria com dados balanceados\"\"\"\n",
        "    \n",
        "    # Data augmentation otimizada por esp√©cie\n",
        "    train_datagen = criar_augmentacao_otimizada(especie)\n",
        "    \n",
        "    val_test_datagen = ImageDataGenerator(rescale=1./255)\n",
        "    \n",
        "    # DataFrames\n",
        "    train_df = pd.DataFrame({'filename': dataset['train']['X'], 'class': dataset['train']['y']})\n",
        "    val_df = pd.DataFrame({'filename': dataset['val']['X'], 'class': dataset['val']['y']})\n",
        "    test_df = pd.DataFrame({'filename': dataset['test']['X'], 'class': dataset['test']['y']})\n",
        "    \n",
        "    # Geradores bin√°rios\n",
        "    train_gen = train_datagen.flow_from_dataframe(\n",
        "        train_df, x_col='filename', y_col='class',\n",
        "        target_size=(config['img_height'], config['img_width']),\n",
        "        batch_size=config['batch_size'],\n",
        "        class_mode='binary', shuffle=True, seed=42\n",
        "    )\n",
        "    \n",
        "    val_gen = val_test_datagen.flow_from_dataframe(\n",
        "        val_df, x_col='filename', y_col='class',\n",
        "        target_size=(config['img_height'], config['img_width']),\n",
        "        batch_size=config['batch_size'],\n",
        "        class_mode='binary', shuffle=False, seed=42\n",
        "    )\n",
        "    \n",
        "    test_gen = val_test_datagen.flow_from_dataframe(\n",
        "        test_df, x_col='filename', y_col='class',\n",
        "        target_size=(config['img_height'], config['img_width']),\n",
        "        batch_size=config['batch_size'],\n",
        "        class_mode='binary', shuffle=False, seed=42\n",
        "    )\n",
        "    \n",
        "    return train_gen, val_gen, test_gen\n",
        "\n",
        "def criar_modelo(especie_nome):\n",
        "    \"\"\"Cria modelo de classifica√ß√£o bin√°ria\"\"\"\n",
        "    base_model = ResNet50(\n",
        "        weights='imagenet',\n",
        "        include_top=False,\n",
        "        input_shape=(224, 224, 3)\n",
        "    )\n",
        "    \n",
        "    # Descongelar √∫ltimas camadas\n",
        "    base_model.trainable = True\n",
        "    for layer in base_model.layers[:-15]:\n",
        "        layer.trainable = False\n",
        "    \n",
        "    # Arquitetura otimizada para classifica√ß√£o bin√°ria\n",
        "    x = base_model.output\n",
        "    x = GlobalAveragePooling2D()(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = Dense(256, activation='relu', kernel_regularizer=l2(0.001))(x)\n",
        "    x = Dropout(0.6)(x)\n",
        "    x = Dense(64, activation='relu', kernel_regularizer=l2(0.001))(x)\n",
        "    x = Dropout(0.5)(x)\n",
        "    \n",
        "    # Sa√≠da bin√°ria com sigmoid\n",
        "    predictions = Dense(1, activation='sigmoid', name=f'output_{especie_nome}')(x)\n",
        "    \n",
        "    modelo = Model(inputs=base_model.input, outputs=predictions)\n",
        "    \n",
        "    print(f\"‚úÖ Modelo bin√°rio {especie_nome}: {modelo.count_params():,} par√¢metros\")\n",
        "    return modelo\n",
        "\n",
        "def calcular_class_weights(dataset):\n",
        "    \"\"\"Calcula class weights balanceados\"\"\"\n",
        "    healthy_count = sum(1 for label in dataset['train']['y'] if label == 'healthy')\n",
        "    unhealthy_count = len(dataset['train']['y']) - healthy_count\n",
        "    \n",
        "    total = len(dataset['train']['y'])\n",
        "    weight_healthy = total / (2 * healthy_count)\n",
        "    weight_unhealthy = total / (2 * unhealthy_count)\n",
        "    \n",
        "    class_weights = {0: weight_healthy, 1: weight_unhealthy}  # 0=healthy, 1=unhealthy\n",
        "    \n",
        "    print(f\"   Class weights: Healthy={weight_healthy:.3f}, Unhealthy={weight_unhealthy:.3f}\")\n",
        "    return class_weights\n",
        "\n",
        "# Criar geradores com dados balanceados\n",
        "print(\"=== CRIANDO GERADORES COM DADOS BALANCEADOS ===\")\n",
        "\n",
        "# Usar datasets balanceados se dispon√≠veis, sen√£o usar originais\n",
        "dataset_tomato_final = datasets_balanceados.get('tomato', dataset_tomato)\n",
        "dataset_potato_final = datasets_balanceados.get('potato', dataset_potato)\n",
        "dataset_pepper_final = datasets_balanceados.get('pepper', dataset_pepper)\n",
        "\n",
        "# Criar geradores otimizados\n",
        "if dataset_tomato_final:\n",
        "    train_gen_tomato, val_gen_tomato, test_gen_tomato = criar_classificao_binaria(dataset_tomato_final, config, 'tomato')\n",
        "    print(\"‚úÖ Geradores Tomato criados com dados balanceados e augmenta√ß√£o otimizada\")\n",
        "\n",
        "if dataset_potato_final:\n",
        "    train_gen_potato, val_gen_potato, test_gen_potato = criar_classificao_binaria(dataset_potato_final, config, 'potato')\n",
        "    print(\"‚úÖ Geradores Potato criados com dados balanceados e augmenta√ß√£o otimizada\")\n",
        "\n",
        "if dataset_pepper_final:\n",
        "    train_gen_pepper, val_gen_pepper, test_gen_pepper = criar_classificao_binaria(dataset_pepper_final, config, 'pepper')\n",
        "    print(\"‚úÖ Geradores Pepper criados com dados balanceados e augmenta√ß√£o otimizada\")\n",
        "\n",
        "print(f\"\\nüéØ Geradores otimizados criados com augmenta√ß√£o direcionada e dados balanceados!\")\n",
        "\n",
        "# Criar modelos\n",
        "print(\"\\n=== CRIANDO MODELOS BIN√ÅRIOS OTIMIZADOS ===\")\n",
        "modelo_tomato = criar_modelo('Tomato')\n",
        "modelo_potato = criar_modelo('Potato')\n",
        "modelo_pepper = criar_modelo('Pepper')\n",
        "\n",
        "# Calcular class weights otimizados para dados balanceados\n",
        "print(\"\\n=== CALCULANDO CLASS WEIGHTS OTIMIZADOS ===\")\n",
        "cw_tomato = calcular_class_weights(dataset_tomato_final) if dataset_tomato_final else None\n",
        "cw_potato = calcular_class_weights(dataset_potato_final) if dataset_potato_final else None\n",
        "cw_pepper = calcular_class_weights(dataset_pepper_final) if dataset_pepper_final else None\n",
        "\n",
        "print(\"‚úÖ Class weights calculados com base nos dados balanceados\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=== TREINAMENTO DOS MODELOS COM DADOS BALANCEADOS ===\n",
            "üéØ EXPECTATIVAS DE MELHORIA:\n",
            "   üö® Potato: 0% ‚Üí 40-60% recall Healthy\n",
            "   ‚ö†Ô∏è Tomato: 56% ‚Üí 70-80% recall Healthy\n",
            "   ‚úÖ Pepper: 80% ‚Üí 85-90% recall Healthy\n",
            "\n",
            "\n",
            "üöÄ Treinando Tomato...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/gustavo/.local/lib/python3.10/site-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
            "  self._warn_if_super_not_called()\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/40\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "I0000 00:00:1752271443.567289   10389 service.cc:152] XLA service 0x7b7080001d90 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
            "I0000 00:00:1752271443.567334   10389 service.cc:160]   StreamExecutor device (0): NVIDIA GeForce RTX 2060, Compute Capability 7.5\n",
            "2025-07-11 19:04:03.993429: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:269] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
            "I0000 00:00:1752271445.800794   10389 cuda_dnn.cc:529] Loaded cuDNN version 90300\n",
            "2025-07-11 19:04:07.364753: I external/local_xla/xla/service/gpu/autotuning/conv_algorithm_picker.cc:549] Omitted potentially buggy algorithm eng14{k25=0} for conv %cudnn-conv-bias-activation.165 = (f32[32,64,56,56]{3,2,1,0}, u8[0]{0}) custom-call(f32[32,64,56,56]{3,2,1,0} %bitcast.14083, f32[64,64,3,3]{3,2,1,0} %bitcast.14090, f32[64]{0} %bitcast.14092), window={size=3x3 pad=1_1x1_1}, dim_labels=bf01_oi01->bf01, custom_call_target=\"__cudnn$convBiasActivationForward\", metadata={op_type=\"Conv2D\" op_name=\"functional_1/conv2_block1_2_conv_1/convolution\" source_file=\"/home/gustavo/.local/lib/python3.10/site-packages/tensorflow/python/framework/ops.py\" source_line=1200}, backend_config={\"operation_queue_id\":\"0\",\"wait_on_operation_queues\":[],\"cudnn_conv_backend_config\":{\"conv_result_scale\":1,\"activation_mode\":\"kNone\",\"side_input_scale\":0,\"leakyrelu_alpha\":0},\"force_earliest_schedule\":false}\n",
            "2025-07-11 19:04:07.874689: I external/local_xla/xla/service/gpu/autotuning/conv_algorithm_picker.cc:549] Omitted potentially buggy algorithm eng14{k25=0} for conv %cudnn-conv-bias-activation.175 = (f32[32,128,28,28]{3,2,1,0}, u8[0]{0}) custom-call(f32[32,128,28,28]{3,2,1,0} %bitcast.14488, f32[128,128,3,3]{3,2,1,0} %bitcast.14495, f32[128]{0} %bitcast.14497), window={size=3x3 pad=1_1x1_1}, dim_labels=bf01_oi01->bf01, custom_call_target=\"__cudnn$convBiasActivationForward\", metadata={op_type=\"Conv2D\" op_name=\"functional_1/conv3_block1_2_conv_1/convolution\" source_file=\"/home/gustavo/.local/lib/python3.10/site-packages/tensorflow/python/framework/ops.py\" source_line=1200}, backend_config={\"operation_queue_id\":\"0\",\"wait_on_operation_queues\":[],\"cudnn_conv_backend_config\":{\"conv_result_scale\":1,\"activation_mode\":\"kNone\",\"side_input_scale\":0,\"leakyrelu_alpha\":0},\"force_earliest_schedule\":false}\n",
            "2025-07-11 19:04:08.367077: I external/local_xla/xla/service/gpu/autotuning/conv_algorithm_picker.cc:549] Omitted potentially buggy algorithm eng14{k25=0} for conv %cudnn-conv-bias-activation.188 = (f32[32,256,14,14]{3,2,1,0}, u8[0]{0}) custom-call(f32[32,256,14,14]{3,2,1,0} %bitcast.15016, f32[256,256,3,3]{3,2,1,0} %bitcast.15023, f32[256]{0} %bitcast.15025), window={size=3x3 pad=1_1x1_1}, dim_labels=bf01_oi01->bf01, custom_call_target=\"__cudnn$convBiasActivationForward\", metadata={op_type=\"Conv2D\" op_name=\"functional_1/conv4_block1_2_conv_1/convolution\" source_file=\"/home/gustavo/.local/lib/python3.10/site-packages/tensorflow/python/framework/ops.py\" source_line=1200}, backend_config={\"operation_queue_id\":\"0\",\"wait_on_operation_queues\":[],\"cudnn_conv_backend_config\":{\"conv_result_scale\":1,\"activation_mode\":\"kNone\",\"side_input_scale\":0,\"leakyrelu_alpha\":0},\"force_earliest_schedule\":false}\n",
            "2025-07-11 19:04:08.924220: I external/local_xla/xla/service/gpu/autotuning/conv_algorithm_picker.cc:549] Omitted potentially buggy algorithm eng14{k25=0} for conv %cudnn-conv-bias-activation.207 = (f32[32,512,7,7]{3,2,1,0}, u8[0]{0}) custom-call(f32[32,512,7,7]{3,2,1,0} %bitcast.15790, f32[512,512,3,3]{3,2,1,0} %bitcast.15797, f32[512]{0} %bitcast.15799), window={size=3x3 pad=1_1x1_1}, dim_labels=bf01_oi01->bf01, custom_call_target=\"__cudnn$convBiasActivationForward\", metadata={op_type=\"Conv2D\" op_name=\"functional_1/conv5_block1_2_conv_1/convolution\" source_file=\"/home/gustavo/.local/lib/python3.10/site-packages/tensorflow/python/framework/ops.py\" source_line=1200}, backend_config={\"operation_queue_id\":\"0\",\"wait_on_operation_queues\":[],\"cudnn_conv_backend_config\":{\"conv_result_scale\":1,\"activation_mode\":\"kNone\",\"side_input_scale\":0,\"leakyrelu_alpha\":0},\"force_earliest_schedule\":false}\n",
            "I0000 00:00:1752271453.989709   10389 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m  3/222\u001b[0m \u001b[37m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[1m56s\u001b[0m 257ms/step - accuracy: 0.6372 - loss: 1.5106 "
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-07-11 19:04:16.987770: I external/local_xla/xla/service/gpu/autotuning/conv_algorithm_picker.cc:549] Omitted potentially buggy algorithm eng14{k25=0} for conv %cudnn-conv-bias-activation.165 = (f32[4,64,56,56]{3,2,1,0}, u8[0]{0}) custom-call(f32[4,64,56,56]{3,2,1,0} %bitcast.14083, f32[64,64,3,3]{3,2,1,0} %bitcast.14090, f32[64]{0} %bitcast.14092), window={size=3x3 pad=1_1x1_1}, dim_labels=bf01_oi01->bf01, custom_call_target=\"__cudnn$convBiasActivationForward\", metadata={op_type=\"Conv2D\" op_name=\"functional_1/conv2_block1_2_conv_1/convolution\" source_file=\"/home/gustavo/.local/lib/python3.10/site-packages/tensorflow/python/framework/ops.py\" source_line=1200}, backend_config={\"operation_queue_id\":\"0\",\"wait_on_operation_queues\":[],\"cudnn_conv_backend_config\":{\"conv_result_scale\":1,\"activation_mode\":\"kNone\",\"side_input_scale\":0,\"leakyrelu_alpha\":0},\"force_earliest_schedule\":false}\n",
            "2025-07-11 19:04:17.215599: I external/local_xla/xla/service/gpu/autotuning/conv_algorithm_picker.cc:549] Omitted potentially buggy algorithm eng14{k25=0} for conv %cudnn-conv-bias-activation.175 = (f32[4,128,28,28]{3,2,1,0}, u8[0]{0}) custom-call(f32[4,128,28,28]{3,2,1,0} %bitcast.14488, f32[128,128,3,3]{3,2,1,0} %bitcast.14495, f32[128]{0} %bitcast.14497), window={size=3x3 pad=1_1x1_1}, dim_labels=bf01_oi01->bf01, custom_call_target=\"__cudnn$convBiasActivationForward\", metadata={op_type=\"Conv2D\" op_name=\"functional_1/conv3_block1_2_conv_1/convolution\" source_file=\"/home/gustavo/.local/lib/python3.10/site-packages/tensorflow/python/framework/ops.py\" source_line=1200}, backend_config={\"operation_queue_id\":\"0\",\"wait_on_operation_queues\":[],\"cudnn_conv_backend_config\":{\"conv_result_scale\":1,\"activation_mode\":\"kNone\",\"side_input_scale\":0,\"leakyrelu_alpha\":0},\"force_earliest_schedule\":false}\n",
            "2025-07-11 19:04:17.436507: I external/local_xla/xla/service/gpu/autotuning/conv_algorithm_picker.cc:549] Omitted potentially buggy algorithm eng14{k25=0} for conv %cudnn-conv-bias-activation.188 = (f32[4,256,14,14]{3,2,1,0}, u8[0]{0}) custom-call(f32[4,256,14,14]{3,2,1,0} %bitcast.15016, f32[256,256,3,3]{3,2,1,0} %bitcast.15023, f32[256]{0} %bitcast.15025), window={size=3x3 pad=1_1x1_1}, dim_labels=bf01_oi01->bf01, custom_call_target=\"__cudnn$convBiasActivationForward\", metadata={op_type=\"Conv2D\" op_name=\"functional_1/conv4_block1_2_conv_1/convolution\" source_file=\"/home/gustavo/.local/lib/python3.10/site-packages/tensorflow/python/framework/ops.py\" source_line=1200}, backend_config={\"operation_queue_id\":\"0\",\"wait_on_operation_queues\":[],\"cudnn_conv_backend_config\":{\"conv_result_scale\":1,\"activation_mode\":\"kNone\",\"side_input_scale\":0,\"leakyrelu_alpha\":0},\"force_earliest_schedule\":false}\n",
            "2025-07-11 19:04:17.699198: I external/local_xla/xla/service/gpu/autotuning/conv_algorithm_picker.cc:549] Omitted potentially buggy algorithm eng14{k25=0} for conv %cudnn-conv-bias-activation.207 = (f32[4,512,7,7]{3,2,1,0}, u8[0]{0}) custom-call(f32[4,512,7,7]{3,2,1,0} %bitcast.15790, f32[512,512,3,3]{3,2,1,0} %bitcast.15797, f32[512]{0} %bitcast.15799), window={size=3x3 pad=1_1x1_1}, dim_labels=bf01_oi01->bf01, custom_call_target=\"__cudnn$convBiasActivationForward\", metadata={op_type=\"Conv2D\" op_name=\"functional_1/conv5_block1_2_conv_1/convolution\" source_file=\"/home/gustavo/.local/lib/python3.10/site-packages/tensorflow/python/framework/ops.py\" source_line=1200}, backend_config={\"operation_queue_id\":\"0\",\"wait_on_operation_queues\":[],\"cudnn_conv_backend_config\":{\"conv_result_scale\":1,\"activation_mode\":\"kNone\",\"side_input_scale\":0,\"leakyrelu_alpha\":0},\"force_earliest_schedule\":false}\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m222/222\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 367ms/step - accuracy: 0.7012 - loss: 1.2003"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-07-11 19:05:42.533260: I external/local_xla/xla/service/gpu/autotuning/conv_algorithm_picker.cc:549] Omitted potentially buggy algorithm eng14{k25=0} for conv %cudnn-conv-bias-activation.162 = (f32[8,64,56,56]{3,2,1,0}, u8[0]{0}) custom-call(f32[8,64,56,56]{3,2,1,0} %bitcast.4834, f32[64,64,3,3]{3,2,1,0} %bitcast.4841, f32[64]{0} %bitcast.4843), window={size=3x3 pad=1_1x1_1}, dim_labels=bf01_oi01->bf01, custom_call_target=\"__cudnn$convBiasActivationForward\", metadata={op_type=\"Conv2D\" op_name=\"functional_1/conv2_block1_2_conv_1/convolution\" source_file=\"/home/gustavo/.local/lib/python3.10/site-packages/tensorflow/python/framework/ops.py\" source_line=1200}, backend_config={\"operation_queue_id\":\"0\",\"wait_on_operation_queues\":[],\"cudnn_conv_backend_config\":{\"conv_result_scale\":1,\"activation_mode\":\"kNone\",\"side_input_scale\":0,\"leakyrelu_alpha\":0},\"force_earliest_schedule\":false}\n",
            "2025-07-11 19:05:42.792782: I external/local_xla/xla/service/gpu/autotuning/conv_algorithm_picker.cc:549] Omitted potentially buggy algorithm eng14{k25=0} for conv %cudnn-conv-bias-activation.172 = (f32[8,128,28,28]{3,2,1,0}, u8[0]{0}) custom-call(f32[8,128,28,28]{3,2,1,0} %bitcast.5239, f32[128,128,3,3]{3,2,1,0} %bitcast.5246, f32[128]{0} %bitcast.5248), window={size=3x3 pad=1_1x1_1}, dim_labels=bf01_oi01->bf01, custom_call_target=\"__cudnn$convBiasActivationForward\", metadata={op_type=\"Conv2D\" op_name=\"functional_1/conv3_block1_2_conv_1/convolution\" source_file=\"/home/gustavo/.local/lib/python3.10/site-packages/tensorflow/python/framework/ops.py\" source_line=1200}, backend_config={\"operation_queue_id\":\"0\",\"wait_on_operation_queues\":[],\"cudnn_conv_backend_config\":{\"conv_result_scale\":1,\"activation_mode\":\"kNone\",\"side_input_scale\":0,\"leakyrelu_alpha\":0},\"force_earliest_schedule\":false}\n",
            "2025-07-11 19:05:43.084173: I external/local_xla/xla/service/gpu/autotuning/conv_algorithm_picker.cc:549] Omitted potentially buggy algorithm eng14{k25=0} for conv %cudnn-conv-bias-activation.185 = (f32[8,256,14,14]{3,2,1,0}, u8[0]{0}) custom-call(f32[8,256,14,14]{3,2,1,0} %bitcast.5767, f32[256,256,3,3]{3,2,1,0} %bitcast.5774, f32[256]{0} %bitcast.5776), window={size=3x3 pad=1_1x1_1}, dim_labels=bf01_oi01->bf01, custom_call_target=\"__cudnn$convBiasActivationForward\", metadata={op_type=\"Conv2D\" op_name=\"functional_1/conv4_block1_2_conv_1/convolution\" source_file=\"/home/gustavo/.local/lib/python3.10/site-packages/tensorflow/python/framework/ops.py\" source_line=1200}, backend_config={\"operation_queue_id\":\"0\",\"wait_on_operation_queues\":[],\"cudnn_conv_backend_config\":{\"conv_result_scale\":1,\"activation_mode\":\"kNone\",\"side_input_scale\":0,\"leakyrelu_alpha\":0},\"force_earliest_schedule\":false}\n",
            "2025-07-11 19:05:43.383355: I external/local_xla/xla/service/gpu/autotuning/conv_algorithm_picker.cc:549] Omitted potentially buggy algorithm eng14{k25=0} for conv %cudnn-conv-bias-activation.204 = (f32[8,512,7,7]{3,2,1,0}, u8[0]{0}) custom-call(f32[8,512,7,7]{3,2,1,0} %bitcast.6541, f32[512,512,3,3]{3,2,1,0} %bitcast.6548, f32[512]{0} %bitcast.6550), window={size=3x3 pad=1_1x1_1}, dim_labels=bf01_oi01->bf01, custom_call_target=\"__cudnn$convBiasActivationForward\", metadata={op_type=\"Conv2D\" op_name=\"functional_1/conv5_block1_2_conv_1/convolution\" source_file=\"/home/gustavo/.local/lib/python3.10/site-packages/tensorflow/python/framework/ops.py\" source_line=1200}, backend_config={\"operation_queue_id\":\"0\",\"wait_on_operation_queues\":[],\"cudnn_conv_backend_config\":{\"conv_result_scale\":1,\"activation_mode\":\"kNone\",\"side_input_scale\":0,\"leakyrelu_alpha\":0},\"force_earliest_schedule\":false}\n",
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m222/222\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m109s\u001b[0m 417ms/step - accuracy: 0.7015 - loss: 1.1996 - val_accuracy: 0.5582 - val_loss: 1.1883 - learning_rate: 1.0000e-04\n",
            "Epoch 2/40\n",
            "\u001b[1m222/222\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 247ms/step - accuracy: 0.8416 - loss: 0.9025"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m222/222\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 263ms/step - accuracy: 0.8416 - loss: 0.9025 - val_accuracy: 0.7692 - val_loss: 0.9759 - learning_rate: 1.0000e-04\n",
            "Epoch 3/40\n",
            "\u001b[1m222/222\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 255ms/step - accuracy: 0.8525 - loss: 0.8634"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m222/222\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m61s\u001b[0m 274ms/step - accuracy: 0.8525 - loss: 0.8634 - val_accuracy: 0.8975 - val_loss: 0.7482 - learning_rate: 1.0000e-04\n",
            "Epoch 4/40\n",
            "\u001b[1m222/222\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 248ms/step - accuracy: 0.8619 - loss: 0.8384"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m222/222\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m60s\u001b[0m 272ms/step - accuracy: 0.8619 - loss: 0.8383 - val_accuracy: 0.9286 - val_loss: 0.6903 - learning_rate: 1.0000e-04\n",
            "Epoch 5/40\n",
            "\u001b[1m222/222\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 247ms/step - accuracy: 0.8930 - loss: 0.7604"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m222/222\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 264ms/step - accuracy: 0.8930 - loss: 0.7605 - val_accuracy: 0.9358 - val_loss: 0.6746 - learning_rate: 1.0000e-04\n",
            "Epoch 6/40\n",
            "\u001b[1m222/222\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m62s\u001b[0m 278ms/step - accuracy: 0.8657 - loss: 0.7941 - val_accuracy: 0.8995 - val_loss: 0.8192 - learning_rate: 1.0000e-04\n",
            "Epoch 7/40\n",
            "\u001b[1m222/222\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 266ms/step - accuracy: 0.8864 - loss: 0.7303 - val_accuracy: 0.7394 - val_loss: 1.7112 - learning_rate: 1.0000e-04\n",
            "Epoch 8/40\n",
            "\u001b[1m222/222\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m61s\u001b[0m 274ms/step - accuracy: 0.8768 - loss: 0.7632 - val_accuracy: 0.4431 - val_loss: 2.0519 - learning_rate: 1.0000e-04\n",
            "Epoch 9/40\n",
            "\u001b[1m222/222\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m60s\u001b[0m 271ms/step - accuracy: 0.9026 - loss: 0.6874 - val_accuracy: 0.9266 - val_loss: 0.6221 - learning_rate: 1.0000e-04\n",
            "Epoch 10/40\n",
            "\u001b[1m222/222\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 265ms/step - accuracy: 0.8876 - loss: 0.6979 - val_accuracy: 0.5622 - val_loss: 1.6071 - learning_rate: 1.0000e-04\n",
            "Epoch 11/40\n",
            "\u001b[1m222/222\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m62s\u001b[0m 278ms/step - accuracy: 0.9023 - loss: 0.6691 - val_accuracy: 0.9193 - val_loss: 0.7565 - learning_rate: 1.0000e-04\n",
            "Epoch 12/40\n",
            "\u001b[1m222/222\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 264ms/step - accuracy: 0.9121 - loss: 0.6346 - val_accuracy: 0.7718 - val_loss: 1.8199 - learning_rate: 1.0000e-04\n",
            "Epoch 13/40\n",
            "\u001b[1m222/222\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 266ms/step - accuracy: 0.8961 - loss: 0.6428 - val_accuracy: 0.8545 - val_loss: 0.6977 - learning_rate: 1.0000e-04\n",
            "Epoch 14/40\n",
            "\u001b[1m222/222\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 254ms/step - accuracy: 0.9147 - loss: 0.6020"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m222/222\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m61s\u001b[0m 273ms/step - accuracy: 0.9146 - loss: 0.6020 - val_accuracy: 0.9537 - val_loss: 0.5090 - learning_rate: 1.0000e-04\n",
            "Epoch 15/40\n",
            "\u001b[1m222/222\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 264ms/step - accuracy: 0.9149 - loss: 0.5674 - val_accuracy: 0.6647 - val_loss: 1.2301 - learning_rate: 1.0000e-04\n",
            "Epoch 16/40\n",
            "\u001b[1m222/222\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 266ms/step - accuracy: 0.9093 - loss: 0.5703 - val_accuracy: 0.5668 - val_loss: 1.4559 - learning_rate: 1.0000e-04\n",
            "Epoch 17/40\n",
            "\u001b[1m222/222\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 259ms/step - accuracy: 0.9083 - loss: 0.5551 - val_accuracy: 0.9478 - val_loss: 0.4792 - learning_rate: 1.0000e-04\n",
            "Epoch 18/40\n",
            "\u001b[1m222/222\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 262ms/step - accuracy: 0.9120 - loss: 0.5363 - val_accuracy: 0.4689 - val_loss: 2.1290 - learning_rate: 1.0000e-04\n",
            "Epoch 19/40\n",
            "\u001b[1m222/222\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m56s\u001b[0m 253ms/step - accuracy: 0.9054 - loss: 0.5370 - val_accuracy: 0.9233 - val_loss: 0.5708 - learning_rate: 1.0000e-04\n",
            "Epoch 20/40\n",
            "\u001b[1m222/222\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m57s\u001b[0m 258ms/step - accuracy: 0.9142 - loss: 0.5120 - val_accuracy: 0.4974 - val_loss: 1.6374 - learning_rate: 1.0000e-04\n",
            "Epoch 21/40\n",
            "\u001b[1m222/222\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 268ms/step - accuracy: 0.9269 - loss: 0.4697 - val_accuracy: 0.9312 - val_loss: 0.4335 - learning_rate: 1.0000e-04\n",
            "Epoch 22/40\n",
            "\u001b[1m222/222\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m57s\u001b[0m 255ms/step - accuracy: 0.9260 - loss: 0.4563 - val_accuracy: 0.8823 - val_loss: 0.5414 - learning_rate: 1.0000e-04\n",
            "Epoch 23/40\n",
            "\u001b[1m222/222\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m61s\u001b[0m 274ms/step - accuracy: 0.9242 - loss: 0.4533 - val_accuracy: 0.8267 - val_loss: 0.6391 - learning_rate: 1.0000e-04\n",
            "Epoch 24/40\n",
            "\u001b[1m222/222\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 267ms/step - accuracy: 0.9265 - loss: 0.4157 - val_accuracy: 0.8929 - val_loss: 0.5048 - learning_rate: 1.0000e-04\n",
            "Epoch 25/40\n",
            "\u001b[1m222/222\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 264ms/step - accuracy: 0.9180 - loss: 0.4316 - val_accuracy: 0.7996 - val_loss: 0.7453 - learning_rate: 1.0000e-04\n",
            "Epoch 26/40\n",
            "\u001b[1m222/222\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m60s\u001b[0m 270ms/step - accuracy: 0.9244 - loss: 0.4076 - val_accuracy: 0.9259 - val_loss: 0.3982 - learning_rate: 1.0000e-04\n",
            "Epoch 27/40\n",
            "\u001b[1m222/222\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 254ms/step - accuracy: 0.9187 - loss: 0.4085 - val_accuracy: 0.9325 - val_loss: 0.3841 - learning_rate: 1.0000e-04\n",
            "Epoch 28/40\n",
            "\u001b[1m222/222\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 252ms/step - accuracy: 0.9279 - loss: 0.3797"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m222/222\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m61s\u001b[0m 273ms/step - accuracy: 0.9278 - loss: 0.3797 - val_accuracy: 0.9663 - val_loss: 0.2882 - learning_rate: 1.0000e-04\n",
            "Epoch 29/40\n",
            "\u001b[1m222/222\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 248ms/step - accuracy: 0.9249 - loss: 0.3710"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m222/222\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 267ms/step - accuracy: 0.9249 - loss: 0.3710 - val_accuracy: 0.9702 - val_loss: 0.2680 - learning_rate: 1.0000e-04\n",
            "Epoch 30/40\n",
            "\u001b[1m222/222\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 259ms/step - accuracy: 0.9329 - loss: 0.3599 - val_accuracy: 0.6640 - val_loss: 1.1744 - learning_rate: 1.0000e-04\n",
            "Epoch 31/40\n",
            "\u001b[1m222/222\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m60s\u001b[0m 271ms/step - accuracy: 0.9249 - loss: 0.3527 - val_accuracy: 0.9478 - val_loss: 0.2930 - learning_rate: 1.0000e-04\n",
            "Epoch 32/40\n",
            "\u001b[1m222/222\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 267ms/step - accuracy: 0.9287 - loss: 0.3493 - val_accuracy: 0.8902 - val_loss: 0.4123 - learning_rate: 1.0000e-04\n",
            "Epoch 33/40\n",
            "\u001b[1m222/222\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 259ms/step - accuracy: 0.9221 - loss: 0.3424 - val_accuracy: 0.7758 - val_loss: 1.8441 - learning_rate: 1.0000e-04\n",
            "Epoch 34/40\n",
            "\u001b[1m222/222\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 265ms/step - accuracy: 0.9222 - loss: 0.3435 - val_accuracy: 0.7606 - val_loss: 0.7486 - learning_rate: 1.0000e-04\n",
            "Epoch 35/40\n",
            "\u001b[1m222/222\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 268ms/step - accuracy: 0.9349 - loss: 0.3018 - val_accuracy: 0.9074 - val_loss: 0.4323 - learning_rate: 1.0000e-04\n",
            "Epoch 36/40\n",
            "\u001b[1m222/222\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 265ms/step - accuracy: 0.9430 - loss: 0.2851 - val_accuracy: 0.8049 - val_loss: 0.6428 - learning_rate: 3.0000e-05\n",
            "Epoch 37/40\n",
            "\u001b[1m222/222\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 266ms/step - accuracy: 0.9447 - loss: 0.2661 - val_accuracy: 0.7374 - val_loss: 0.8532 - learning_rate: 3.0000e-05\n",
            "Epoch 38/40\n",
            "\u001b[1m222/222\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 264ms/step - accuracy: 0.9352 - loss: 0.2821 - val_accuracy: 0.6772 - val_loss: 1.0532 - learning_rate: 3.0000e-05\n",
            "Epoch 39/40\n",
            "\u001b[1m222/222\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m60s\u001b[0m 268ms/step - accuracy: 0.9457 - loss: 0.2597 - val_accuracy: 0.5919 - val_loss: 1.5063 - learning_rate: 3.0000e-05\n",
            "Epoch 40/40\n",
            "\u001b[1m222/222\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 260ms/step - accuracy: 0.9458 - loss: 0.2679 - val_accuracy: 0.8585 - val_loss: 0.4913 - learning_rate: 3.0000e-05\n",
            "‚úÖ Tomato conclu√≠do! Melhor accuracy: 0.9702\n",
            "\n",
            "üöÄ Treinando Potato...\n",
            "Epoch 1/40\n",
            "\u001b[1m41/43\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m‚îÅ\u001b[0m \u001b[1m0s\u001b[0m 298ms/step - accuracy: 0.5514 - loss: 1.5617"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-07-11 19:44:40.365071: I external/local_xla/xla/service/gpu/autotuning/conv_algorithm_picker.cc:549] Omitted potentially buggy algorithm eng14{k25=0} for conv %cudnn-conv-bias-activation.165 = (f32[28,64,56,56]{3,2,1,0}, u8[0]{0}) custom-call(f32[28,64,56,56]{3,2,1,0} %bitcast.14083, f32[64,64,3,3]{3,2,1,0} %bitcast.14090, f32[64]{0} %bitcast.14092), window={size=3x3 pad=1_1x1_1}, dim_labels=bf01_oi01->bf01, custom_call_target=\"__cudnn$convBiasActivationForward\", metadata={op_type=\"Conv2D\" op_name=\"functional_1_1/conv2_block1_2_conv_1/convolution\" source_file=\"/home/gustavo/.local/lib/python3.10/site-packages/tensorflow/python/framework/ops.py\" source_line=1200}, backend_config={\"operation_queue_id\":\"0\",\"wait_on_operation_queues\":[],\"cudnn_conv_backend_config\":{\"conv_result_scale\":1,\"activation_mode\":\"kNone\",\"side_input_scale\":0,\"leakyrelu_alpha\":0},\"force_earliest_schedule\":false}\n",
            "2025-07-11 19:44:40.820848: I external/local_xla/xla/service/gpu/autotuning/conv_algorithm_picker.cc:549] Omitted potentially buggy algorithm eng14{k25=0} for conv %cudnn-conv-bias-activation.175 = (f32[28,128,28,28]{3,2,1,0}, u8[0]{0}) custom-call(f32[28,128,28,28]{3,2,1,0} %bitcast.14488, f32[128,128,3,3]{3,2,1,0} %bitcast.14495, f32[128]{0} %bitcast.14497), window={size=3x3 pad=1_1x1_1}, dim_labels=bf01_oi01->bf01, custom_call_target=\"__cudnn$convBiasActivationForward\", metadata={op_type=\"Conv2D\" op_name=\"functional_1_1/conv3_block1_2_conv_1/convolution\" source_file=\"/home/gustavo/.local/lib/python3.10/site-packages/tensorflow/python/framework/ops.py\" source_line=1200}, backend_config={\"operation_queue_id\":\"0\",\"wait_on_operation_queues\":[],\"cudnn_conv_backend_config\":{\"conv_result_scale\":1,\"activation_mode\":\"kNone\",\"side_input_scale\":0,\"leakyrelu_alpha\":0},\"force_earliest_schedule\":false}\n",
            "2025-07-11 19:44:41.300114: I external/local_xla/xla/service/gpu/autotuning/conv_algorithm_picker.cc:549] Omitted potentially buggy algorithm eng14{k25=0} for conv %cudnn-conv-bias-activation.188 = (f32[28,256,14,14]{3,2,1,0}, u8[0]{0}) custom-call(f32[28,256,14,14]{3,2,1,0} %bitcast.15016, f32[256,256,3,3]{3,2,1,0} %bitcast.15023, f32[256]{0} %bitcast.15025), window={size=3x3 pad=1_1x1_1}, dim_labels=bf01_oi01->bf01, custom_call_target=\"__cudnn$convBiasActivationForward\", metadata={op_type=\"Conv2D\" op_name=\"functional_1_1/conv4_block1_2_conv_1/convolution\" source_file=\"/home/gustavo/.local/lib/python3.10/site-packages/tensorflow/python/framework/ops.py\" source_line=1200}, backend_config={\"operation_queue_id\":\"0\",\"wait_on_operation_queues\":[],\"cudnn_conv_backend_config\":{\"conv_result_scale\":1,\"activation_mode\":\"kNone\",\"side_input_scale\":0,\"leakyrelu_alpha\":0},\"force_earliest_schedule\":false}\n",
            "2025-07-11 19:44:41.788389: I external/local_xla/xla/service/gpu/autotuning/conv_algorithm_picker.cc:549] Omitted potentially buggy algorithm eng14{k25=0} for conv %cudnn-conv-bias-activation.207 = (f32[28,512,7,7]{3,2,1,0}, u8[0]{0}) custom-call(f32[28,512,7,7]{3,2,1,0} %bitcast.15790, f32[512,512,3,3]{3,2,1,0} %bitcast.15797, f32[512]{0} %bitcast.15799), window={size=3x3 pad=1_1x1_1}, dim_labels=bf01_oi01->bf01, custom_call_target=\"__cudnn$convBiasActivationForward\", metadata={op_type=\"Conv2D\" op_name=\"functional_1_1/conv5_block1_2_conv_1/convolution\" source_file=\"/home/gustavo/.local/lib/python3.10/site-packages/tensorflow/python/framework/ops.py\" source_line=1200}, backend_config={\"operation_queue_id\":\"0\",\"wait_on_operation_queues\":[],\"cudnn_conv_backend_config\":{\"conv_result_scale\":1,\"activation_mode\":\"kNone\",\"side_input_scale\":0,\"leakyrelu_alpha\":0},\"force_earliest_schedule\":false}\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m43/43\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 479ms/step - accuracy: 0.5532 - loss: 1.5572"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-07-11 19:44:49.904383: I external/local_xla/xla/service/gpu/autotuning/conv_algorithm_picker.cc:549] Omitted potentially buggy algorithm eng14{k25=0} for conv %cudnn-conv-bias-activation.162 = (f32[6,64,56,56]{3,2,1,0}, u8[0]{0}) custom-call(f32[6,64,56,56]{3,2,1,0} %bitcast.4834, f32[64,64,3,3]{3,2,1,0} %bitcast.4841, f32[64]{0} %bitcast.4843), window={size=3x3 pad=1_1x1_1}, dim_labels=bf01_oi01->bf01, custom_call_target=\"__cudnn$convBiasActivationForward\", metadata={op_type=\"Conv2D\" op_name=\"functional_1_1/conv2_block1_2_conv_1/convolution\" source_file=\"/home/gustavo/.local/lib/python3.10/site-packages/tensorflow/python/framework/ops.py\" source_line=1200}, backend_config={\"operation_queue_id\":\"0\",\"wait_on_operation_queues\":[],\"cudnn_conv_backend_config\":{\"conv_result_scale\":1,\"activation_mode\":\"kNone\",\"side_input_scale\":0,\"leakyrelu_alpha\":0},\"force_earliest_schedule\":false}\n",
            "2025-07-11 19:44:50.121618: I external/local_xla/xla/service/gpu/autotuning/conv_algorithm_picker.cc:549] Omitted potentially buggy algorithm eng14{k25=0} for conv %cudnn-conv-bias-activation.172 = (f32[6,128,28,28]{3,2,1,0}, u8[0]{0}) custom-call(f32[6,128,28,28]{3,2,1,0} %bitcast.5239, f32[128,128,3,3]{3,2,1,0} %bitcast.5246, f32[128]{0} %bitcast.5248), window={size=3x3 pad=1_1x1_1}, dim_labels=bf01_oi01->bf01, custom_call_target=\"__cudnn$convBiasActivationForward\", metadata={op_type=\"Conv2D\" op_name=\"functional_1_1/conv3_block1_2_conv_1/convolution\" source_file=\"/home/gustavo/.local/lib/python3.10/site-packages/tensorflow/python/framework/ops.py\" source_line=1200}, backend_config={\"operation_queue_id\":\"0\",\"wait_on_operation_queues\":[],\"cudnn_conv_backend_config\":{\"conv_result_scale\":1,\"activation_mode\":\"kNone\",\"side_input_scale\":0,\"leakyrelu_alpha\":0},\"force_earliest_schedule\":false}\n",
            "2025-07-11 19:44:50.382646: I external/local_xla/xla/service/gpu/autotuning/conv_algorithm_picker.cc:549] Omitted potentially buggy algorithm eng14{k25=0} for conv %cudnn-conv-bias-activation.185 = (f32[6,256,14,14]{3,2,1,0}, u8[0]{0}) custom-call(f32[6,256,14,14]{3,2,1,0} %bitcast.5767, f32[256,256,3,3]{3,2,1,0} %bitcast.5774, f32[256]{0} %bitcast.5776), window={size=3x3 pad=1_1x1_1}, dim_labels=bf01_oi01->bf01, custom_call_target=\"__cudnn$convBiasActivationForward\", metadata={op_type=\"Conv2D\" op_name=\"functional_1_1/conv4_block1_2_conv_1/convolution\" source_file=\"/home/gustavo/.local/lib/python3.10/site-packages/tensorflow/python/framework/ops.py\" source_line=1200}, backend_config={\"operation_queue_id\":\"0\",\"wait_on_operation_queues\":[],\"cudnn_conv_backend_config\":{\"conv_result_scale\":1,\"activation_mode\":\"kNone\",\"side_input_scale\":0,\"leakyrelu_alpha\":0},\"force_earliest_schedule\":false}\n",
            "2025-07-11 19:44:50.647298: I external/local_xla/xla/service/gpu/autotuning/conv_algorithm_picker.cc:549] Omitted potentially buggy algorithm eng14{k25=0} for conv %cudnn-conv-bias-activation.204 = (f32[6,512,7,7]{3,2,1,0}, u8[0]{0}) custom-call(f32[6,512,7,7]{3,2,1,0} %bitcast.6541, f32[512,512,3,3]{3,2,1,0} %bitcast.6548, f32[512]{0} %bitcast.6550), window={size=3x3 pad=1_1x1_1}, dim_labels=bf01_oi01->bf01, custom_call_target=\"__cudnn$convBiasActivationForward\", metadata={op_type=\"Conv2D\" op_name=\"functional_1_1/conv5_block1_2_conv_1/convolution\" source_file=\"/home/gustavo/.local/lib/python3.10/site-packages/tensorflow/python/framework/ops.py\" source_line=1200}, backend_config={\"operation_queue_id\":\"0\",\"wait_on_operation_queues\":[],\"cudnn_conv_backend_config\":{\"conv_result_scale\":1,\"activation_mode\":\"kNone\",\"side_input_scale\":0,\"leakyrelu_alpha\":0},\"force_earliest_schedule\":false}\n",
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m43/43\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 655ms/step - accuracy: 0.5540 - loss: 1.5550 - val_accuracy: 0.5000 - val_loss: 1.3085 - learning_rate: 1.0000e-04\n",
            "Epoch 2/40\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 251ms/step - accuracy: 0.6363 - loss: 1.4391 - val_accuracy: 0.5000 - val_loss: 1.2867 - learning_rate: 1.0000e-04\n",
            "Epoch 3/40\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 303ms/step - accuracy: 0.6293 - loss: 1.3758 - val_accuracy: 0.5000 - val_loss: 1.2613 - learning_rate: 1.0000e-04\n",
            "Epoch 4/40\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 236ms/step - accuracy: 0.6693 - loss: 1.2419"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m43/43\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 279ms/step - accuracy: 0.6695 - loss: 1.2421 - val_accuracy: 0.5068 - val_loss: 1.2266 - learning_rate: 1.0000e-04\n",
            "Epoch 5/40\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 238ms/step - accuracy: 0.6903 - loss: 1.2337"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m43/43\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 271ms/step - accuracy: 0.6901 - loss: 1.2338 - val_accuracy: 0.7517 - val_loss: 1.1904 - learning_rate: 1.0000e-04\n",
            "Epoch 6/40\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 301ms/step - accuracy: 0.6784 - loss: 1.2097 - val_accuracy: 0.7279 - val_loss: 1.1537 - learning_rate: 1.0000e-04\n",
            "Epoch 7/40\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 252ms/step - accuracy: 0.6745 - loss: 1.2166 - val_accuracy: 0.7109 - val_loss: 1.1264 - learning_rate: 1.0000e-04\n",
            "Epoch 8/40\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 241ms/step - accuracy: 0.7075 - loss: 1.1212"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m43/43\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 318ms/step - accuracy: 0.7070 - loss: 1.1218 - val_accuracy: 0.8061 - val_loss: 1.0943 - learning_rate: 1.0000e-04\n",
            "Epoch 9/40\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 255ms/step - accuracy: 0.6940 - loss: 1.1686 - val_accuracy: 0.7075 - val_loss: 1.0976 - learning_rate: 1.0000e-04\n",
            "Epoch 10/40\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 251ms/step - accuracy: 0.6801 - loss: 1.1687"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m43/43\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 290ms/step - accuracy: 0.6803 - loss: 1.1685 - val_accuracy: 0.8197 - val_loss: 1.0214 - learning_rate: 1.0000e-04\n",
            "Epoch 11/40\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 297ms/step - accuracy: 0.7100 - loss: 1.1356 - val_accuracy: 0.7279 - val_loss: 1.0589 - learning_rate: 1.0000e-04\n",
            "Epoch 12/40\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 257ms/step - accuracy: 0.7362 - loss: 1.1379 - val_accuracy: 0.7381 - val_loss: 1.0460 - learning_rate: 1.0000e-04\n",
            "Epoch 13/40\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 253ms/step - accuracy: 0.7072 - loss: 1.0994 - val_accuracy: 0.6905 - val_loss: 1.1112 - learning_rate: 1.0000e-04\n",
            "Epoch 14/40\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 299ms/step - accuracy: 0.7051 - loss: 1.1441 - val_accuracy: 0.6429 - val_loss: 1.2251 - learning_rate: 1.0000e-04\n",
            "Epoch 15/40\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 257ms/step - accuracy: 0.7319 - loss: 1.0751 - val_accuracy: 0.7483 - val_loss: 1.0271 - learning_rate: 1.0000e-04\n",
            "Epoch 16/40\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 264ms/step - accuracy: 0.7116 - loss: 1.0885 - val_accuracy: 0.6395 - val_loss: 1.3369 - learning_rate: 1.0000e-04\n",
            "Epoch 17/40\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 316ms/step - accuracy: 0.7202 - loss: 1.1162 - val_accuracy: 0.7109 - val_loss: 1.1432 - learning_rate: 3.0000e-05\n",
            "Epoch 18/40\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 250ms/step - accuracy: 0.7544 - loss: 1.0715 - val_accuracy: 0.6837 - val_loss: 1.1758 - learning_rate: 3.0000e-05\n",
            "Epoch 19/40\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 257ms/step - accuracy: 0.7330 - loss: 1.0691 - val_accuracy: 0.6497 - val_loss: 1.2432 - learning_rate: 3.0000e-05\n",
            "Epoch 20/40\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 303ms/step - accuracy: 0.7644 - loss: 1.0406 - val_accuracy: 0.6054 - val_loss: 1.5584 - learning_rate: 3.0000e-05\n",
            "Epoch 21/40\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 248ms/step - accuracy: 0.7405 - loss: 1.0626 - val_accuracy: 0.6020 - val_loss: 1.5778 - learning_rate: 3.0000e-05\n",
            "Epoch 22/40\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 254ms/step - accuracy: 0.7681 - loss: 1.0231 - val_accuracy: 0.6293 - val_loss: 1.4741 - learning_rate: 3.0000e-05\n",
            "Epoch 23/40\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 296ms/step - accuracy: 0.7518 - loss: 1.0276 - val_accuracy: 0.6463 - val_loss: 1.2240 - learning_rate: 9.0000e-06\n",
            "Epoch 24/40\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 256ms/step - accuracy: 0.7477 - loss: 1.0198 - val_accuracy: 0.6735 - val_loss: 1.1520 - learning_rate: 9.0000e-06\n",
            "Epoch 25/40\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 260ms/step - accuracy: 0.7454 - loss: 1.0350 - val_accuracy: 0.7041 - val_loss: 1.1068 - learning_rate: 9.0000e-06\n",
            "‚úÖ Potato conclu√≠do! Melhor accuracy: 0.8197\n",
            "\n",
            "üöÄ Treinando Pepper...\n",
            "Epoch 1/40\n",
            "\u001b[1m 7/61\u001b[0m \u001b[32m‚îÅ‚îÅ\u001b[0m\u001b[37m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[1m11s\u001b[0m 212ms/step - accuracy: 0.6706 - loss: 1.8356"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-07-11 19:49:55.340052: I external/local_xla/xla/service/gpu/autotuning/conv_algorithm_picker.cc:549] Omitted potentially buggy algorithm eng14{k25=0} for conv %cudnn-conv-bias-activation.165 = (f32[17,64,56,56]{3,2,1,0}, u8[0]{0}) custom-call(f32[17,64,56,56]{3,2,1,0} %bitcast.14083, f32[64,64,3,3]{3,2,1,0} %bitcast.14090, f32[64]{0} %bitcast.14092), window={size=3x3 pad=1_1x1_1}, dim_labels=bf01_oi01->bf01, custom_call_target=\"__cudnn$convBiasActivationForward\", metadata={op_type=\"Conv2D\" op_name=\"functional_2_1/conv2_block1_2_conv_1/convolution\" source_file=\"/home/gustavo/.local/lib/python3.10/site-packages/tensorflow/python/framework/ops.py\" source_line=1200}, backend_config={\"operation_queue_id\":\"0\",\"wait_on_operation_queues\":[],\"cudnn_conv_backend_config\":{\"conv_result_scale\":1,\"activation_mode\":\"kNone\",\"side_input_scale\":0,\"leakyrelu_alpha\":0},\"force_earliest_schedule\":false}\n",
            "2025-07-11 19:49:55.688490: I external/local_xla/xla/service/gpu/autotuning/conv_algorithm_picker.cc:549] Omitted potentially buggy algorithm eng14{k25=0} for conv %cudnn-conv-bias-activation.175 = (f32[17,128,28,28]{3,2,1,0}, u8[0]{0}) custom-call(f32[17,128,28,28]{3,2,1,0} %bitcast.14488, f32[128,128,3,3]{3,2,1,0} %bitcast.14495, f32[128]{0} %bitcast.14497), window={size=3x3 pad=1_1x1_1}, dim_labels=bf01_oi01->bf01, custom_call_target=\"__cudnn$convBiasActivationForward\", metadata={op_type=\"Conv2D\" op_name=\"functional_2_1/conv3_block1_2_conv_1/convolution\" source_file=\"/home/gustavo/.local/lib/python3.10/site-packages/tensorflow/python/framework/ops.py\" source_line=1200}, backend_config={\"operation_queue_id\":\"0\",\"wait_on_operation_queues\":[],\"cudnn_conv_backend_config\":{\"conv_result_scale\":1,\"activation_mode\":\"kNone\",\"side_input_scale\":0,\"leakyrelu_alpha\":0},\"force_earliest_schedule\":false}\n",
            "2025-07-11 19:49:56.111087: I external/local_xla/xla/service/gpu/autotuning/conv_algorithm_picker.cc:549] Omitted potentially buggy algorithm eng14{k25=0} for conv %cudnn-conv-bias-activation.188 = (f32[17,256,14,14]{3,2,1,0}, u8[0]{0}) custom-call(f32[17,256,14,14]{3,2,1,0} %bitcast.15016, f32[256,256,3,3]{3,2,1,0} %bitcast.15023, f32[256]{0} %bitcast.15025), window={size=3x3 pad=1_1x1_1}, dim_labels=bf01_oi01->bf01, custom_call_target=\"__cudnn$convBiasActivationForward\", metadata={op_type=\"Conv2D\" op_name=\"functional_2_1/conv4_block1_2_conv_1/convolution\" source_file=\"/home/gustavo/.local/lib/python3.10/site-packages/tensorflow/python/framework/ops.py\" source_line=1200}, backend_config={\"operation_queue_id\":\"0\",\"wait_on_operation_queues\":[],\"cudnn_conv_backend_config\":{\"conv_result_scale\":1,\"activation_mode\":\"kNone\",\"side_input_scale\":0,\"leakyrelu_alpha\":0},\"force_earliest_schedule\":false}\n",
            "2025-07-11 19:49:56.492112: I external/local_xla/xla/service/gpu/autotuning/conv_algorithm_picker.cc:549] Omitted potentially buggy algorithm eng14{k25=0} for conv %cudnn-conv-bias-activation.207 = (f32[17,512,7,7]{3,2,1,0}, u8[0]{0}) custom-call(f32[17,512,7,7]{3,2,1,0} %bitcast.15790, f32[512,512,3,3]{3,2,1,0} %bitcast.15797, f32[512]{0} %bitcast.15799), window={size=3x3 pad=1_1x1_1}, dim_labels=bf01_oi01->bf01, custom_call_target=\"__cudnn$convBiasActivationForward\", metadata={op_type=\"Conv2D\" op_name=\"functional_2_1/conv5_block1_2_conv_1/convolution\" source_file=\"/home/gustavo/.local/lib/python3.10/site-packages/tensorflow/python/framework/ops.py\" source_line=1200}, backend_config={\"operation_queue_id\":\"0\",\"wait_on_operation_queues\":[],\"cudnn_conv_backend_config\":{\"conv_result_scale\":1,\"activation_mode\":\"kNone\",\"side_input_scale\":0,\"leakyrelu_alpha\":0},\"force_earliest_schedule\":false}\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m61/61\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 356ms/step - accuracy: 0.6932 - loss: 1.5552"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-07-11 19:50:19.735899: I external/local_xla/xla/service/gpu/autotuning/conv_algorithm_picker.cc:549] Omitted potentially buggy algorithm eng14{k25=0} for conv %cudnn-conv-bias-activation.162 = (f32[30,64,56,56]{3,2,1,0}, u8[0]{0}) custom-call(f32[30,64,56,56]{3,2,1,0} %bitcast.4834, f32[64,64,3,3]{3,2,1,0} %bitcast.4841, f32[64]{0} %bitcast.4843), window={size=3x3 pad=1_1x1_1}, dim_labels=bf01_oi01->bf01, custom_call_target=\"__cudnn$convBiasActivationForward\", metadata={op_type=\"Conv2D\" op_name=\"functional_2_1/conv2_block1_2_conv_1/convolution\" source_file=\"/home/gustavo/.local/lib/python3.10/site-packages/tensorflow/python/framework/ops.py\" source_line=1200}, backend_config={\"operation_queue_id\":\"0\",\"wait_on_operation_queues\":[],\"cudnn_conv_backend_config\":{\"conv_result_scale\":1,\"activation_mode\":\"kNone\",\"side_input_scale\":0,\"leakyrelu_alpha\":0},\"force_earliest_schedule\":false}\n",
            "2025-07-11 19:50:20.189045: I external/local_xla/xla/service/gpu/autotuning/conv_algorithm_picker.cc:549] Omitted potentially buggy algorithm eng14{k25=0} for conv %cudnn-conv-bias-activation.172 = (f32[30,128,28,28]{3,2,1,0}, u8[0]{0}) custom-call(f32[30,128,28,28]{3,2,1,0} %bitcast.5239, f32[128,128,3,3]{3,2,1,0} %bitcast.5246, f32[128]{0} %bitcast.5248), window={size=3x3 pad=1_1x1_1}, dim_labels=bf01_oi01->bf01, custom_call_target=\"__cudnn$convBiasActivationForward\", metadata={op_type=\"Conv2D\" op_name=\"functional_2_1/conv3_block1_2_conv_1/convolution\" source_file=\"/home/gustavo/.local/lib/python3.10/site-packages/tensorflow/python/framework/ops.py\" source_line=1200}, backend_config={\"operation_queue_id\":\"0\",\"wait_on_operation_queues\":[],\"cudnn_conv_backend_config\":{\"conv_result_scale\":1,\"activation_mode\":\"kNone\",\"side_input_scale\":0,\"leakyrelu_alpha\":0},\"force_earliest_schedule\":false}\n",
            "2025-07-11 19:50:20.698830: I external/local_xla/xla/service/gpu/autotuning/conv_algorithm_picker.cc:549] Omitted potentially buggy algorithm eng14{k25=0} for conv %cudnn-conv-bias-activation.185 = (f32[30,256,14,14]{3,2,1,0}, u8[0]{0}) custom-call(f32[30,256,14,14]{3,2,1,0} %bitcast.5767, f32[256,256,3,3]{3,2,1,0} %bitcast.5774, f32[256]{0} %bitcast.5776), window={size=3x3 pad=1_1x1_1}, dim_labels=bf01_oi01->bf01, custom_call_target=\"__cudnn$convBiasActivationForward\", metadata={op_type=\"Conv2D\" op_name=\"functional_2_1/conv4_block1_2_conv_1/convolution\" source_file=\"/home/gustavo/.local/lib/python3.10/site-packages/tensorflow/python/framework/ops.py\" source_line=1200}, backend_config={\"operation_queue_id\":\"0\",\"wait_on_operation_queues\":[],\"cudnn_conv_backend_config\":{\"conv_result_scale\":1,\"activation_mode\":\"kNone\",\"side_input_scale\":0,\"leakyrelu_alpha\":0},\"force_earliest_schedule\":false}\n",
            "2025-07-11 19:50:21.180688: I external/local_xla/xla/service/gpu/autotuning/conv_algorithm_picker.cc:549] Omitted potentially buggy algorithm eng14{k25=0} for conv %cudnn-conv-bias-activation.204 = (f32[30,512,7,7]{3,2,1,0}, u8[0]{0}) custom-call(f32[30,512,7,7]{3,2,1,0} %bitcast.6541, f32[512,512,3,3]{3,2,1,0} %bitcast.6548, f32[512]{0} %bitcast.6550), window={size=3x3 pad=1_1x1_1}, dim_labels=bf01_oi01->bf01, custom_call_target=\"__cudnn$convBiasActivationForward\", metadata={op_type=\"Conv2D\" op_name=\"functional_2_1/conv5_block1_2_conv_1/convolution\" source_file=\"/home/gustavo/.local/lib/python3.10/site-packages/tensorflow/python/framework/ops.py\" source_line=1200}, backend_config={\"operation_queue_id\":\"0\",\"wait_on_operation_queues\":[],\"cudnn_conv_backend_config\":{\"conv_result_scale\":1,\"activation_mode\":\"kNone\",\"side_input_scale\":0,\"leakyrelu_alpha\":0},\"force_earliest_schedule\":false}\n",
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m61/61\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 544ms/step - accuracy: 0.6935 - loss: 1.5529 - val_accuracy: 0.2633 - val_loss: 1.2732 - learning_rate: 1.0000e-04\n",
            "Epoch 2/40\n",
            "\u001b[1m61/61\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 244ms/step - accuracy: 0.7162 - loss: 1.2879 - val_accuracy: 0.2512 - val_loss: 1.4593 - learning_rate: 1.0000e-04\n",
            "Epoch 3/40\n",
            "\u001b[1m61/61\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 283ms/step - accuracy: 0.7259 - loss: 1.2927 - val_accuracy: 0.2560 - val_loss: 1.4249 - learning_rate: 1.0000e-04\n",
            "Epoch 4/40\n",
            "\u001b[1m61/61\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 232ms/step - accuracy: 0.7532 - loss: 1.1843"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m61/61\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 262ms/step - accuracy: 0.7528 - loss: 1.1851 - val_accuracy: 0.4227 - val_loss: 1.3053 - learning_rate: 1.0000e-04\n",
            "Epoch 5/40\n",
            "\u001b[1m61/61\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 269ms/step - accuracy: 0.7340 - loss: 1.2007"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m61/61\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 296ms/step - accuracy: 0.7342 - loss: 1.2003 - val_accuracy: 0.7826 - val_loss: 1.1229 - learning_rate: 1.0000e-04\n",
            "Epoch 6/40\n",
            "\u001b[1m61/61\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 236ms/step - accuracy: 0.7347 - loss: 1.1327 - val_accuracy: 0.4879 - val_loss: 1.2646 - learning_rate: 1.0000e-04\n",
            "Epoch 7/40\n",
            "\u001b[1m61/61\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 271ms/step - accuracy: 0.7173 - loss: 1.2007"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m61/61\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 302ms/step - accuracy: 0.7176 - loss: 1.1996 - val_accuracy: 0.8213 - val_loss: 0.9300 - learning_rate: 1.0000e-04\n",
            "Epoch 8/40\n",
            "\u001b[1m61/61\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 234ms/step - accuracy: 0.7521 - loss: 1.0871"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m61/61\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 261ms/step - accuracy: 0.7522 - loss: 1.0868 - val_accuracy: 0.8285 - val_loss: 0.9615 - learning_rate: 1.0000e-04\n",
            "Epoch 9/40\n",
            "\u001b[1m61/61\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 283ms/step - accuracy: 0.7712 - loss: 1.0723 - val_accuracy: 0.7560 - val_loss: 1.1094 - learning_rate: 1.0000e-04\n",
            "Epoch 10/40\n",
            "\u001b[1m61/61\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 218ms/step - accuracy: 0.7840 - loss: 1.0564"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m61/61\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 247ms/step - accuracy: 0.7841 - loss: 1.0559 - val_accuracy: 0.8696 - val_loss: 0.8413 - learning_rate: 1.0000e-04\n",
            "Epoch 11/40\n",
            "\u001b[1m61/61\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 290ms/step - accuracy: 0.7750 - loss: 1.0712 - val_accuracy: 0.8527 - val_loss: 0.9067 - learning_rate: 1.0000e-04\n",
            "Epoch 12/40\n",
            "\u001b[1m61/61\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 248ms/step - accuracy: 0.7660 - loss: 1.0605 - val_accuracy: 0.8671 - val_loss: 0.8456 - learning_rate: 1.0000e-04\n",
            "Epoch 13/40\n",
            "\u001b[1m61/61\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 275ms/step - accuracy: 0.7778 - loss: 1.0495 - val_accuracy: 0.6401 - val_loss: 1.2316 - learning_rate: 1.0000e-04\n",
            "Epoch 14/40\n",
            "\u001b[1m61/61\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 238ms/step - accuracy: 0.7907 - loss: 1.0169 - val_accuracy: 0.3841 - val_loss: 1.5749 - learning_rate: 1.0000e-04\n",
            "Epoch 15/40\n",
            "\u001b[1m61/61\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 285ms/step - accuracy: 0.7726 - loss: 1.0038 - val_accuracy: 0.5242 - val_loss: 1.3402 - learning_rate: 1.0000e-04\n",
            "Epoch 16/40\n",
            "\u001b[1m61/61\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 249ms/step - accuracy: 0.7870 - loss: 1.0059 - val_accuracy: 0.8213 - val_loss: 0.9230 - learning_rate: 1.0000e-04\n",
            "Epoch 17/40\n",
            "\u001b[1m61/61\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 277ms/step - accuracy: 0.7927 - loss: 0.9939 - val_accuracy: 0.8478 - val_loss: 0.8713 - learning_rate: 3.0000e-05\n",
            "Epoch 18/40\n",
            "\u001b[1m61/61\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 221ms/step - accuracy: 0.8265 - loss: 0.9443"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m61/61\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 250ms/step - accuracy: 0.8264 - loss: 0.9442 - val_accuracy: 0.8816 - val_loss: 0.8086 - learning_rate: 3.0000e-05\n",
            "Epoch 19/40\n",
            "\u001b[1m61/61\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 289ms/step - accuracy: 0.8362 - loss: 0.9472 - val_accuracy: 0.8744 - val_loss: 0.8000 - learning_rate: 3.0000e-05\n",
            "Epoch 20/40\n",
            "\u001b[1m61/61\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 235ms/step - accuracy: 0.8136 - loss: 0.9263"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m61/61\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 262ms/step - accuracy: 0.8137 - loss: 0.9263 - val_accuracy: 0.8913 - val_loss: 0.7673 - learning_rate: 3.0000e-05\n",
            "Epoch 21/40\n",
            "\u001b[1m61/61\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 252ms/step - accuracy: 0.8268 - loss: 0.8958 - val_accuracy: 0.8792 - val_loss: 0.7987 - learning_rate: 3.0000e-05\n",
            "Epoch 22/40\n",
            "\u001b[1m61/61\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 264ms/step - accuracy: 0.8131 - loss: 0.9080 - val_accuracy: 0.8792 - val_loss: 0.7812 - learning_rate: 3.0000e-05\n",
            "Epoch 23/40\n",
            "\u001b[1m61/61\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 255ms/step - accuracy: 0.8320 - loss: 0.9341 - val_accuracy: 0.8841 - val_loss: 0.7659 - learning_rate: 3.0000e-05\n",
            "Epoch 24/40\n",
            "\u001b[1m61/61\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 269ms/step - accuracy: 0.8471 - loss: 0.8821 - val_accuracy: 0.8430 - val_loss: 0.9417 - learning_rate: 3.0000e-05\n",
            "Epoch 25/40\n",
            "\u001b[1m61/61\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 243ms/step - accuracy: 0.8159 - loss: 0.9253 - val_accuracy: 0.8140 - val_loss: 0.9364 - learning_rate: 3.0000e-05\n",
            "Epoch 26/40\n",
            "\u001b[1m61/61\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 296ms/step - accuracy: 0.8475 - loss: 0.8785 - val_accuracy: 0.7874 - val_loss: 0.9534 - learning_rate: 3.0000e-05\n",
            "Epoch 27/40\n",
            "\u001b[1m61/61\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 233ms/step - accuracy: 0.8542 - loss: 0.8545"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m61/61\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 261ms/step - accuracy: 0.8543 - loss: 0.8545 - val_accuracy: 0.8961 - val_loss: 0.7470 - learning_rate: 3.0000e-05\n",
            "Epoch 28/40\n",
            "\u001b[1m61/61\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 278ms/step - accuracy: 0.8481 - loss: 0.8778 - val_accuracy: 0.8865 - val_loss: 0.7901 - learning_rate: 3.0000e-05\n",
            "Epoch 29/40\n",
            "\u001b[1m61/61\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 240ms/step - accuracy: 0.8458 - loss: 0.8773 - val_accuracy: 0.8937 - val_loss: 0.7598 - learning_rate: 3.0000e-05\n",
            "Epoch 30/40\n",
            "\u001b[1m61/61\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 276ms/step - accuracy: 0.8359 - loss: 0.8785"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m61/61\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 304ms/step - accuracy: 0.8358 - loss: 0.8786 - val_accuracy: 0.9034 - val_loss: 0.7314 - learning_rate: 3.0000e-05\n",
            "Epoch 31/40\n",
            "\u001b[1m61/61\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 240ms/step - accuracy: 0.8318 - loss: 0.8736 - val_accuracy: 0.8478 - val_loss: 0.8290 - learning_rate: 3.0000e-05\n",
            "Epoch 32/40\n",
            "\u001b[1m61/61\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 274ms/step - accuracy: 0.8572 - loss: 0.8814 - val_accuracy: 0.8551 - val_loss: 0.8298 - learning_rate: 3.0000e-05\n",
            "Epoch 33/40\n",
            "\u001b[1m61/61\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 245ms/step - accuracy: 0.8360 - loss: 0.8950 - val_accuracy: 0.9034 - val_loss: 0.7355 - learning_rate: 3.0000e-05\n",
            "Epoch 34/40\n",
            "\u001b[1m61/61\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 272ms/step - accuracy: 0.8653 - loss: 0.8674 - val_accuracy: 0.8792 - val_loss: 0.7492 - learning_rate: 3.0000e-05\n",
            "Epoch 35/40\n",
            "\u001b[1m61/61\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 246ms/step - accuracy: 0.8341 - loss: 0.9362 - val_accuracy: 0.8865 - val_loss: 0.7709 - learning_rate: 3.0000e-05\n",
            "Epoch 36/40\n",
            "\u001b[1m61/61\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 291ms/step - accuracy: 0.8525 - loss: 0.8629 - val_accuracy: 0.9010 - val_loss: 0.7262 - learning_rate: 3.0000e-05\n",
            "Epoch 37/40\n",
            "\u001b[1m61/61\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 234ms/step - accuracy: 0.8408 - loss: 0.8753"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m61/61\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 265ms/step - accuracy: 0.8410 - loss: 0.8753 - val_accuracy: 0.9082 - val_loss: 0.7195 - learning_rate: 3.0000e-05\n",
            "Epoch 38/40\n",
            "\u001b[1m61/61\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 272ms/step - accuracy: 0.8469 - loss: 0.8779 - val_accuracy: 0.8961 - val_loss: 0.7282 - learning_rate: 3.0000e-05\n",
            "Epoch 39/40\n",
            "\u001b[1m61/61\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 248ms/step - accuracy: 0.8491 - loss: 0.8627 - val_accuracy: 0.5386 - val_loss: 1.2674 - learning_rate: 3.0000e-05\n",
            "Epoch 40/40\n",
            "\u001b[1m61/61\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 270ms/step - accuracy: 0.8477 - loss: 0.8549 - val_accuracy: 0.5580 - val_loss: 1.3299 - learning_rate: 3.0000e-05\n",
            "‚úÖ Pepper conclu√≠do! Melhor accuracy: 0.9082\n",
            "\n",
            "üéØ TODOS OS MODELOS TREINADOS COM DADOS BALANCEADOS!\n"
          ]
        }
      ],
      "source": [
        "# 5. TREINAMENTO OTIMIZADO COM DADOS BALANCEADOS E CLASS WEIGHTS\n",
        "def treinar_modelo_binario(modelo, especie, train_gen, val_gen, class_weights):\n",
        "    \"\"\"Treina modelo de classifica√ß√£o bin√°ria com class weights\"\"\"\n",
        "    print(f\"\\nüöÄ Treinando {especie}...\")\n",
        "    \n",
        "    # Compila√ß√£o\n",
        "    modelo.compile(\n",
        "        optimizer=Adam(learning_rate=0.0001),\n",
        "        loss='binary_crossentropy',\n",
        "        metrics=['accuracy']\n",
        "    )\n",
        "    \n",
        "    # Callback\n",
        "    callbacks = [\n",
        "        EarlyStopping(\n",
        "            monitor='val_accuracy',\n",
        "            patience=15,\n",
        "            restore_best_weights=True,\n",
        "            min_delta=0.001\n",
        "        ),\n",
        "        ReduceLROnPlateau(\n",
        "            monitor='val_loss',\n",
        "            factor=0.3,\n",
        "            patience=6,\n",
        "            min_lr=1e-8\n",
        "        ),\n",
        "        ModelCheckpoint(\n",
        "            filepath=f'modelos_salvos/especialistas/modelo_binario_{especie.lower()}.h5',\n",
        "            monitor='val_accuracy',\n",
        "            save_best_only=True\n",
        "        )\n",
        "    ]\n",
        "    \n",
        "    # Treinamento\n",
        "    history = modelo.fit(\n",
        "        train_gen,\n",
        "        epochs=40,\n",
        "        validation_data=val_gen,\n",
        "        class_weight=class_weights,\n",
        "        callbacks=callbacks,\n",
        "        verbose=1\n",
        "    )\n",
        "    \n",
        "    final_accuracy = max(history.history['val_accuracy'])\n",
        "    print(f\"‚úÖ {especie} conclu√≠do! Melhor accuracy: {final_accuracy:.4f}\")\n",
        "    \n",
        "    return history\n",
        "\n",
        "# Treinar todos os modelos com dados balanceados\n",
        "os.makedirs('modelos_salvos', exist_ok=True)\n",
        "os.makedirs('modelos_salvos/especialistas', exist_ok=True)\n",
        "\n",
        "print(\"=== TREINAMENTO DOS MODELOS COM DADOS BALANCEADOS ===\")\n",
        "print(\"üéØ EXPECTATIVAS DE MELHORIA:\")\n",
        "print(\"   üö® Potato: 0% ‚Üí 40-60% recall Healthy\")\n",
        "print(\"   ‚ö†Ô∏è Tomato: 56% ‚Üí 70-80% recall Healthy\") \n",
        "print(\"   ‚úÖ Pepper: 80% ‚Üí 85-90% recall Healthy\")\n",
        "print()\n",
        "\n",
        "# Treinar modelos com verifica√ß√£o de dados balanceados\n",
        "histories = {}\n",
        "\n",
        "if cw_tomato is not None:\n",
        "    histories['tomato'] = treinar_modelo_binario(modelo_tomato, 'Tomato', train_gen_tomato, val_gen_tomato, cw_tomato)\n",
        "\n",
        "if cw_potato is not None:\n",
        "    histories['potato'] = treinar_modelo_binario(modelo_potato, 'Potato', train_gen_potato, val_gen_potato, cw_potato)\n",
        "\n",
        "if cw_pepper is not None:\n",
        "    histories['pepper'] = treinar_modelo_binario(modelo_pepper, 'Pepper', train_gen_pepper, val_gen_pepper, cw_pepper)\n",
        "\n",
        "print(\"\\nüéØ TODOS OS MODELOS TREINADOS COM DADOS BALANCEADOS!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=== AVALIA√á√ÉO FINAL COM THRESHOLD INTELIGENTE ===\n",
            "üß† Sistema inteligente priorizando detec√ß√£o de plantas doentes:\n",
            "   - Thresholds din√¢micos baseados na confian√ßa da predi√ß√£o\n",
            "   - Tomato: Base 0.55 (ajuste 0.39-0.63)\n",
            "   - Potato: Base 0.45 (ajuste 0.32-0.52)\n",
            "   - Pepper: Base 0.50 (ajuste 0.35-0.58)\n",
            "\n",
            "üìä Avaliando Tomato com threshold inteligente (foco em Unhealthy)...\n",
            "   üß† Aplicando threshold inteligente para Tomato...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-07-11 20:01:07.365390: I external/local_xla/xla/service/gpu/autotuning/conv_algorithm_picker.cc:549] Omitted potentially buggy algorithm eng14{k25=0} for conv %cudnn-conv-bias-activation.162 = (f32[12,64,56,56]{3,2,1,0}, u8[0]{0}) custom-call(f32[12,64,56,56]{3,2,1,0} %bitcast.4595, f32[64,64,3,3]{3,2,1,0} %bitcast.4602, f32[64]{0} %bitcast.4604), window={size=3x3 pad=1_1x1_1}, dim_labels=bf01_oi01->bf01, custom_call_target=\"__cudnn$convBiasActivationForward\", metadata={op_type=\"Conv2D\" op_name=\"functional_1/conv2_block1_2_conv_1/convolution\" source_file=\"/home/gustavo/.local/lib/python3.10/site-packages/tensorflow/python/framework/ops.py\" source_line=1200}, backend_config={\"operation_queue_id\":\"0\",\"wait_on_operation_queues\":[],\"cudnn_conv_backend_config\":{\"conv_result_scale\":1,\"activation_mode\":\"kNone\",\"side_input_scale\":0,\"leakyrelu_alpha\":0},\"force_earliest_schedule\":false}\n",
            "2025-07-11 20:01:07.654231: I external/local_xla/xla/service/gpu/autotuning/conv_algorithm_picker.cc:549] Omitted potentially buggy algorithm eng14{k25=0} for conv %cudnn-conv-bias-activation.172 = (f32[12,128,28,28]{3,2,1,0}, u8[0]{0}) custom-call(f32[12,128,28,28]{3,2,1,0} %bitcast.5000, f32[128,128,3,3]{3,2,1,0} %bitcast.5007, f32[128]{0} %bitcast.5009), window={size=3x3 pad=1_1x1_1}, dim_labels=bf01_oi01->bf01, custom_call_target=\"__cudnn$convBiasActivationForward\", metadata={op_type=\"Conv2D\" op_name=\"functional_1/conv3_block1_2_conv_1/convolution\" source_file=\"/home/gustavo/.local/lib/python3.10/site-packages/tensorflow/python/framework/ops.py\" source_line=1200}, backend_config={\"operation_queue_id\":\"0\",\"wait_on_operation_queues\":[],\"cudnn_conv_backend_config\":{\"conv_result_scale\":1,\"activation_mode\":\"kNone\",\"side_input_scale\":0,\"leakyrelu_alpha\":0},\"force_earliest_schedule\":false}\n",
            "2025-07-11 20:01:07.977585: I external/local_xla/xla/service/gpu/autotuning/conv_algorithm_picker.cc:549] Omitted potentially buggy algorithm eng14{k25=0} for conv %cudnn-conv-bias-activation.185 = (f32[12,256,14,14]{3,2,1,0}, u8[0]{0}) custom-call(f32[12,256,14,14]{3,2,1,0} %bitcast.5528, f32[256,256,3,3]{3,2,1,0} %bitcast.5535, f32[256]{0} %bitcast.5537), window={size=3x3 pad=1_1x1_1}, dim_labels=bf01_oi01->bf01, custom_call_target=\"__cudnn$convBiasActivationForward\", metadata={op_type=\"Conv2D\" op_name=\"functional_1/conv4_block1_2_conv_1/convolution\" source_file=\"/home/gustavo/.local/lib/python3.10/site-packages/tensorflow/python/framework/ops.py\" source_line=1200}, backend_config={\"operation_queue_id\":\"0\",\"wait_on_operation_queues\":[],\"cudnn_conv_backend_config\":{\"conv_result_scale\":1,\"activation_mode\":\"kNone\",\"side_input_scale\":0,\"leakyrelu_alpha\":0},\"force_earliest_schedule\":false}\n",
            "2025-07-11 20:01:08.305813: I external/local_xla/xla/service/gpu/autotuning/conv_algorithm_picker.cc:549] Omitted potentially buggy algorithm eng14{k25=0} for conv %cudnn-conv-bias-activation.204 = (f32[12,512,7,7]{3,2,1,0}, u8[0]{0}) custom-call(f32[12,512,7,7]{3,2,1,0} %bitcast.6302, f32[512,512,3,3]{3,2,1,0} %bitcast.6309, f32[512]{0} %bitcast.6311), window={size=3x3 pad=1_1x1_1}, dim_labels=bf01_oi01->bf01, custom_call_target=\"__cudnn$convBiasActivationForward\", metadata={op_type=\"Conv2D\" op_name=\"functional_1/conv5_block1_2_conv_1/convolution\" source_file=\"/home/gustavo/.local/lib/python3.10/site-packages/tensorflow/python/framework/ops.py\" source_line=1200}, backend_config={\"operation_queue_id\":\"0\",\"wait_on_operation_queues\":[],\"cudnn_conv_backend_config\":{\"conv_result_scale\":1,\"activation_mode\":\"kNone\",\"side_input_scale\":0,\"leakyrelu_alpha\":0},\"force_earliest_schedule\":false}\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "   üéØ Threshold inteligente aplicado:\n",
            "      Faixa de thresholds: 0.385 - 0.632\n",
            "      Recall Healthy: 0.982\n",
            "      Recall Unhealthy: 0.950 ‚≠ê\n",
            "\n",
            "   üìä THRESHOLD PADR√ÉO (0.5):\n",
            "      Accuracy: 0.9624\n",
            "      Recall Healthy: 0.9824\n",
            "      Recall Unhealthy: 0.9538\n",
            "\n",
            "   üß† THRESHOLD INTELIGENTE:\n",
            "      Accuracy: 0.9598\n",
            "      AUC-ROC: 0.9953\n",
            "      Recall Healthy: 0.9824\n",
            "      Recall Unhealthy: 0.9500 ‚≠ê\n",
            "      Matriz: [[447,   8], [ 53, 1008]]\n",
            "\n",
            "   Classification Report (Threshold Inteligente):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "     Healthy       0.89      0.98      0.94       455\n",
            "   Unhealthy       0.99      0.95      0.97      1061\n",
            "\n",
            "    accuracy                           0.96      1516\n",
            "   macro avg       0.94      0.97      0.95      1516\n",
            "weighted avg       0.96      0.96      0.96      1516\n",
            "\n",
            "\n",
            "   üéØ MELHORIA NO RECALL UNHEALTHY: -0.004\n",
            "\n",
            "üìä Avaliando Potato com threshold inteligente (foco em Unhealthy)...\n",
            "   üß† Aplicando threshold inteligente para Potato...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/gustavo/.local/lib/python3.10/site-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
            "  self._warn_if_super_not_called()\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "   üéØ Threshold inteligente aplicado:\n",
            "      Faixa de thresholds: 0.315 - 0.517\n",
            "      Recall Healthy: 0.558\n",
            "      Recall Unhealthy: 0.912 ‚≠ê\n",
            "\n",
            "   üìä THRESHOLD PADR√ÉO (0.5):\n",
            "      Accuracy: 0.7687\n",
            "      Recall Healthy: 0.6871\n",
            "      Recall Unhealthy: 0.8503\n",
            "\n",
            "   üß† THRESHOLD INTELIGENTE:\n",
            "      Accuracy: 0.7347\n",
            "      AUC-ROC: 0.8674\n",
            "      Recall Healthy: 0.5578\n",
            "      Recall Unhealthy: 0.9116 ‚≠ê\n",
            "      Matriz: [[ 82,  65], [ 13, 134]]\n",
            "\n",
            "   Classification Report (Threshold Inteligente):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "     Healthy       0.86      0.56      0.68       147\n",
            "   Unhealthy       0.67      0.91      0.77       147\n",
            "\n",
            "    accuracy                           0.73       294\n",
            "   macro avg       0.77      0.73      0.73       294\n",
            "weighted avg       0.77      0.73      0.73       294\n",
            "\n",
            "\n",
            "   üéØ MELHORIA NO RECALL UNHEALTHY: +0.061\n",
            "\n",
            "üìä Avaliando Pepper com threshold inteligente (foco em Unhealthy)...\n",
            "   üß† Aplicando threshold inteligente para Pepper...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/gustavo/.local/lib/python3.10/site-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
            "  self._warn_if_super_not_called()\n",
            "2025-07-11 20:01:25.417173: I external/local_xla/xla/service/gpu/autotuning/conv_algorithm_picker.cc:549] Omitted potentially buggy algorithm eng14{k25=0} for conv %cudnn-conv-bias-activation.162 = (f32[31,64,56,56]{3,2,1,0}, u8[0]{0}) custom-call(f32[31,64,56,56]{3,2,1,0} %bitcast.4595, f32[64,64,3,3]{3,2,1,0} %bitcast.4602, f32[64]{0} %bitcast.4604), window={size=3x3 pad=1_1x1_1}, dim_labels=bf01_oi01->bf01, custom_call_target=\"__cudnn$convBiasActivationForward\", metadata={op_type=\"Conv2D\" op_name=\"functional_2_1/conv2_block1_2_conv_1/convolution\" source_file=\"/home/gustavo/.local/lib/python3.10/site-packages/tensorflow/python/framework/ops.py\" source_line=1200}, backend_config={\"operation_queue_id\":\"0\",\"wait_on_operation_queues\":[],\"cudnn_conv_backend_config\":{\"conv_result_scale\":1,\"activation_mode\":\"kNone\",\"side_input_scale\":0,\"leakyrelu_alpha\":0},\"force_earliest_schedule\":false}\n",
            "2025-07-11 20:01:25.873062: I external/local_xla/xla/service/gpu/autotuning/conv_algorithm_picker.cc:549] Omitted potentially buggy algorithm eng14{k25=0} for conv %cudnn-conv-bias-activation.172 = (f32[31,128,28,28]{3,2,1,0}, u8[0]{0}) custom-call(f32[31,128,28,28]{3,2,1,0} %bitcast.5000, f32[128,128,3,3]{3,2,1,0} %bitcast.5007, f32[128]{0} %bitcast.5009), window={size=3x3 pad=1_1x1_1}, dim_labels=bf01_oi01->bf01, custom_call_target=\"__cudnn$convBiasActivationForward\", metadata={op_type=\"Conv2D\" op_name=\"functional_2_1/conv3_block1_2_conv_1/convolution\" source_file=\"/home/gustavo/.local/lib/python3.10/site-packages/tensorflow/python/framework/ops.py\" source_line=1200}, backend_config={\"operation_queue_id\":\"0\",\"wait_on_operation_queues\":[],\"cudnn_conv_backend_config\":{\"conv_result_scale\":1,\"activation_mode\":\"kNone\",\"side_input_scale\":0,\"leakyrelu_alpha\":0},\"force_earliest_schedule\":false}\n",
            "2025-07-11 20:01:26.357245: I external/local_xla/xla/service/gpu/autotuning/conv_algorithm_picker.cc:549] Omitted potentially buggy algorithm eng14{k25=0} for conv %cudnn-conv-bias-activation.185 = (f32[31,256,14,14]{3,2,1,0}, u8[0]{0}) custom-call(f32[31,256,14,14]{3,2,1,0} %bitcast.5528, f32[256,256,3,3]{3,2,1,0} %bitcast.5535, f32[256]{0} %bitcast.5537), window={size=3x3 pad=1_1x1_1}, dim_labels=bf01_oi01->bf01, custom_call_target=\"__cudnn$convBiasActivationForward\", metadata={op_type=\"Conv2D\" op_name=\"functional_2_1/conv4_block1_2_conv_1/convolution\" source_file=\"/home/gustavo/.local/lib/python3.10/site-packages/tensorflow/python/framework/ops.py\" source_line=1200}, backend_config={\"operation_queue_id\":\"0\",\"wait_on_operation_queues\":[],\"cudnn_conv_backend_config\":{\"conv_result_scale\":1,\"activation_mode\":\"kNone\",\"side_input_scale\":0,\"leakyrelu_alpha\":0},\"force_earliest_schedule\":false}\n",
            "2025-07-11 20:01:26.831708: I external/local_xla/xla/service/gpu/autotuning/conv_algorithm_picker.cc:549] Omitted potentially buggy algorithm eng14{k25=0} for conv %cudnn-conv-bias-activation.204 = (f32[31,512,7,7]{3,2,1,0}, u8[0]{0}) custom-call(f32[31,512,7,7]{3,2,1,0} %bitcast.6302, f32[512,512,3,3]{3,2,1,0} %bitcast.6309, f32[512]{0} %bitcast.6311), window={size=3x3 pad=1_1x1_1}, dim_labels=bf01_oi01->bf01, custom_call_target=\"__cudnn$convBiasActivationForward\", metadata={op_type=\"Conv2D\" op_name=\"functional_2_1/conv5_block1_2_conv_1/convolution\" source_file=\"/home/gustavo/.local/lib/python3.10/site-packages/tensorflow/python/framework/ops.py\" source_line=1200}, backend_config={\"operation_queue_id\":\"0\",\"wait_on_operation_queues\":[],\"cudnn_conv_backend_config\":{\"conv_result_scale\":1,\"activation_mode\":\"kNone\",\"side_input_scale\":0,\"leakyrelu_alpha\":0},\"force_earliest_schedule\":false}\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "   üéØ Threshold inteligente aplicado:\n",
            "      Faixa de thresholds: 0.350 - 0.575\n",
            "      Recall Healthy: 0.952\n",
            "      Recall Unhealthy: 0.800 ‚≠ê\n",
            "\n",
            "   üìä THRESHOLD PADR√ÉO (0.5):\n",
            "      Accuracy: 0.9133\n",
            "      Recall Healthy: 0.9516\n",
            "      Recall Unhealthy: 0.8000\n",
            "\n",
            "   üß† THRESHOLD INTELIGENTE:\n",
            "      Accuracy: 0.9133\n",
            "      AUC-ROC: 0.9737\n",
            "      Recall Healthy: 0.9516\n",
            "      Recall Unhealthy: 0.8000 ‚≠ê\n",
            "      Matriz: [[295,  15], [ 21,  84]]\n",
            "\n",
            "   Classification Report (Threshold Inteligente):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "     Healthy       0.93      0.95      0.94       310\n",
            "   Unhealthy       0.85      0.80      0.82       105\n",
            "\n",
            "    accuracy                           0.91       415\n",
            "   macro avg       0.89      0.88      0.88       415\n",
            "weighted avg       0.91      0.91      0.91       415\n",
            "\n",
            "\n",
            "   üéØ MELHORIA NO RECALL UNHEALTHY: +0.000\n",
            "\n",
            "=== COMPARA√á√ÉO FINAL: RECALL UNHEALTHY (THRESHOLD INTELIGENTE) ===\n",
            "üéØ FOCO: Detectar plantas doentes (evitar falsos negativos)\n",
            "======================================================================\n",
            "   Tomato:\n",
            "      ü¶† Recall UNHEALTHY: 95.4% ‚Üí 95.0% (-0.4%) (Meta: 80-90%) üü¢ EXCELENTE\n",
            "      ‚úÖ Recall HEALTHY: 98.2% ‚Üí 98.2% (monitoramento)\n",
            "      üß† Thresholds aplicados: 0.385 - 0.632\n",
            "   Potato:\n",
            "      ü¶† Recall UNHEALTHY: 85.0% ‚Üí 91.2% (+6.1%) (Meta: 75-85%) üü¢ EXCELENTE\n",
            "      ‚úÖ Recall HEALTHY: 68.7% ‚Üí 55.8% (monitoramento)\n",
            "      üß† Thresholds aplicados: 0.315 - 0.517\n",
            "   Pepper:\n",
            "      ü¶† Recall UNHEALTHY: 80.0% ‚Üí 80.0% (+0.0%) (Meta: 70-80%) üü¢ EXCELENTE\n",
            "      ‚úÖ Recall HEALTHY: 95.2% ‚Üí 95.2% (monitoramento)\n",
            "      üß† Thresholds aplicados: 0.350 - 0.575\n",
            "======================================================================\n",
            "\n",
            "üéØ RESUMO DAS MELHORIAS NO RECALL UNHEALTHY:\n",
            "   üìâ Tomato: -0.4 pontos percentuais\n",
            "   üìà Potato: +6.1 pontos percentuais\n",
            "   ‚û°Ô∏è Pepper: +0.0 pontos percentuais\n",
            "\n",
            "üèÜ MELHORIA M√âDIA: +1.9 pontos percentuais no recall Unhealthy\n"
          ]
        }
      ],
      "source": [
        "# 6. AVALIA√á√ÉO OTIMIZADA E COMPARA√á√ÉO DE RESULTADOS\n",
        "from sklearn.metrics import (\n",
        "    classification_report, \n",
        "    accuracy_score, \n",
        "    confusion_matrix, \n",
        "    roc_auc_score, \n",
        "    recall_score, \n",
        "    precision_score, \n",
        "    f1_score\n",
        "    )\n",
        "\n",
        "def threshold_inteligente(probabilidade, especie, confianca_base=0.5):\n",
        "    \"\"\"\n",
        "    Threshold inteligente baseado na confian√ßa da predi√ß√£o\n",
        "    Prioriza detec√ß√£o de plantas doentes (recall Unhealthy)\n",
        "    \"\"\"\n",
        "    \n",
        "    # Configura√ß√µes base por esp√©cie (mais baixos = mais sens√≠vel a unhealthy)\n",
        "    thresholds_base = {\n",
        "        'tomato': 0.55,\n",
        "        'potato': 0.45,\n",
        "        'pepper': 0.50\n",
        "    }\n",
        "    \n",
        "    threshold_base = thresholds_base.get(especie.lower(), 0.5)\n",
        "    \n",
        "    # Ajuste din√¢mico baseado na confian√ßa\n",
        "    if probabilidade >= 0.8:\n",
        "        # Alta confian√ßa - usar threshold mais baixo para capturar unhealthy\n",
        "        threshold_ajustado = threshold_base * 0.7\n",
        "    elif probabilidade >= 0.6:\n",
        "        # Confian√ßa m√©dia-alta - leve redu√ß√£o\n",
        "        threshold_ajustado = threshold_base * 0.85\n",
        "    elif probabilidade >= 0.4:\n",
        "        # Confian√ßa m√©dia - threshold base\n",
        "        threshold_ajustado = threshold_base\n",
        "    else:\n",
        "        # Baixa confian√ßa - ser mais conservador\n",
        "        threshold_ajustado = threshold_base * 1.15\n",
        "    \n",
        "    # Garantir limites v√°lidos\n",
        "    threshold_ajustado = max(0.2, min(0.8, threshold_ajustado))\n",
        "    \n",
        "    return threshold_ajustado\n",
        "\n",
        "def otimizar_threshold_inteligente(modelo, test_gen, dataset_test, especie):\n",
        "    \"\"\"Aplica threshold inteligente priorizando detec√ß√£o de plantas doentes\"\"\"\n",
        "    \n",
        "    print(f\"   üß† Aplicando threshold inteligente para {especie}...\")\n",
        "    \n",
        "    # Obter todas as predi√ß√µes\n",
        "    test_gen.reset()\n",
        "    predictions_prob = modelo.predict(test_gen, verbose=0).flatten()\n",
        "    true_classes = [1 if label == 'unhealthy' else 0 for label in dataset_test['y']]\n",
        "    \n",
        "    # Aplicar threshold inteligente para cada predi√ß√£o\n",
        "    predictions_class_inteligente = []\n",
        "    thresholds_usados = []\n",
        "    \n",
        "    for prob in predictions_prob:\n",
        "        threshold = threshold_inteligente(prob, especie)\n",
        "        prediction = 1 if prob > threshold else 0\n",
        "        predictions_class_inteligente.append(prediction)\n",
        "        thresholds_usados.append(threshold)\n",
        "    \n",
        "    predictions_class_inteligente = np.array(predictions_class_inteligente)\n",
        "    \n",
        "    # Calcular m√©tricas\n",
        "    cm_inteligente = confusion_matrix(true_classes, predictions_class_inteligente)\n",
        "    \n",
        "    if cm_inteligente.shape == (2, 2):\n",
        "        tn, fp, fn, tp = cm_inteligente.ravel()\n",
        "        healthy_recall = tn / (tn + fp) if (tn + fp) > 0 else 0\n",
        "        unhealthy_recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
        "        \n",
        "        print(f\"   üéØ Threshold inteligente aplicado:\")\n",
        "        print(f\"      Faixa de thresholds: {min(thresholds_usados):.3f} - {max(thresholds_usados):.3f}\")\n",
        "        print(f\"      Recall Healthy: {healthy_recall:.3f}\")\n",
        "        print(f\"      Recall Unhealthy: {unhealthy_recall:.3f} ‚≠ê\")\n",
        "        \n",
        "        return {\n",
        "            'thresholds_usados': thresholds_usados,\n",
        "            'predictions': predictions_class_inteligente,\n",
        "            'recall_healthy': healthy_recall,\n",
        "            'recall_unhealthy': unhealthy_recall\n",
        "        }\n",
        "    \n",
        "    return None\n",
        "\n",
        "def avaliar_modelo_otimizado(modelo, especie, test_gen, dataset_test):\n",
        "    \"\"\"Avalia√ß√£o completa com threshold inteligente priorizando detec√ß√£o de doen√ßas\"\"\"\n",
        "    print(f\"\\nüìä Avaliando {especie} com threshold inteligente (foco em Unhealthy)...\")\n",
        "    \n",
        "    test_gen.reset()\n",
        "    \n",
        "    # Aplicar threshold inteligente\n",
        "    resultado_inteligente = otimizar_threshold_inteligente(modelo, test_gen, dataset_test, especie)\n",
        "    \n",
        "    # Predi√ß√µes com threshold padr√£o para compara√ß√£o\n",
        "    test_gen.reset()\n",
        "    predictions_prob = modelo.predict(test_gen, verbose=0).flatten()\n",
        "    predictions_class_default = (predictions_prob > 0.5).astype(int).flatten()\n",
        "    \n",
        "    # Classes verdadeiras\n",
        "    true_classes = [1 if label == 'unhealthy' else 0 for label in dataset_test['y']]\n",
        "    \n",
        "    # M√©tricas com threshold padr√£o (0.5)\n",
        "    print(f\"\\n   üìä THRESHOLD PADR√ÉO (0.5):\")\n",
        "    cm_default = confusion_matrix(true_classes, predictions_class_default)\n",
        "    accuracy_default = accuracy_score(true_classes, predictions_class_default)\n",
        "    \n",
        "    if cm_default.shape == (2, 2):\n",
        "        tn, fp, fn, tp = cm_default.ravel()\n",
        "        healthy_recall_default = tn / (tn + fp) if (tn + fp) > 0 else 0\n",
        "        unhealthy_recall_default = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
        "        print(f\"      Accuracy: {accuracy_default:.4f}\")\n",
        "        print(f\"      Recall Healthy: {healthy_recall_default:.4f}\")\n",
        "        print(f\"      Recall Unhealthy: {unhealthy_recall_default:.4f}\")\n",
        "    \n",
        "    # M√©tricas com threshold inteligente\n",
        "    if resultado_inteligente:\n",
        "        predictions_class_inteligente = resultado_inteligente['predictions']\n",
        "        cm_inteligente = confusion_matrix(true_classes, predictions_class_inteligente)\n",
        "        accuracy_inteligente = accuracy_score(true_classes, predictions_class_inteligente)\n",
        "        auc_score = roc_auc_score(true_classes, predictions_prob)\n",
        "        \n",
        "        print(f\"\\n   üß† THRESHOLD INTELIGENTE:\")\n",
        "        if cm_inteligente.shape == (2, 2):\n",
        "            tn, fp, fn, tp = cm_inteligente.ravel()\n",
        "            healthy_recall_inteligente = tn / (tn + fp) if (tn + fp) > 0 else 0\n",
        "            unhealthy_recall_inteligente = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
        "            \n",
        "            print(f\"      Accuracy: {accuracy_inteligente:.4f}\")\n",
        "            print(f\"      AUC-ROC: {auc_score:.4f}\")\n",
        "            print(f\"      Recall Healthy: {healthy_recall_inteligente:.4f}\")\n",
        "            print(f\"      Recall Unhealthy: {unhealthy_recall_inteligente:.4f} ‚≠ê\")\n",
        "            print(f\"      Matriz: [[{tn:3d}, {fp:3d}], [{fn:3d}, {tp:3d}]]\")\n",
        "            \n",
        "            # Relat√≥rio detalhado\n",
        "            print(\"\\n   Classification Report (Threshold Inteligente):\")\n",
        "            print(classification_report(true_classes, predictions_class_inteligente, target_names=['Healthy', 'Unhealthy'], zero_division=0))\n",
        "            \n",
        "            # An√°lise da melhoria\n",
        "            melhoria_unhealthy = unhealthy_recall_inteligente - unhealthy_recall_default\n",
        "            print(f\"\\n   üéØ MELHORIA NO RECALL UNHEALTHY: {melhoria_unhealthy:+.3f}\")\n",
        "            \n",
        "            return {\n",
        "                'threshold_inteligente': resultado_inteligente,\n",
        "                'accuracy_default': accuracy_default,\n",
        "                'accuracy_otimo': accuracy_inteligente,\n",
        "                'healthy_recall_default': healthy_recall_default,\n",
        "                'healthy_recall_otimo': healthy_recall_inteligente,\n",
        "                'unhealthy_recall_default': unhealthy_recall_default,\n",
        "                'unhealthy_recall_otimo': unhealthy_recall_inteligente,\n",
        "                'auc_roc': auc_score,\n",
        "                'confusion_matrix': cm_inteligente,\n",
        "                'melhoria_unhealthy': melhoria_unhealthy\n",
        "            }\n",
        "    \n",
        "    return None\n",
        "\n",
        "# Avaliar todos os modelos com threshold inteligente\n",
        "print(\"=== AVALIA√á√ÉO FINAL COM THRESHOLD INTELIGENTE ===\")\n",
        "print(\"üß† Sistema inteligente priorizando detec√ß√£o de plantas doentes:\")\n",
        "print(\"   - Thresholds din√¢micos baseados na confian√ßa da predi√ß√£o\")\n",
        "print(\"   - Tomato: Base 0.55 (ajuste 0.39-0.63)\")\n",
        "print(\"   - Potato: Base 0.45 (ajuste 0.32-0.52)\")  \n",
        "print(\"   - Pepper: Base 0.50 (ajuste 0.35-0.58)\")\n",
        "\n",
        "resultados_finais = {}\n",
        "\n",
        "# Usar datasets finais para teste (balanceados t√™m mesmo test set que originais)\n",
        "dataset_tomato_test = dataset_tomato_final['test'] if dataset_tomato_final else None\n",
        "dataset_potato_test = dataset_potato_final['test'] if dataset_potato_final else None  \n",
        "dataset_pepper_test = dataset_pepper_final['test'] if dataset_pepper_final else None\n",
        "\n",
        "if dataset_tomato_test and 'tomato' in histories:\n",
        "    resultados_finais['tomato'] = avaliar_modelo_otimizado(modelo_tomato, 'Tomato', test_gen_tomato, dataset_tomato_test)\n",
        "\n",
        "if dataset_potato_test and 'potato' in histories:\n",
        "    resultados_finais['potato'] = avaliar_modelo_otimizado(modelo_potato, 'Potato', test_gen_potato, dataset_potato_test)\n",
        "\n",
        "if dataset_pepper_test and 'pepper' in histories:\n",
        "    resultados_finais['pepper'] = avaliar_modelo_otimizado(modelo_pepper, 'Pepper', test_gen_pepper, dataset_pepper_test)\n",
        "\n",
        "# Compara√ß√£o final focando na melhoria do recall Unhealthy\n",
        "print(f\"\\n=== COMPARA√á√ÉO FINAL: RECALL UNHEALTHY (THRESHOLD INTELIGENTE) ===\")\n",
        "print(\"üéØ FOCO: Detectar plantas doentes (evitar falsos negativos)\")\n",
        "print(\"=\" * 70)\n",
        "for especie, resultado in resultados_finais.items():\n",
        "    if resultado:\n",
        "        # M√©tricas Unhealthy (principal objetivo)\n",
        "        unhealthy_antes = resultado['unhealthy_recall_default'] * 100\n",
        "        unhealthy_depois = resultado['unhealthy_recall_otimo'] * 100\n",
        "        melhoria_unhealthy = resultado.get('melhoria_unhealthy', 0) * 100\n",
        "        \n",
        "        # M√©tricas Healthy (monitoramento)\n",
        "        healthy_antes = resultado['healthy_recall_default'] * 100\n",
        "        healthy_depois = resultado['healthy_recall_otimo'] * 100\n",
        "        \n",
        "        # Metas para recall Unhealthy (detectar plantas doentes)\n",
        "        if especie == 'potato':\n",
        "            meta_unhealthy = \"75-85%\"\n",
        "            status = \"üü¢ EXCELENTE\" if unhealthy_depois >= 85 else \"üü° BOM\" if unhealthy_depois >= 75 else \"üî¥ INSUFICIENTE\"\n",
        "        elif especie == 'tomato':\n",
        "            meta_unhealthy = \"80-90%\"\n",
        "            status = \"üü¢ EXCELENTE\" if unhealthy_depois >= 90 else \"üü° BOM\" if unhealthy_depois >= 80 else \"üî¥ INSUFICIENTE\"\n",
        "        else:  # pepper\n",
        "            meta_unhealthy = \"70-80%\"\n",
        "            status = \"üü¢ EXCELENTE\" if unhealthy_depois >= 80 else \"üü° BOM\" if unhealthy_depois >= 70 else \"üî¥ INSUFICIENTE\"\n",
        "        \n",
        "        print(f\"   {especie.capitalize()}:\")\n",
        "        print(f\"      ü¶† Recall UNHEALTHY: {unhealthy_antes:.1f}% ‚Üí {unhealthy_depois:.1f}% ({melhoria_unhealthy:+.1f}%) (Meta: {meta_unhealthy}) {status}\")\n",
        "        print(f\"      ‚úÖ Recall HEALTHY: {healthy_antes:.1f}% ‚Üí {healthy_depois:.1f}% (monitoramento)\")\n",
        "        \n",
        "        # Mostrar faixa de thresholds usados\n",
        "        if 'threshold_inteligente' in resultado:\n",
        "            thresholds = resultado['threshold_inteligente']['thresholds_usados']\n",
        "            print(f\"      üß† Thresholds aplicados: {min(thresholds):.3f} - {max(thresholds):.3f}\")\n",
        "\n",
        "print(\"=\" * 70)\n",
        "\n",
        "# Resumo das melhorias\n",
        "print(f\"\\nüéØ RESUMO DAS MELHORIAS NO RECALL UNHEALTHY:\")\n",
        "total_melhoria = 0\n",
        "especies_melhoradas = 0\n",
        "\n",
        "for especie, resultado in resultados_finais.items():\n",
        "    if resultado and 'melhoria_unhealthy' in resultado:\n",
        "        melhoria = resultado['melhoria_unhealthy'] * 100\n",
        "        total_melhoria += melhoria\n",
        "        especies_melhoradas += 1\n",
        "        \n",
        "        status_emoji = \"üìà\" if melhoria > 0 else \"üìâ\" if melhoria < 0 else \"‚û°Ô∏è\"\n",
        "        print(f\"   {status_emoji} {especie.capitalize()}: {melhoria:+.1f} pontos percentuais\")\n",
        "\n",
        "if especies_melhoradas > 0:\n",
        "    melhoria_media = total_melhoria / especies_melhoradas\n",
        "    print(f\"\\nüèÜ MELHORIA M√âDIA: {melhoria_media:+.1f} pontos percentuais no recall Unhealthy\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "=== SALVANDO MODELOS OTIMIZADOS COM BALANCEAMENTO ===\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ Modelo Tomato salvo com melhorias de balanceamento\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ Modelo Potato salvo com melhorias de balanceamento\n",
            "‚úÖ Modelo Pepper salvo com melhorias de balanceamento\n",
            "\n",
            "üéØ RESUMO DAS MELHORIAS IMPLEMENTADAS:\n",
            "======================================================================\n",
            "‚úÖ Data Augmentation Direcionada aplicada\n",
            "‚úÖ SMOTE para balanceamento num√©rico aplicado\n",
            "‚úÖ Sistema de Threshold Inteligente implementado\n",
            "‚úÖ Foco na detec√ß√£o de plantas doentes (recall Unhealthy)\n",
            "‚úÖ Thresholds din√¢micos baseados na confian√ßa da predi√ß√£o\n",
            "‚úÖ Modelos treinados com dados balanceados\n",
            "‚úÖ Arquivos salvos com sistema inteligente\n",
            "======================================================================\n",
            "\n",
            "üéØ ARQUIVOS SALVOS:\n",
            "   - Modelos: modelos_salvos/especialistas/\n",
            "   - Sistema: modelos_salvos/especialistas/sistema_threshold_inteligente.pkl\n",
            "   - Pr√≥ximo passo: Implementar na API e testar!\n",
            "\n",
            "üß† SISTEMA DE THRESHOLD INTELIGENTE:\n",
            "   - Tomato: Base 0.55 ‚Üí Din√¢mico 0.39-0.63\n",
            "   - Potato: Base 0.45 ‚Üí Din√¢mico 0.32-0.52\n",
            "   - Pepper: Base 0.50 ‚Üí Din√¢mico 0.35-0.58\n",
            "   - L√≥gica: Confian√ßa alta = threshold baixo (mais sens√≠vel a unhealthy)\n",
            "   - Objetivo: Maximizar detec√ß√£o de plantas doentes\n"
          ]
        }
      ],
      "source": [
        "# 7. SALVAR MODELOS FINAIS COM MELHORIAS\n",
        "print(\"\\n=== SALVANDO MODELOS OTIMIZADOS COM BALANCEAMENTO ===\")\n",
        "\n",
        "# Salvar modelos treinados com dados balanceados\n",
        "import pickle\n",
        "\n",
        "# Salvar modelos\n",
        "if 'tomato' in histories:\n",
        "    modelo_tomato.save('modelos_salvos/especialistas/especialista_tomato_balanceado_final.h5')\n",
        "    print(\"‚úÖ Modelo Tomato salvo com melhorias de balanceamento\")\n",
        "\n",
        "if 'potato' in histories:\n",
        "    modelo_potato.save('modelos_salvos/especialistas/especialista_potato_balanceado_final.h5')\n",
        "    print(\"‚úÖ Modelo Potato salvo com melhorias de balanceamento\")\n",
        "\n",
        "if 'pepper' in histories:\n",
        "    modelo_pepper.save('modelos_salvos/especialistas/especialista_pepper_balanceado_final.h5')\n",
        "    print(\"‚úÖ Modelo Pepper salvo com melhorias de balanceamento\")\n",
        "\n",
        "# Salvar informa√ß√µes do sistema de threshold inteligente\n",
        "sistema_threshold_inteligente = {}\n",
        "for especie, resultado in resultados_finais.items():\n",
        "    if resultado and 'threshold_inteligente' in resultado:\n",
        "        threshold_info = resultado['threshold_inteligente']\n",
        "        sistema_threshold_inteligente[especie] = {\n",
        "            'sistema': 'threshold_inteligente',\n",
        "            'thresholds_range': f\"{min(threshold_info['thresholds_usados']):.3f}-{max(threshold_info['thresholds_usados']):.3f}\",\n",
        "            'healthy_recall': resultado['healthy_recall_otimo'],\n",
        "            'unhealthy_recall': resultado['unhealthy_recall_otimo'],\n",
        "            'melhoria_unhealthy': resultado.get('melhoria_unhealthy', 0),\n",
        "            'accuracy': resultado['accuracy_otimo'],\n",
        "            'thresholds_detalhados': threshold_info['thresholds_usados']\n",
        "        }\n",
        "\n",
        "with open('modelos_salvos/especialistas/sistema_threshold_inteligente.pkl', 'wb') as f:\n",
        "    pickle.dump(sistema_threshold_inteligente, f)\n",
        "\n",
        "print(\"\\nüéØ RESUMO DAS MELHORIAS IMPLEMENTADAS:\")\n",
        "print(\"=\" * 70)\n",
        "print(\"‚úÖ Data Augmentation Direcionada aplicada\")\n",
        "print(\"‚úÖ SMOTE para balanceamento num√©rico aplicado\")\n",
        "print(\"‚úÖ Sistema de Threshold Inteligente implementado\")\n",
        "print(\"‚úÖ Foco na detec√ß√£o de plantas doentes (recall Unhealthy)\")\n",
        "print(\"‚úÖ Thresholds din√¢micos baseados na confian√ßa da predi√ß√£o\")\n",
        "print(\"‚úÖ Modelos treinados com dados balanceados\")\n",
        "print(\"‚úÖ Arquivos salvos com sistema inteligente\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "print(f\"\\nüéØ ARQUIVOS SALVOS:\")\n",
        "print(f\"   - Modelos: modelos_salvos/especialistas/\")\n",
        "print(f\"   - Sistema: modelos_salvos/especialistas/sistema_threshold_inteligente.pkl\")\n",
        "print(f\"   - Pr√≥ximo passo: Implementar na API e testar!\")\n",
        "\n",
        "print(f\"\\nüß† SISTEMA DE THRESHOLD INTELIGENTE:\")\n",
        "print(f\"   - Tomato: Base 0.55 ‚Üí Din√¢mico 0.39-0.63\")\n",
        "print(f\"   - Potato: Base 0.45 ‚Üí Din√¢mico 0.32-0.52\") \n",
        "print(f\"   - Pepper: Base 0.50 ‚Üí Din√¢mico 0.35-0.58\")\n",
        "print(f\"   - L√≥gica: Confian√ßa alta = threshold baixo (mais sens√≠vel a unhealthy)\")\n",
        "print(f\"   - Objetivo: Maximizar detec√ß√£o de plantas doentes\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
